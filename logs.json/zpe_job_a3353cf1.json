{
  "job_id": "zpe_job_a3353cf1",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-Colab-Sim_hnn_step8",
    "totalEpochs": 60,
    "batchSize": 32,
    "learningRate": 0.0013,
    "weightDecay": 1.2567843546879078e-05,
    "momentumParams": [
      0.97,
      0.85,
      0.78,
      0.72,
      0.41,
      0.76
    ],
    "strengthParams": [
      0.19,
      0.55,
      0.41,
      0.6,
      0.37,
      0.44
    ],
    "noiseParams": [
      0.17,
      0.26,
      0.24,
      0.43,
      0.28,
      0.24
    ],
    "couplingParams": [
      0.75,
      0.85,
      0.82,
      0.79,
      0.76,
      0.6
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.07,
    "quantumMode": true,
    "baseConfigId": "zpe_job_169cda4c",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T22:12:52.000Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/60 - Batch 0/1875 - Loss: 2.3239",
    "Epoch 1/60 - Batch 100/1875 - Loss: 0.6867",
    "Epoch 1/60 - Batch 200/1875 - Loss: 0.4587",
    "Epoch 1/60 - Batch 300/1875 - Loss: 0.1775",
    "Epoch 1/60 - Batch 400/1875 - Loss: 0.2828",
    "Epoch 1/60 - Batch 500/1875 - Loss: 0.1874",
    "Epoch 1/60 - Batch 600/1875 - Loss: 0.3466",
    "Epoch 1/60 - Batch 700/1875 - Loss: 0.7570",
    "Epoch 1/60 - Batch 800/1875 - Loss: 0.1830",
    "Epoch 1/60 - Batch 900/1875 - Loss: 0.0657",
    "Epoch 1/60 - Batch 1000/1875 - Loss: 0.3323",
    "Epoch 1/60 - Batch 1100/1875 - Loss: 0.1600",
    "Epoch 1/60 - Batch 1200/1875 - Loss: 0.1241",
    "Epoch 1/60 - Batch 1300/1875 - Loss: 0.0621",
    "Epoch 1/60 - Batch 1400/1875 - Loss: 0.1761",
    "Epoch 1/60 - Batch 1500/1875 - Loss: 0.3764",
    "Epoch 1/60 - Batch 1600/1875 - Loss: 0.4022",
    "Epoch 1/60 - Batch 1700/1875 - Loss: 0.2462",
    "Epoch 1/60 - Batch 1800/1875 - Loss: 0.3048",
    "Epoch 1 completed - Train Acc: 91.51% - Val Acc: 95.57%",
    "Epoch 2/60 - Batch 0/1875 - Loss: 0.2358",
    "Epoch 2/60 - Batch 100/1875 - Loss: 0.0444",
    "Epoch 2/60 - Batch 200/1875 - Loss: 0.2762",
    "Epoch 2/60 - Batch 300/1875 - Loss: 0.2906",
    "Epoch 2/60 - Batch 400/1875 - Loss: 0.3071",
    "Epoch 2/60 - Batch 500/1875 - Loss: 0.0178",
    "Epoch 2/60 - Batch 600/1875 - Loss: 0.0389",
    "Epoch 2/60 - Batch 700/1875 - Loss: 0.0475",
    "Epoch 2/60 - Batch 800/1875 - Loss: 0.0257",
    "Epoch 2/60 - Batch 900/1875 - Loss: 0.1993",
    "Epoch 2/60 - Batch 1000/1875 - Loss: 0.2526",
    "Epoch 2/60 - Batch 1100/1875 - Loss: 0.1849",
    "Epoch 2/60 - Batch 1200/1875 - Loss: 0.2179",
    "Epoch 2/60 - Batch 1300/1875 - Loss: 0.0929",
    "Epoch 2/60 - Batch 1400/1875 - Loss: 0.2156",
    "Epoch 2/60 - Batch 1500/1875 - Loss: 0.0162",
    "Epoch 2/60 - Batch 1600/1875 - Loss: 0.0921",
    "Epoch 2/60 - Batch 1700/1875 - Loss: 0.1303",
    "Epoch 2/60 - Batch 1800/1875 - Loss: 0.1712",
    "Epoch 2 completed - Train Acc: 96.24% - Val Acc: 96.57%",
    "Epoch 3/60 - Batch 0/1875 - Loss: 0.2652",
    "Epoch 3/60 - Batch 100/1875 - Loss: 0.1583",
    "Epoch 3/60 - Batch 200/1875 - Loss: 0.1090",
    "Epoch 3/60 - Batch 300/1875 - Loss: 0.0603",
    "Epoch 3/60 - Batch 400/1875 - Loss: 0.0050",
    "Epoch 3/60 - Batch 500/1875 - Loss: 0.0546",
    "Epoch 3/60 - Batch 600/1875 - Loss: 0.0197",
    "Epoch 3/60 - Batch 700/1875 - Loss: 0.1918",
    "Epoch 3/60 - Batch 800/1875 - Loss: 0.0533",
    "Epoch 3/60 - Batch 900/1875 - Loss: 0.0484",
    "Epoch 3/60 - Batch 1000/1875 - Loss: 0.2127",
    "Epoch 3/60 - Batch 1100/1875 - Loss: 0.1389",
    "Epoch 3/60 - Batch 1200/1875 - Loss: 0.0079",
    "Epoch 3/60 - Batch 1300/1875 - Loss: 0.0414",
    "Epoch 3/60 - Batch 1400/1875 - Loss: 0.1725",
    "Epoch 3/60 - Batch 1500/1875 - Loss: 0.3324",
    "Epoch 3/60 - Batch 1600/1875 - Loss: 0.1269",
    "Epoch 3/60 - Batch 1700/1875 - Loss: 0.0080",
    "Epoch 3/60 - Batch 1800/1875 - Loss: 0.0028",
    "Epoch 3 completed - Train Acc: 97.16% - Val Acc: 96.75%",
    "Epoch 4/60 - Batch 0/1875 - Loss: 0.0731",
    "Epoch 4/60 - Batch 100/1875 - Loss: 0.2861",
    "Epoch 4/60 - Batch 200/1875 - Loss: 0.0060",
    "Epoch 4/60 - Batch 300/1875 - Loss: 0.1189",
    "Epoch 4/60 - Batch 400/1875 - Loss: 0.0448",
    "Epoch 4/60 - Batch 500/1875 - Loss: 0.0328",
    "Epoch 4/60 - Batch 600/1875 - Loss: 0.0064",
    "Epoch 4/60 - Batch 700/1875 - Loss: 0.5571",
    "Epoch 4/60 - Batch 800/1875 - Loss: 0.0457",
    "Epoch 4/60 - Batch 900/1875 - Loss: 0.1016",
    "Epoch 4/60 - Batch 1000/1875 - Loss: 0.0466",
    "Epoch 4/60 - Batch 1100/1875 - Loss: 0.0011",
    "Epoch 4/60 - Batch 1200/1875 - Loss: 0.0148",
    "Epoch 4/60 - Batch 1300/1875 - Loss: 0.0500",
    "Epoch 4/60 - Batch 1400/1875 - Loss: 0.1816",
    "Epoch 4/60 - Batch 1500/1875 - Loss: 0.0118",
    "Epoch 4/60 - Batch 1600/1875 - Loss: 0.0554",
    "Epoch 4/60 - Batch 1700/1875 - Loss: 0.0016",
    "Epoch 4/60 - Batch 1800/1875 - Loss: 0.0076",
    "Epoch 4 completed - Train Acc: 97.53% - Val Acc: 97.04%",
    "Epoch 5/60 - Batch 0/1875 - Loss: 0.4030",
    "Epoch 5/60 - Batch 100/1875 - Loss: 0.1853",
    "Epoch 5/60 - Batch 200/1875 - Loss: 0.0402",
    "Epoch 5/60 - Batch 300/1875 - Loss: 0.1902",
    "Epoch 5/60 - Batch 400/1875 - Loss: 0.0719",
    "Epoch 5/60 - Batch 500/1875 - Loss: 0.1254",
    "Epoch 5/60 - Batch 600/1875 - Loss: 0.0042",
    "Epoch 5/60 - Batch 700/1875 - Loss: 0.0768",
    "Epoch 5/60 - Batch 800/1875 - Loss: 0.0192",
    "Epoch 5/60 - Batch 900/1875 - Loss: 0.0155",
    "Epoch 5/60 - Batch 1000/1875 - Loss: 0.2419",
    "Epoch 5/60 - Batch 1100/1875 - Loss: 0.0962",
    "Epoch 5/60 - Batch 1200/1875 - Loss: 0.0230",
    "Epoch 5/60 - Batch 1300/1875 - Loss: 0.0650",
    "Epoch 5/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 5/60 - Batch 1500/1875 - Loss: 0.1946",
    "Epoch 5/60 - Batch 1600/1875 - Loss: 0.0339",
    "Epoch 5/60 - Batch 1700/1875 - Loss: 0.3416",
    "Epoch 5/60 - Batch 1800/1875 - Loss: 0.0676",
    "Epoch 5 completed - Train Acc: 97.98% - Val Acc: 97.08%",
    "Epoch 6/60 - Batch 0/1875 - Loss: 0.1167",
    "Epoch 6/60 - Batch 100/1875 - Loss: 0.0022",
    "Epoch 6/60 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 6/60 - Batch 300/1875 - Loss: 0.0118",
    "Epoch 6/60 - Batch 400/1875 - Loss: 0.1186",
    "Epoch 6/60 - Batch 500/1875 - Loss: 0.0269",
    "Epoch 6/60 - Batch 600/1875 - Loss: 0.0067",
    "Epoch 6/60 - Batch 700/1875 - Loss: 0.0118",
    "Epoch 6/60 - Batch 800/1875 - Loss: 0.0013",
    "Epoch 6/60 - Batch 900/1875 - Loss: 0.0015",
    "Epoch 6/60 - Batch 1000/1875 - Loss: 0.0671",
    "Epoch 6/60 - Batch 1100/1875 - Loss: 0.0683",
    "Epoch 6/60 - Batch 1200/1875 - Loss: 0.1886",
    "Epoch 6/60 - Batch 1300/1875 - Loss: 0.0638",
    "Epoch 6/60 - Batch 1400/1875 - Loss: 0.0769",
    "Epoch 6/60 - Batch 1500/1875 - Loss: 0.2191",
    "Epoch 6/60 - Batch 1600/1875 - Loss: 0.0040",
    "Epoch 6/60 - Batch 1700/1875 - Loss: 0.0192",
    "Epoch 6/60 - Batch 1800/1875 - Loss: 0.0050",
    "Epoch 6 completed - Train Acc: 98.21% - Val Acc: 97.26%",
    "Epoch 7/60 - Batch 0/1875 - Loss: 0.0977",
    "Epoch 7/60 - Batch 100/1875 - Loss: 0.0007",
    "Epoch 7/60 - Batch 200/1875 - Loss: 0.0019",
    "Epoch 7/60 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 7/60 - Batch 400/1875 - Loss: 0.0160",
    "Epoch 7/60 - Batch 500/1875 - Loss: 0.0381",
    "Epoch 7/60 - Batch 600/1875 - Loss: 0.1118",
    "Epoch 7/60 - Batch 700/1875 - Loss: 0.0083",
    "Epoch 7/60 - Batch 800/1875 - Loss: 0.1339",
    "Epoch 7/60 - Batch 900/1875 - Loss: 0.0091",
    "Epoch 7/60 - Batch 1000/1875 - Loss: 0.0037",
    "Epoch 7/60 - Batch 1100/1875 - Loss: 0.0610",
    "Epoch 7/60 - Batch 1200/1875 - Loss: 0.0866",
    "Epoch 7/60 - Batch 1300/1875 - Loss: 0.0021",
    "Epoch 7/60 - Batch 1400/1875 - Loss: 0.0038",
    "Epoch 7/60 - Batch 1500/1875 - Loss: 0.0092",
    "Epoch 7/60 - Batch 1600/1875 - Loss: 0.0191",
    "Epoch 7/60 - Batch 1700/1875 - Loss: 0.0029",
    "Epoch 7/60 - Batch 1800/1875 - Loss: 0.0131",
    "Epoch 7 completed - Train Acc: 98.34% - Val Acc: 97.55%",
    "Epoch 8/60 - Batch 0/1875 - Loss: 0.0065",
    "Epoch 8/60 - Batch 100/1875 - Loss: 0.0831",
    "Epoch 8/60 - Batch 200/1875 - Loss: 0.0678",
    "Epoch 8/60 - Batch 300/1875 - Loss: 0.1346",
    "Epoch 8/60 - Batch 400/1875 - Loss: 0.0037",
    "Epoch 8/60 - Batch 500/1875 - Loss: 0.1073",
    "Epoch 8/60 - Batch 600/1875 - Loss: 0.0665",
    "Epoch 8/60 - Batch 700/1875 - Loss: 0.0497",
    "Epoch 8/60 - Batch 800/1875 - Loss: 0.0722",
    "Epoch 8/60 - Batch 900/1875 - Loss: 0.2452",
    "Epoch 8/60 - Batch 1000/1875 - Loss: 0.0103",
    "Epoch 8/60 - Batch 1100/1875 - Loss: 0.0147",
    "Epoch 8/60 - Batch 1200/1875 - Loss: 0.0133",
    "Epoch 8/60 - Batch 1300/1875 - Loss: 0.0686",
    "Epoch 8/60 - Batch 1400/1875 - Loss: 0.0297",
    "Epoch 8/60 - Batch 1500/1875 - Loss: 0.0581",
    "Epoch 8/60 - Batch 1600/1875 - Loss: 0.0093",
    "Epoch 8/60 - Batch 1700/1875 - Loss: 0.1606",
    "Epoch 8/60 - Batch 1800/1875 - Loss: 0.0195",
    "Epoch 8 completed - Train Acc: 98.43% - Val Acc: 97.37%",
    "Epoch 9/60 - Batch 0/1875 - Loss: 0.0119",
    "Epoch 9/60 - Batch 100/1875 - Loss: 0.1268",
    "Epoch 9/60 - Batch 200/1875 - Loss: 0.0419",
    "Epoch 9/60 - Batch 300/1875 - Loss: 0.0068",
    "Epoch 9/60 - Batch 400/1875 - Loss: 0.4835",
    "Epoch 9/60 - Batch 500/1875 - Loss: 0.0187",
    "Epoch 9/60 - Batch 600/1875 - Loss: 0.0009",
    "Epoch 9/60 - Batch 700/1875 - Loss: 0.0130",
    "Epoch 9/60 - Batch 800/1875 - Loss: 0.0418",
    "Epoch 9/60 - Batch 900/1875 - Loss: 0.1075",
    "Epoch 9/60 - Batch 1000/1875 - Loss: 0.0086",
    "Epoch 9/60 - Batch 1100/1875 - Loss: 0.1750",
    "Epoch 9/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 9/60 - Batch 1300/1875 - Loss: 0.0015",
    "Epoch 9/60 - Batch 1400/1875 - Loss: 0.0063",
    "Epoch 9/60 - Batch 1500/1875 - Loss: 0.0657",
    "Epoch 9/60 - Batch 1600/1875 - Loss: 0.0026",
    "Epoch 9/60 - Batch 1700/1875 - Loss: 0.0025",
    "Epoch 9/60 - Batch 1800/1875 - Loss: 0.0402",
    "Epoch 9 completed - Train Acc: 98.70% - Val Acc: 97.53%",
    "Epoch 10/60 - Batch 0/1875 - Loss: 0.0025",
    "Epoch 10/60 - Batch 100/1875 - Loss: 0.2595",
    "Epoch 10/60 - Batch 200/1875 - Loss: 0.0145",
    "Epoch 10/60 - Batch 300/1875 - Loss: 0.0065",
    "Epoch 10/60 - Batch 400/1875 - Loss: 0.0092",
    "Epoch 10/60 - Batch 500/1875 - Loss: 0.0351",
    "Epoch 10/60 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 10/60 - Batch 700/1875 - Loss: 0.0712",
    "Epoch 10/60 - Batch 800/1875 - Loss: 0.0617",
    "Epoch 10/60 - Batch 900/1875 - Loss: 0.0022",
    "Epoch 10/60 - Batch 1000/1875 - Loss: 0.0106",
    "Epoch 10/60 - Batch 1100/1875 - Loss: 0.0044",
    "Epoch 10/60 - Batch 1200/1875 - Loss: 0.0689",
    "Epoch 10/60 - Batch 1300/1875 - Loss: 0.0096",
    "Epoch 10/60 - Batch 1400/1875 - Loss: 0.0878",
    "Epoch 10/60 - Batch 1500/1875 - Loss: 0.1343",
    "Epoch 10/60 - Batch 1600/1875 - Loss: 0.0035",
    "Epoch 10/60 - Batch 1700/1875 - Loss: 0.0117",
    "Epoch 10/60 - Batch 1800/1875 - Loss: 0.0087",
    "Epoch 10 completed - Train Acc: 98.68% - Val Acc: 97.59%",
    "Epoch 11/60 - Batch 0/1875 - Loss: 0.0161",
    "Epoch 11/60 - Batch 100/1875 - Loss: 0.0100",
    "Epoch 11/60 - Batch 200/1875 - Loss: 0.0527",
    "Epoch 11/60 - Batch 300/1875 - Loss: 0.1319",
    "Epoch 11/60 - Batch 400/1875 - Loss: 0.0335",
    "Epoch 11/60 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 11/60 - Batch 600/1875 - Loss: 0.0043",
    "Epoch 11/60 - Batch 700/1875 - Loss: 0.0343",
    "Epoch 11/60 - Batch 800/1875 - Loss: 0.0199",
    "Epoch 11/60 - Batch 900/1875 - Loss: 0.0544",
    "Epoch 11/60 - Batch 1000/1875 - Loss: 0.1661",
    "Epoch 11/60 - Batch 1100/1875 - Loss: 0.1156",
    "Epoch 11/60 - Batch 1200/1875 - Loss: 0.0020",
    "Epoch 11/60 - Batch 1300/1875 - Loss: 0.0172",
    "Epoch 11/60 - Batch 1400/1875 - Loss: 0.2424",
    "Epoch 11/60 - Batch 1500/1875 - Loss: 0.1079",
    "Epoch 11/60 - Batch 1600/1875 - Loss: 0.0155",
    "Epoch 11/60 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 11/60 - Batch 1800/1875 - Loss: 0.0123",
    "Epoch 11 completed - Train Acc: 98.87% - Val Acc: 97.98%",
    "Epoch 12/60 - Batch 0/1875 - Loss: 0.0059",
    "Epoch 12/60 - Batch 100/1875 - Loss: 0.0039",
    "Epoch 12/60 - Batch 200/1875 - Loss: 0.0023",
    "Epoch 12/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 12/60 - Batch 400/1875 - Loss: 0.0639",
    "Epoch 12/60 - Batch 500/1875 - Loss: 0.0028",
    "Epoch 12/60 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 12/60 - Batch 700/1875 - Loss: 0.0603",
    "Epoch 12/60 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 12/60 - Batch 900/1875 - Loss: 0.0130",
    "Epoch 12/60 - Batch 1000/1875 - Loss: 0.1105",
    "Epoch 12/60 - Batch 1100/1875 - Loss: 0.0016",
    "Epoch 12/60 - Batch 1200/1875 - Loss: 0.0069",
    "Epoch 12/60 - Batch 1300/1875 - Loss: 0.0017",
    "Epoch 12/60 - Batch 1400/1875 - Loss: 0.0020",
    "Epoch 12/60 - Batch 1500/1875 - Loss: 0.0014",
    "Epoch 12/60 - Batch 1600/1875 - Loss: 0.0613",
    "Epoch 12/60 - Batch 1700/1875 - Loss: 0.0090",
    "Epoch 12/60 - Batch 1800/1875 - Loss: 0.0029",
    "Epoch 12 completed - Train Acc: 98.89% - Val Acc: 97.15%",
    "Epoch 13/60 - Batch 0/1875 - Loss: 0.0193",
    "Epoch 13/60 - Batch 100/1875 - Loss: 0.0017",
    "Epoch 13/60 - Batch 200/1875 - Loss: 0.0075",
    "Epoch 13/60 - Batch 300/1875 - Loss: 0.0175",
    "Epoch 13/60 - Batch 400/1875 - Loss: 0.0140",
    "Epoch 13/60 - Batch 500/1875 - Loss: 0.0166",
    "Epoch 13/60 - Batch 600/1875 - Loss: 0.0406",
    "Epoch 13/60 - Batch 700/1875 - Loss: 0.0610",
    "Epoch 13/60 - Batch 800/1875 - Loss: 0.1239",
    "Epoch 13/60 - Batch 900/1875 - Loss: 0.0158",
    "Epoch 13/60 - Batch 1000/1875 - Loss: 0.0377",
    "Epoch 13/60 - Batch 1100/1875 - Loss: 0.0026",
    "Epoch 13/60 - Batch 1200/1875 - Loss: 0.0065",
    "Epoch 13/60 - Batch 1300/1875 - Loss: 0.0164",
    "Epoch 13/60 - Batch 1400/1875 - Loss: 0.0682",
    "Epoch 13/60 - Batch 1500/1875 - Loss: 0.0151",
    "Epoch 13/60 - Batch 1600/1875 - Loss: 0.0021",
    "Epoch 13/60 - Batch 1700/1875 - Loss: 0.0047",
    "Epoch 13/60 - Batch 1800/1875 - Loss: 0.0072",
    "Epoch 13 completed - Train Acc: 98.94% - Val Acc: 97.50%",
    "Epoch 14/60 - Batch 0/1875 - Loss: 0.0567",
    "Epoch 14/60 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 14/60 - Batch 200/1875 - Loss: 0.0021",
    "Epoch 14/60 - Batch 300/1875 - Loss: 0.0033",
    "Epoch 14/60 - Batch 400/1875 - Loss: 0.0482",
    "Epoch 14/60 - Batch 500/1875 - Loss: 0.0021",
    "Epoch 14/60 - Batch 600/1875 - Loss: 0.1011",
    "Epoch 14/60 - Batch 700/1875 - Loss: 0.0644",
    "Epoch 14/60 - Batch 800/1875 - Loss: 0.0082",
    "Epoch 14/60 - Batch 900/1875 - Loss: 0.0952",
    "Epoch 14/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 14/60 - Batch 1100/1875 - Loss: 0.0384",
    "Epoch 14/60 - Batch 1200/1875 - Loss: 0.0042",
    "Epoch 14/60 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 14/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 14/60 - Batch 1500/1875 - Loss: 0.0449",
    "Epoch 14/60 - Batch 1600/1875 - Loss: 0.1567",
    "Epoch 14/60 - Batch 1700/1875 - Loss: 0.0905",
    "Epoch 14/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 14 completed - Train Acc: 99.09% - Val Acc: 97.70%",
    "Epoch 15/60 - Batch 0/1875 - Loss: 0.0026",
    "Epoch 15/60 - Batch 100/1875 - Loss: 0.0639",
    "Epoch 15/60 - Batch 200/1875 - Loss: 0.0034",
    "Epoch 15/60 - Batch 300/1875 - Loss: 0.1905",
    "Epoch 15/60 - Batch 400/1875 - Loss: 0.0436",
    "Epoch 15/60 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 15/60 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 15/60 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 15/60 - Batch 800/1875 - Loss: 0.0162",
    "Epoch 15/60 - Batch 900/1875 - Loss: 0.0175",
    "Epoch 15/60 - Batch 1000/1875 - Loss: 0.0031",
    "Epoch 15/60 - Batch 1100/1875 - Loss: 0.0042",
    "Epoch 15/60 - Batch 1200/1875 - Loss: 0.0040",
    "Epoch 15/60 - Batch 1300/1875 - Loss: 0.0428",
    "Epoch 15/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 15/60 - Batch 1500/1875 - Loss: 0.1116",
    "Epoch 15/60 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 15/60 - Batch 1700/1875 - Loss: 0.0078",
    "Epoch 15/60 - Batch 1800/1875 - Loss: 0.1470",
    "Epoch 15 completed - Train Acc: 98.97% - Val Acc: 97.83%",
    "Epoch 16/60 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 16/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 16/60 - Batch 200/1875 - Loss: 0.2100",
    "Epoch 16/60 - Batch 300/1875 - Loss: 0.0224",
    "Epoch 16/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 16/60 - Batch 500/1875 - Loss: 0.0087",
    "Epoch 16/60 - Batch 600/1875 - Loss: 0.0019",
    "Epoch 16/60 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 16/60 - Batch 800/1875 - Loss: 0.0099",
    "Epoch 16/60 - Batch 900/1875 - Loss: 0.1295",
    "Epoch 16/60 - Batch 1000/1875 - Loss: 0.1578",
    "Epoch 16/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 16/60 - Batch 1200/1875 - Loss: 0.0229",
    "Epoch 16/60 - Batch 1300/1875 - Loss: 0.0017",
    "Epoch 16/60 - Batch 1400/1875 - Loss: 0.0029",
    "Epoch 16/60 - Batch 1500/1875 - Loss: 0.0031",
    "Epoch 16/60 - Batch 1600/1875 - Loss: 0.0029",
    "Epoch 16/60 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 16/60 - Batch 1800/1875 - Loss: 0.0530",
    "Epoch 16 completed - Train Acc: 99.06% - Val Acc: 97.51%",
    "Epoch 17/60 - Batch 0/1875 - Loss: 0.0380",
    "Epoch 17/60 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 17/60 - Batch 200/1875 - Loss: 0.0129",
    "Epoch 17/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 17/60 - Batch 400/1875 - Loss: 0.3039",
    "Epoch 17/60 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 17/60 - Batch 600/1875 - Loss: 0.0071",
    "Epoch 17/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 17/60 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 17/60 - Batch 900/1875 - Loss: 0.0025",
    "Epoch 17/60 - Batch 1000/1875 - Loss: 0.0100",
    "Epoch 17/60 - Batch 1100/1875 - Loss: 0.0101",
    "Epoch 17/60 - Batch 1200/1875 - Loss: 0.0011",
    "Epoch 17/60 - Batch 1300/1875 - Loss: 0.0006",
    "Epoch 17/60 - Batch 1400/1875 - Loss: 0.0979",
    "Epoch 17/60 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 17/60 - Batch 1600/1875 - Loss: 0.0576",
    "Epoch 17/60 - Batch 1700/1875 - Loss: 0.0020",
    "Epoch 17/60 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 17 completed - Train Acc: 99.16% - Val Acc: 97.37%",
    "Epoch 18/60 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 18/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 18/60 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 18/60 - Batch 300/1875 - Loss: 0.0059",
    "Epoch 18/60 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 18/60 - Batch 500/1875 - Loss: 0.0067",
    "Epoch 18/60 - Batch 600/1875 - Loss: 0.0102",
    "Epoch 18/60 - Batch 700/1875 - Loss: 0.0257",
    "Epoch 18/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 18/60 - Batch 900/1875 - Loss: 0.0120",
    "Epoch 18/60 - Batch 1000/1875 - Loss: 0.0043",
    "Epoch 18/60 - Batch 1100/1875 - Loss: 0.0128",
    "Epoch 18/60 - Batch 1200/1875 - Loss: 0.0009",
    "Epoch 18/60 - Batch 1300/1875 - Loss: 0.0168",
    "Epoch 18/60 - Batch 1400/1875 - Loss: 0.0061",
    "Epoch 18/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 18/60 - Batch 1600/1875 - Loss: 0.0078",
    "Epoch 18/60 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 18/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 18 completed - Train Acc: 99.13% - Val Acc: 97.79%",
    "Epoch 19/60 - Batch 0/1875 - Loss: 0.1346",
    "Epoch 19/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 19/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 19/60 - Batch 300/1875 - Loss: 0.0009",
    "Epoch 19/60 - Batch 400/1875 - Loss: 0.0188",
    "Epoch 19/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 19/60 - Batch 600/1875 - Loss: 0.0050",
    "Epoch 19/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 19/60 - Batch 800/1875 - Loss: 0.0017",
    "Epoch 19/60 - Batch 900/1875 - Loss: 0.0615",
    "Epoch 19/60 - Batch 1000/1875 - Loss: 0.1103",
    "Epoch 19/60 - Batch 1100/1875 - Loss: 0.0067",
    "Epoch 19/60 - Batch 1200/1875 - Loss: 0.0055",
    "Epoch 19/60 - Batch 1300/1875 - Loss: 0.0066",
    "Epoch 19/60 - Batch 1400/1875 - Loss: 0.0094",
    "Epoch 19/60 - Batch 1500/1875 - Loss: 0.0713",
    "Epoch 19/60 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 19/60 - Batch 1700/1875 - Loss: 0.0026",
    "Epoch 19/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 19 completed - Train Acc: 99.17% - Val Acc: 97.87%",
    "Epoch 20/60 - Batch 0/1875 - Loss: 0.0446",
    "Epoch 20/60 - Batch 100/1875 - Loss: 0.0009",
    "Epoch 20/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 20/60 - Batch 300/1875 - Loss: 0.0062",
    "Epoch 20/60 - Batch 400/1875 - Loss: 0.0257",
    "Epoch 20/60 - Batch 500/1875 - Loss: 0.1075",
    "Epoch 20/60 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 20/60 - Batch 700/1875 - Loss: 0.0170",
    "Epoch 20/60 - Batch 800/1875 - Loss: 0.0304",
    "Epoch 20/60 - Batch 900/1875 - Loss: 0.0008",
    "Epoch 20/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 20/60 - Batch 1100/1875 - Loss: 0.0299",
    "Epoch 20/60 - Batch 1200/1875 - Loss: 0.0010",
    "Epoch 20/60 - Batch 1300/1875 - Loss: 0.0024",
    "Epoch 20/60 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 20/60 - Batch 1500/1875 - Loss: 0.0048",
    "Epoch 20/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 20/60 - Batch 1700/1875 - Loss: 0.0355",
    "Epoch 20/60 - Batch 1800/1875 - Loss: 0.0208",
    "Epoch 20 completed - Train Acc: 99.22% - Val Acc: 97.83%",
    "Epoch 21/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 21/60 - Batch 100/1875 - Loss: 0.0499",
    "Epoch 21/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 21/60 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 21/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 21/60 - Batch 500/1875 - Loss: 0.0369",
    "Epoch 21/60 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 21/60 - Batch 700/1875 - Loss: 0.1395",
    "Epoch 21/60 - Batch 800/1875 - Loss: 0.0018",
    "Epoch 21/60 - Batch 900/1875 - Loss: 0.0296",
    "Epoch 21/60 - Batch 1000/1875 - Loss: 0.0573",
    "Epoch 21/60 - Batch 1100/1875 - Loss: 0.0668",
    "Epoch 21/60 - Batch 1200/1875 - Loss: 0.0004",
    "Epoch 21/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 21/60 - Batch 1400/1875 - Loss: 0.0159",
    "Epoch 21/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 21/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 21/60 - Batch 1700/1875 - Loss: 0.0064",
    "Epoch 21/60 - Batch 1800/1875 - Loss: 0.0017",
    "Epoch 21 completed - Train Acc: 99.29% - Val Acc: 97.84%",
    "Epoch 22/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 22/60 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 22/60 - Batch 200/1875 - Loss: 0.0022",
    "Epoch 22/60 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 22/60 - Batch 400/1875 - Loss: 0.0832",
    "Epoch 22/60 - Batch 500/1875 - Loss: 0.0225",
    "Epoch 22/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 22/60 - Batch 700/1875 - Loss: 0.0102",
    "Epoch 22/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 22/60 - Batch 900/1875 - Loss: 0.0038",
    "Epoch 22/60 - Batch 1000/1875 - Loss: 0.0421",
    "Epoch 22/60 - Batch 1100/1875 - Loss: 0.1437",
    "Epoch 22/60 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 22/60 - Batch 1300/1875 - Loss: 0.0871",
    "Epoch 22/60 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 22/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 22/60 - Batch 1600/1875 - Loss: 0.3474",
    "Epoch 22/60 - Batch 1700/1875 - Loss: 0.0029",
    "Epoch 22/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 22 completed - Train Acc: 99.31% - Val Acc: 97.98%",
    "Epoch 23/60 - Batch 0/1875 - Loss: 0.0064",
    "Epoch 23/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 23/60 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 23/60 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 23/60 - Batch 400/1875 - Loss: 0.0196",
    "Epoch 23/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 23/60 - Batch 600/1875 - Loss: 0.0702",
    "Epoch 23/60 - Batch 700/1875 - Loss: 0.0146",
    "Epoch 23/60 - Batch 800/1875 - Loss: 0.0027",
    "Epoch 23/60 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 23/60 - Batch 1000/1875 - Loss: 0.0035",
    "Epoch 23/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 23/60 - Batch 1200/1875 - Loss: 0.0260",
    "Epoch 23/60 - Batch 1300/1875 - Loss: 0.0080",
    "Epoch 23/60 - Batch 1400/1875 - Loss: 0.0234",
    "Epoch 23/60 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 23/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 23/60 - Batch 1700/1875 - Loss: 0.0018",
    "Epoch 23/60 - Batch 1800/1875 - Loss: 0.0050",
    "Epoch 23 completed - Train Acc: 99.27% - Val Acc: 98.06%",
    "Epoch 24/60 - Batch 0/1875 - Loss: 0.0931",
    "Epoch 24/60 - Batch 100/1875 - Loss: 0.0076",
    "Epoch 24/60 - Batch 200/1875 - Loss: 0.0259",
    "Epoch 24/60 - Batch 300/1875 - Loss: 0.0363",
    "Epoch 24/60 - Batch 400/1875 - Loss: 0.0075",
    "Epoch 24/60 - Batch 500/1875 - Loss: 0.0215",
    "Epoch 24/60 - Batch 600/1875 - Loss: 0.0010",
    "Epoch 24/60 - Batch 700/1875 - Loss: 0.0409",
    "Epoch 24/60 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 24/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 24/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 24/60 - Batch 1100/1875 - Loss: 0.0020",
    "Epoch 24/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 24/60 - Batch 1300/1875 - Loss: 0.0107",
    "Epoch 24/60 - Batch 1400/1875 - Loss: 0.0402",
    "Epoch 24/60 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 24/60 - Batch 1600/1875 - Loss: 0.0166",
    "Epoch 24/60 - Batch 1700/1875 - Loss: 0.0112",
    "Epoch 24/60 - Batch 1800/1875 - Loss: 0.0006",
    "Epoch 24 completed - Train Acc: 99.25% - Val Acc: 98.06%",
    "Epoch 25/60 - Batch 0/1875 - Loss: 0.0144",
    "Epoch 25/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 25/60 - Batch 200/1875 - Loss: 0.0042",
    "Epoch 25/60 - Batch 300/1875 - Loss: 0.1113",
    "Epoch 25/60 - Batch 400/1875 - Loss: 0.0131",
    "Epoch 25/60 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 25/60 - Batch 600/1875 - Loss: 0.0816",
    "Epoch 25/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 25/60 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 25/60 - Batch 900/1875 - Loss: 0.0091",
    "Epoch 25/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 25/60 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 25/60 - Batch 1200/1875 - Loss: 0.0214",
    "Epoch 25/60 - Batch 1300/1875 - Loss: 0.0028",
    "Epoch 25/60 - Batch 1400/1875 - Loss: 0.0856",
    "Epoch 25/60 - Batch 1500/1875 - Loss: 0.0644",
    "Epoch 25/60 - Batch 1600/1875 - Loss: 0.0022",
    "Epoch 25/60 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 25/60 - Batch 1800/1875 - Loss: 0.0760",
    "Epoch 25 completed - Train Acc: 99.33% - Val Acc: 97.83%",
    "Epoch 26/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 26/60 - Batch 100/1875 - Loss: 0.0009",
    "Epoch 26/60 - Batch 200/1875 - Loss: 0.0008",
    "Epoch 26/60 - Batch 300/1875 - Loss: 0.0563",
    "Epoch 26/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 26/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 26/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 26/60 - Batch 700/1875 - Loss: 0.2494",
    "Epoch 26/60 - Batch 800/1875 - Loss: 0.0050",
    "Epoch 26/60 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 26/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 26/60 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 26/60 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 26/60 - Batch 1300/1875 - Loss: 0.0158",
    "Epoch 26/60 - Batch 1400/1875 - Loss: 0.0100",
    "Epoch 26/60 - Batch 1500/1875 - Loss: 0.1430",
    "Epoch 26/60 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 26/60 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 26/60 - Batch 1800/1875 - Loss: 0.1717",
    "Epoch 26 completed - Train Acc: 99.37% - Val Acc: 98.04%",
    "Epoch 27/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 27/60 - Batch 100/1875 - Loss: 0.0051",
    "Epoch 27/60 - Batch 200/1875 - Loss: 0.0148",
    "Epoch 27/60 - Batch 300/1875 - Loss: 0.2435",
    "Epoch 27/60 - Batch 400/1875 - Loss: 0.0374",
    "Epoch 27/60 - Batch 500/1875 - Loss: 0.3050",
    "Epoch 27/60 - Batch 600/1875 - Loss: 0.0079",
    "Epoch 27/60 - Batch 700/1875 - Loss: 0.0420",
    "Epoch 27/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 27/60 - Batch 900/1875 - Loss: 0.2166",
    "Epoch 27/60 - Batch 1000/1875 - Loss: 0.0535",
    "Epoch 27/60 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 27/60 - Batch 1200/1875 - Loss: 0.0018",
    "Epoch 27/60 - Batch 1300/1875 - Loss: 0.0596",
    "Epoch 27/60 - Batch 1400/1875 - Loss: 0.3971",
    "Epoch 27/60 - Batch 1500/1875 - Loss: 0.0055",
    "Epoch 27/60 - Batch 1600/1875 - Loss: 0.0064",
    "Epoch 27/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 27/60 - Batch 1800/1875 - Loss: 0.2431",
    "Epoch 27 completed - Train Acc: 99.31% - Val Acc: 97.76%",
    "Epoch 28/60 - Batch 0/1875 - Loss: 0.0040",
    "Epoch 28/60 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 28/60 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 28/60 - Batch 300/1875 - Loss: 0.3544",
    "Epoch 28/60 - Batch 400/1875 - Loss: 0.0069",
    "Epoch 28/60 - Batch 500/1875 - Loss: 0.0640",
    "Epoch 28/60 - Batch 600/1875 - Loss: 0.0029",
    "Epoch 28/60 - Batch 700/1875 - Loss: 0.0029",
    "Epoch 28/60 - Batch 800/1875 - Loss: 0.0863",
    "Epoch 28/60 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 28/60 - Batch 1000/1875 - Loss: 0.0035",
    "Epoch 28/60 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 28/60 - Batch 1200/1875 - Loss: 0.0155",
    "Epoch 28/60 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 28/60 - Batch 1400/1875 - Loss: 0.0413",
    "Epoch 28/60 - Batch 1500/1875 - Loss: 0.0027",
    "Epoch 28/60 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 28/60 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 28/60 - Batch 1800/1875 - Loss: 0.0662",
    "Epoch 28 completed - Train Acc: 99.33% - Val Acc: 98.02%",
    "Epoch 29/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 29/60 - Batch 100/1875 - Loss: 0.0122",
    "Epoch 29/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 29/60 - Batch 300/1875 - Loss: 0.0007",
    "Epoch 29/60 - Batch 400/1875 - Loss: 0.0053",
    "Epoch 29/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 29/60 - Batch 600/1875 - Loss: 0.0017",
    "Epoch 29/60 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 29/60 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 29/60 - Batch 900/1875 - Loss: 0.4214",
    "Epoch 29/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 29/60 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 29/60 - Batch 1200/1875 - Loss: 0.0784",
    "Epoch 29/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 29/60 - Batch 1400/1875 - Loss: 0.1121",
    "Epoch 29/60 - Batch 1500/1875 - Loss: 0.0006",
    "Epoch 29/60 - Batch 1600/1875 - Loss: 0.0518",
    "Epoch 29/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 29/60 - Batch 1800/1875 - Loss: 0.0316",
    "Epoch 29 completed - Train Acc: 99.34% - Val Acc: 97.87%",
    "Epoch 30/60 - Batch 0/1875 - Loss: 0.0069",
    "Epoch 30/60 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 30/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 30/60 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 30/60 - Batch 400/1875 - Loss: 0.1242",
    "Epoch 30/60 - Batch 500/1875 - Loss: 0.0008",
    "Epoch 30/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 30/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 30/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 30/60 - Batch 900/1875 - Loss: 0.0178",
    "Epoch 30/60 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 30/60 - Batch 1100/1875 - Loss: 0.0873",
    "Epoch 30/60 - Batch 1200/1875 - Loss: 0.0346",
    "Epoch 30/60 - Batch 1300/1875 - Loss: 0.0011",
    "Epoch 30/60 - Batch 1400/1875 - Loss: 0.0045",
    "Epoch 30/60 - Batch 1500/1875 - Loss: 0.0351",
    "Epoch 30/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 30/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 30/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 30 completed - Train Acc: 99.40% - Val Acc: 98.54%",
    "Epoch 31/60 - Batch 0/1875 - Loss: 0.0029",
    "Epoch 31/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 31/60 - Batch 200/1875 - Loss: 0.0055",
    "Epoch 31/60 - Batch 300/1875 - Loss: 0.0120",
    "Epoch 31/60 - Batch 400/1875 - Loss: 0.0018",
    "Epoch 31/60 - Batch 500/1875 - Loss: 0.0018",
    "Epoch 31/60 - Batch 600/1875 - Loss: 0.0027",
    "Epoch 31/60 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 31/60 - Batch 800/1875 - Loss: 0.0071",
    "Epoch 31/60 - Batch 900/1875 - Loss: 0.0133",
    "Epoch 31/60 - Batch 1000/1875 - Loss: 0.0041",
    "Epoch 31/60 - Batch 1100/1875 - Loss: 0.0239",
    "Epoch 31/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 31/60 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 31/60 - Batch 1400/1875 - Loss: 0.0019",
    "Epoch 31/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 31/60 - Batch 1600/1875 - Loss: 0.0009",
    "Epoch 31/60 - Batch 1700/1875 - Loss: 0.0019",
    "Epoch 31/60 - Batch 1800/1875 - Loss: 0.0612",
    "Epoch 31 completed - Train Acc: 99.25% - Val Acc: 97.86%",
    "Epoch 32/60 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 32/60 - Batch 100/1875 - Loss: 0.0116",
    "Epoch 32/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 300/1875 - Loss: 0.0025",
    "Epoch 32/60 - Batch 400/1875 - Loss: 0.0315",
    "Epoch 32/60 - Batch 500/1875 - Loss: 0.0021",
    "Epoch 32/60 - Batch 600/1875 - Loss: 0.0333",
    "Epoch 32/60 - Batch 700/1875 - Loss: 0.0040",
    "Epoch 32/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 32/60 - Batch 900/1875 - Loss: 0.0026",
    "Epoch 32/60 - Batch 1000/1875 - Loss: 0.0019",
    "Epoch 32/60 - Batch 1100/1875 - Loss: 0.0012",
    "Epoch 32/60 - Batch 1200/1875 - Loss: 0.0114",
    "Epoch 32/60 - Batch 1300/1875 - Loss: 0.3059",
    "Epoch 32/60 - Batch 1400/1875 - Loss: 0.0348",
    "Epoch 32/60 - Batch 1500/1875 - Loss: 0.0017",
    "Epoch 32/60 - Batch 1600/1875 - Loss: 0.0016",
    "Epoch 32/60 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 32/60 - Batch 1800/1875 - Loss: 0.0165",
    "Epoch 32 completed - Train Acc: 99.43% - Val Acc: 97.98%",
    "Epoch 33/60 - Batch 0/1875 - Loss: 0.0244",
    "Epoch 33/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 33/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 33/60 - Batch 300/1875 - Loss: 0.0802",
    "Epoch 33/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 33/60 - Batch 500/1875 - Loss: 0.0120",
    "Epoch 33/60 - Batch 600/1875 - Loss: 0.1878",
    "Epoch 33/60 - Batch 700/1875 - Loss: 0.0071",
    "Epoch 33/60 - Batch 800/1875 - Loss: 0.0388",
    "Epoch 33/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 33/60 - Batch 1000/1875 - Loss: 0.1234",
    "Epoch 33/60 - Batch 1100/1875 - Loss: 0.0464",
    "Epoch 33/60 - Batch 1200/1875 - Loss: 0.1092",
    "Epoch 33/60 - Batch 1300/1875 - Loss: 0.0011",
    "Epoch 33/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 33/60 - Batch 1500/1875 - Loss: 0.0014",
    "Epoch 33/60 - Batch 1600/1875 - Loss: 0.0009",
    "Epoch 33/60 - Batch 1700/1875 - Loss: 0.0671",
    "Epoch 33/60 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 33 completed - Train Acc: 99.42% - Val Acc: 97.97%",
    "Epoch 34/60 - Batch 0/1875 - Loss: 0.0344",
    "Epoch 34/60 - Batch 100/1875 - Loss: 0.0210",
    "Epoch 34/60 - Batch 200/1875 - Loss: 0.0586",
    "Epoch 34/60 - Batch 300/1875 - Loss: 0.0027",
    "Epoch 34/60 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 34/60 - Batch 500/1875 - Loss: 0.0012",
    "Epoch 34/60 - Batch 600/1875 - Loss: 0.0073",
    "Epoch 34/60 - Batch 700/1875 - Loss: 0.0170",
    "Epoch 34/60 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 34/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 34/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 34/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 34/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 34/60 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 34/60 - Batch 1400/1875 - Loss: 0.0005",
    "Epoch 34/60 - Batch 1500/1875 - Loss: 0.0087",
    "Epoch 34/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 34/60 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 34/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 34 completed - Train Acc: 99.44% - Val Acc: 98.15%",
    "Epoch 35/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 35/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 35/60 - Batch 200/1875 - Loss: 0.0186",
    "Epoch 35/60 - Batch 300/1875 - Loss: 0.0048",
    "Epoch 35/60 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 35/60 - Batch 500/1875 - Loss: 0.0140",
    "Epoch 35/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 35/60 - Batch 700/1875 - Loss: 0.0057",
    "Epoch 35/60 - Batch 800/1875 - Loss: 0.0008",
    "Epoch 35/60 - Batch 900/1875 - Loss: 0.0029",
    "Epoch 35/60 - Batch 1000/1875 - Loss: 0.0044",
    "Epoch 35/60 - Batch 1100/1875 - Loss: 0.0007",
    "Epoch 35/60 - Batch 1200/1875 - Loss: 0.0132",
    "Epoch 35/60 - Batch 1300/1875 - Loss: 0.0103",
    "Epoch 35/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 35/60 - Batch 1500/1875 - Loss: 0.2160",
    "Epoch 35/60 - Batch 1600/1875 - Loss: 0.2609",
    "Epoch 35/60 - Batch 1700/1875 - Loss: 0.0012",
    "Epoch 35/60 - Batch 1800/1875 - Loss: 0.0017",
    "Epoch 35 completed - Train Acc: 99.36% - Val Acc: 97.97%",
    "Epoch 36/60 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 36/60 - Batch 100/1875 - Loss: 0.0014",
    "Epoch 36/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 36/60 - Batch 300/1875 - Loss: 0.0047",
    "Epoch 36/60 - Batch 400/1875 - Loss: 0.0218",
    "Epoch 36/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 700/1875 - Loss: 0.0021",
    "Epoch 36/60 - Batch 800/1875 - Loss: 0.0127",
    "Epoch 36/60 - Batch 900/1875 - Loss: 0.0034",
    "Epoch 36/60 - Batch 1000/1875 - Loss: 0.0637",
    "Epoch 36/60 - Batch 1100/1875 - Loss: 0.0014",
    "Epoch 36/60 - Batch 1200/1875 - Loss: 0.0043",
    "Epoch 36/60 - Batch 1300/1875 - Loss: 0.0511",
    "Epoch 36/60 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 36/60 - Batch 1500/1875 - Loss: 0.0102",
    "Epoch 36/60 - Batch 1600/1875 - Loss: 0.0075",
    "Epoch 36/60 - Batch 1700/1875 - Loss: 0.2388",
    "Epoch 36/60 - Batch 1800/1875 - Loss: 0.1908",
    "Epoch 36 completed - Train Acc: 99.40% - Val Acc: 98.13%",
    "Epoch 37/60 - Batch 0/1875 - Loss: 0.0177",
    "Epoch 37/60 - Batch 100/1875 - Loss: 0.1518",
    "Epoch 37/60 - Batch 200/1875 - Loss: 0.0719",
    "Epoch 37/60 - Batch 300/1875 - Loss: 0.0487",
    "Epoch 37/60 - Batch 400/1875 - Loss: 0.0196",
    "Epoch 37/60 - Batch 500/1875 - Loss: 0.0023",
    "Epoch 37/60 - Batch 600/1875 - Loss: 0.0067",
    "Epoch 37/60 - Batch 700/1875 - Loss: 0.0046",
    "Epoch 37/60 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 37/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 37/60 - Batch 1000/1875 - Loss: 0.0283",
    "Epoch 37/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 37/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 1400/1875 - Loss: 0.0066",
    "Epoch 37/60 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 37/60 - Batch 1600/1875 - Loss: 0.0897",
    "Epoch 37/60 - Batch 1700/1875 - Loss: 0.0349",
    "Epoch 37/60 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 37 completed - Train Acc: 99.45% - Val Acc: 98.15%",
    "Epoch 38/60 - Batch 0/1875 - Loss: 0.0179",
    "Epoch 38/60 - Batch 100/1875 - Loss: 0.0026",
    "Epoch 38/60 - Batch 200/1875 - Loss: 0.0091",
    "Epoch 38/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 38/60 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 38/60 - Batch 500/1875 - Loss: 0.0114",
    "Epoch 38/60 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 38/60 - Batch 700/1875 - Loss: 0.0022",
    "Epoch 38/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 900/1875 - Loss: 0.1079",
    "Epoch 38/60 - Batch 1000/1875 - Loss: 0.0005",
    "Epoch 38/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 38/60 - Batch 1300/1875 - Loss: 0.0023",
    "Epoch 38/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 38/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 1700/1875 - Loss: 0.0341",
    "Epoch 38/60 - Batch 1800/1875 - Loss: 0.0335",
    "Epoch 38 completed - Train Acc: 99.42% - Val Acc: 97.84%",
    "Epoch 39/60 - Batch 0/1875 - Loss: 0.0063",
    "Epoch 39/60 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 39/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 300/1875 - Loss: 0.0043",
    "Epoch 39/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 39/60 - Batch 500/1875 - Loss: 0.0189",
    "Epoch 39/60 - Batch 600/1875 - Loss: 0.0031",
    "Epoch 39/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 800/1875 - Loss: 0.0166",
    "Epoch 39/60 - Batch 900/1875 - Loss: 0.0058",
    "Epoch 39/60 - Batch 1000/1875 - Loss: 0.0026",
    "Epoch 39/60 - Batch 1100/1875 - Loss: 0.0075",
    "Epoch 39/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 39/60 - Batch 1400/1875 - Loss: 0.0361",
    "Epoch 39/60 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 39/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 39/60 - Batch 1800/1875 - Loss: 0.0066",
    "Epoch 39 completed - Train Acc: 99.51% - Val Acc: 97.59%",
    "Epoch 40/60 - Batch 0/1875 - Loss: 0.0031",
    "Epoch 40/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 40/60 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 40/60 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 40/60 - Batch 400/1875 - Loss: 0.0985",
    "Epoch 40/60 - Batch 500/1875 - Loss: 0.0017",
    "Epoch 40/60 - Batch 600/1875 - Loss: 0.0021",
    "Epoch 40/60 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 40/60 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 40/60 - Batch 900/1875 - Loss: 0.0012",
    "Epoch 40/60 - Batch 1000/1875 - Loss: 0.0008",
    "Epoch 40/60 - Batch 1100/1875 - Loss: 0.0049",
    "Epoch 40/60 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 40/60 - Batch 1300/1875 - Loss: 0.0236",
    "Epoch 40/60 - Batch 1400/1875 - Loss: 0.1332",
    "Epoch 40/60 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 40/60 - Batch 1600/1875 - Loss: 0.0023",
    "Epoch 40/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 1800/1875 - Loss: 0.1092",
    "Epoch 40 completed - Train Acc: 99.40% - Val Acc: 97.67%",
    "Epoch 41/60 - Batch 0/1875 - Loss: 0.0018",
    "Epoch 41/60 - Batch 100/1875 - Loss: 0.2569",
    "Epoch 41/60 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 41/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 41/60 - Batch 400/1875 - Loss: 0.0274",
    "Epoch 41/60 - Batch 500/1875 - Loss: 0.0875",
    "Epoch 41/60 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 41/60 - Batch 700/1875 - Loss: 0.0073",
    "Epoch 41/60 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 41/60 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 41/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 41/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 41/60 - Batch 1200/1875 - Loss: 0.0725",
    "Epoch 41/60 - Batch 1300/1875 - Loss: 0.1472",
    "Epoch 41/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 41/60 - Batch 1500/1875 - Loss: 0.0030",
    "Epoch 41/60 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 41/60 - Batch 1700/1875 - Loss: 0.0240",
    "Epoch 41/60 - Batch 1800/1875 - Loss: 0.0086",
    "Epoch 41 completed - Train Acc: 99.47% - Val Acc: 97.57%",
    "Epoch 42/60 - Batch 0/1875 - Loss: 0.0793",
    "Epoch 42/60 - Batch 100/1875 - Loss: 0.0050",
    "Epoch 42/60 - Batch 200/1875 - Loss: 0.0016",
    "Epoch 42/60 - Batch 300/1875 - Loss: 0.0050",
    "Epoch 42/60 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 42/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 42/60 - Batch 600/1875 - Loss: 0.0043",
    "Epoch 42/60 - Batch 700/1875 - Loss: 0.0667",
    "Epoch 42/60 - Batch 800/1875 - Loss: 0.0027",
    "Epoch 42/60 - Batch 900/1875 - Loss: 0.0026",
    "Epoch 42/60 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 42/60 - Batch 1100/1875 - Loss: 0.0217",
    "Epoch 42/60 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 42/60 - Batch 1300/1875 - Loss: 0.0047",
    "Epoch 42/60 - Batch 1400/1875 - Loss: 0.0759",
    "Epoch 42/60 - Batch 1500/1875 - Loss: 0.0088",
    "Epoch 42/60 - Batch 1600/1875 - Loss: 0.0043",
    "Epoch 42/60 - Batch 1700/1875 - Loss: 0.1417",
    "Epoch 42/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 42 completed - Train Acc: 99.36% - Val Acc: 98.24%",
    "Epoch 43/60 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 43/60 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 43/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 43/60 - Batch 300/1875 - Loss: 0.1271",
    "Epoch 43/60 - Batch 400/1875 - Loss: 0.0020",
    "Epoch 43/60 - Batch 500/1875 - Loss: 0.0123",
    "Epoch 43/60 - Batch 600/1875 - Loss: 0.0583",
    "Epoch 43/60 - Batch 700/1875 - Loss: 0.0029",
    "Epoch 43/60 - Batch 800/1875 - Loss: 0.0116",
    "Epoch 43/60 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 43/60 - Batch 1000/1875 - Loss: 0.0016",
    "Epoch 43/60 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 43/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 43/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 43/60 - Batch 1400/1875 - Loss: 0.0117",
    "Epoch 43/60 - Batch 1500/1875 - Loss: 0.0094",
    "Epoch 43/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 43/60 - Batch 1700/1875 - Loss: 0.0012",
    "Epoch 43/60 - Batch 1800/1875 - Loss: 0.0008",
    "Epoch 43 completed - Train Acc: 99.50% - Val Acc: 97.97%",
    "Epoch 44/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 44/60 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 44/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 44/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 44/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 44/60 - Batch 500/1875 - Loss: 0.0391",
    "Epoch 44/60 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 44/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 44/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 44/60 - Batch 900/1875 - Loss: 0.0031",
    "Epoch 44/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 44/60 - Batch 1100/1875 - Loss: 0.0052",
    "Epoch 44/60 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 44/60 - Batch 1300/1875 - Loss: 0.0090",
    "Epoch 44/60 - Batch 1400/1875 - Loss: 0.0008",
    "Epoch 44/60 - Batch 1500/1875 - Loss: 0.0851",
    "Epoch 44/60 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 44/60 - Batch 1700/1875 - Loss: 0.0815",
    "Epoch 44/60 - Batch 1800/1875 - Loss: 0.0425",
    "Epoch 44 completed - Train Acc: 99.45% - Val Acc: 97.13%",
    "Epoch 45/60 - Batch 0/1875 - Loss: 0.0616",
    "Epoch 45/60 - Batch 100/1875 - Loss: 0.0111",
    "Epoch 45/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 45/60 - Batch 300/1875 - Loss: 0.0069",
    "Epoch 45/60 - Batch 400/1875 - Loss: 0.0029",
    "Epoch 45/60 - Batch 500/1875 - Loss: 0.0338",
    "Epoch 45/60 - Batch 600/1875 - Loss: 0.0154",
    "Epoch 45/60 - Batch 700/1875 - Loss: 0.2611",
    "Epoch 45/60 - Batch 800/1875 - Loss: 0.0225",
    "Epoch 45/60 - Batch 900/1875 - Loss: 0.0014",
    "Epoch 45/60 - Batch 1000/1875 - Loss: 0.1062",
    "Epoch 45/60 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 45/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 45/60 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 45/60 - Batch 1400/1875 - Loss: 0.0010",
    "Epoch 45/60 - Batch 1500/1875 - Loss: 0.0025",
    "Epoch 45/60 - Batch 1600/1875 - Loss: 0.0047",
    "Epoch 45/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 1800/1875 - Loss: 0.0610",
    "Epoch 45 completed - Train Acc: 99.48% - Val Acc: 98.03%",
    "Epoch 46/60 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 46/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 46/60 - Batch 200/1875 - Loss: 0.0017",
    "Epoch 46/60 - Batch 300/1875 - Loss: 0.0034",
    "Epoch 46/60 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 46/60 - Batch 500/1875 - Loss: 0.0006",
    "Epoch 46/60 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 46/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 46/60 - Batch 800/1875 - Loss: 0.0191",
    "Epoch 46/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 46/60 - Batch 1000/1875 - Loss: 0.0025",
    "Epoch 46/60 - Batch 1100/1875 - Loss: 0.0026",
    "Epoch 46/60 - Batch 1200/1875 - Loss: 0.0036",
    "Epoch 46/60 - Batch 1300/1875 - Loss: 0.0060",
    "Epoch 46/60 - Batch 1400/1875 - Loss: 0.0057",
    "Epoch 46/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 46/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 46/60 - Batch 1700/1875 - Loss: 0.0051",
    "Epoch 46/60 - Batch 1800/1875 - Loss: 0.0894",
    "Epoch 46 completed - Train Acc: 99.44% - Val Acc: 98.01%",
    "Epoch 47/60 - Batch 0/1875 - Loss: 0.0027",
    "Epoch 47/60 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 47/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 47/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 47/60 - Batch 600/1875 - Loss: 0.0017",
    "Epoch 47/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 1000/1875 - Loss: 0.0168",
    "Epoch 47/60 - Batch 1100/1875 - Loss: 0.0080",
    "Epoch 47/60 - Batch 1200/1875 - Loss: 0.0087",
    "Epoch 47/60 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 47/60 - Batch 1400/1875 - Loss: 0.0085",
    "Epoch 47/60 - Batch 1500/1875 - Loss: 0.0074",
    "Epoch 47/60 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 47/60 - Batch 1700/1875 - Loss: 0.1028",
    "Epoch 47/60 - Batch 1800/1875 - Loss: 0.0006",
    "Epoch 47 completed - Train Acc: 99.45% - Val Acc: 98.09%",
    "Epoch 48/60 - Batch 0/1875 - Loss: 0.0075",
    "Epoch 48/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 48/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 48/60 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 48/60 - Batch 500/1875 - Loss: 0.0067",
    "Epoch 48/60 - Batch 600/1875 - Loss: 0.0018",
    "Epoch 48/60 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 48/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 900/1875 - Loss: 0.0103",
    "Epoch 48/60 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 48/60 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 48/60 - Batch 1200/1875 - Loss: 0.0027",
    "Epoch 48/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 48/60 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 48/60 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 48/60 - Batch 1600/1875 - Loss: 0.0027",
    "Epoch 48/60 - Batch 1700/1875 - Loss: 0.0039",
    "Epoch 48/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 48 completed - Train Acc: 99.53% - Val Acc: 97.98%",
    "Epoch 49/60 - Batch 0/1875 - Loss: 0.0080",
    "Epoch 49/60 - Batch 100/1875 - Loss: 0.0010",
    "Epoch 49/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 49/60 - Batch 300/1875 - Loss: 0.0021",
    "Epoch 49/60 - Batch 400/1875 - Loss: 0.0006",
    "Epoch 49/60 - Batch 500/1875 - Loss: 0.0063",
    "Epoch 49/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 49/60 - Batch 700/1875 - Loss: 0.0058",
    "Epoch 49/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 49/60 - Batch 900/1875 - Loss: 0.4990",
    "Epoch 49/60 - Batch 1000/1875 - Loss: 0.0008",
    "Epoch 49/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 49/60 - Batch 1200/1875 - Loss: 0.0061",
    "Epoch 49/60 - Batch 1300/1875 - Loss: 0.0011",
    "Epoch 49/60 - Batch 1400/1875 - Loss: 0.0017",
    "Epoch 49/60 - Batch 1500/1875 - Loss: 0.0010",
    "Epoch 49/60 - Batch 1600/1875 - Loss: 0.0251",
    "Epoch 49/60 - Batch 1700/1875 - Loss: 0.0609",
    "Epoch 49/60 - Batch 1800/1875 - Loss: 0.0120",
    "Epoch 49 completed - Train Acc: 99.45% - Val Acc: 98.18%",
    "Epoch 50/60 - Batch 0/1875 - Loss: 0.0006",
    "Epoch 50/60 - Batch 100/1875 - Loss: 0.0065",
    "Epoch 50/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 300/1875 - Loss: 0.0030",
    "Epoch 50/60 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 50/60 - Batch 500/1875 - Loss: 0.0008",
    "Epoch 50/60 - Batch 600/1875 - Loss: 0.0142",
    "Epoch 50/60 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 50/60 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 50/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 50/60 - Batch 1000/1875 - Loss: 0.0040",
    "Epoch 50/60 - Batch 1100/1875 - Loss: 0.1115",
    "Epoch 50/60 - Batch 1200/1875 - Loss: 0.0046",
    "Epoch 50/60 - Batch 1300/1875 - Loss: 0.0038",
    "Epoch 50/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 1500/1875 - Loss: 0.0031",
    "Epoch 50/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 1700/1875 - Loss: 0.0015",
    "Epoch 50/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 50 completed - Train Acc: 99.50% - Val Acc: 96.81%",
    "Epoch 51/60 - Batch 0/1875 - Loss: 0.0261",
    "Epoch 51/60 - Batch 100/1875 - Loss: 0.0401",
    "Epoch 51/60 - Batch 200/1875 - Loss: 0.0029",
    "Epoch 51/60 - Batch 300/1875 - Loss: 0.0040",
    "Epoch 51/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 500/1875 - Loss: 0.0463",
    "Epoch 51/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 51/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 51/60 - Batch 900/1875 - Loss: 0.0014",
    "Epoch 51/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 1100/1875 - Loss: 0.0032",
    "Epoch 51/60 - Batch 1200/1875 - Loss: 0.0332",
    "Epoch 51/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 51/60 - Batch 1500/1875 - Loss: 0.0026",
    "Epoch 51/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 51/60 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 51/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 51 completed - Train Acc: 99.45% - Val Acc: 98.02%",
    "Epoch 52/60 - Batch 0/1875 - Loss: 0.0037",
    "Epoch 52/60 - Batch 100/1875 - Loss: 0.0106",
    "Epoch 52/60 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 52/60 - Batch 300/1875 - Loss: 0.0027",
    "Epoch 52/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 52/60 - Batch 500/1875 - Loss: 0.0015",
    "Epoch 52/60 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 52/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 52/60 - Batch 900/1875 - Loss: 0.0092",
    "Epoch 52/60 - Batch 1000/1875 - Loss: 0.0038",
    "Epoch 52/60 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 52/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 52/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 52/60 - Batch 1400/1875 - Loss: 0.0410",
    "Epoch 52/60 - Batch 1500/1875 - Loss: 0.1087",
    "Epoch 52/60 - Batch 1600/1875 - Loss: 0.0341",
    "Epoch 52/60 - Batch 1700/1875 - Loss: 0.1027",
    "Epoch 52/60 - Batch 1800/1875 - Loss: 0.1878",
    "Epoch 52 completed - Train Acc: 99.50% - Val Acc: 98.01%",
    "Epoch 53/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 53/60 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 53/60 - Batch 200/1875 - Loss: 0.0650",
    "Epoch 53/60 - Batch 300/1875 - Loss: 0.0669",
    "Epoch 53/60 - Batch 400/1875 - Loss: 0.0089",
    "Epoch 53/60 - Batch 500/1875 - Loss: 0.0075",
    "Epoch 53/60 - Batch 600/1875 - Loss: 0.0796",
    "Epoch 53/60 - Batch 700/1875 - Loss: 0.0104",
    "Epoch 53/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 53/60 - Batch 900/1875 - Loss: 0.0121",
    "Epoch 53/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 53/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 53/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 53/60 - Batch 1400/1875 - Loss: 0.0074",
    "Epoch 53/60 - Batch 1500/1875 - Loss: 0.0444",
    "Epoch 53/60 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 53/60 - Batch 1700/1875 - Loss: 0.0010",
    "Epoch 53/60 - Batch 1800/1875 - Loss: 0.0216",
    "Epoch 53 completed - Train Acc: 99.39% - Val Acc: 98.04%",
    "Epoch 54/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 54/60 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 54/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 54/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 54/60 - Batch 500/1875 - Loss: 0.1073",
    "Epoch 54/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 700/1875 - Loss: 0.0010",
    "Epoch 54/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 54/60 - Batch 900/1875 - Loss: 0.0334",
    "Epoch 54/60 - Batch 1000/1875 - Loss: 0.0008",
    "Epoch 54/60 - Batch 1100/1875 - Loss: 0.0024",
    "Epoch 54/60 - Batch 1200/1875 - Loss: 0.0012",
    "Epoch 54/60 - Batch 1300/1875 - Loss: 0.0017",
    "Epoch 54/60 - Batch 1400/1875 - Loss: 0.1323",
    "Epoch 54/60 - Batch 1500/1875 - Loss: 0.0027",
    "Epoch 54/60 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 54/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 1800/1875 - Loss: 0.0008",
    "Epoch 54 completed - Train Acc: 99.49% - Val Acc: 97.78%",
    "Epoch 55/60 - Batch 0/1875 - Loss: 0.0634",
    "Epoch 55/60 - Batch 100/1875 - Loss: 0.0012",
    "Epoch 55/60 - Batch 200/1875 - Loss: 0.0077",
    "Epoch 55/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 400/1875 - Loss: 0.0008",
    "Epoch 55/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 600/1875 - Loss: 0.0211",
    "Epoch 55/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 800/1875 - Loss: 0.0028",
    "Epoch 55/60 - Batch 900/1875 - Loss: 0.0221",
    "Epoch 55/60 - Batch 1000/1875 - Loss: 0.0392",
    "Epoch 55/60 - Batch 1100/1875 - Loss: 0.0218",
    "Epoch 55/60 - Batch 1200/1875 - Loss: 0.0092",
    "Epoch 55/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 55/60 - Batch 1500/1875 - Loss: 0.0008",
    "Epoch 55/60 - Batch 1600/1875 - Loss: 0.1684",
    "Epoch 55/60 - Batch 1700/1875 - Loss: 0.0037",
    "Epoch 55/60 - Batch 1800/1875 - Loss: 0.0020",
    "Epoch 55 completed - Train Acc: 99.48% - Val Acc: 97.73%",
    "Epoch 56/60 - Batch 0/1875 - Loss: 0.0506",
    "Epoch 56/60 - Batch 100/1875 - Loss: 0.0872",
    "Epoch 56/60 - Batch 200/1875 - Loss: 0.0038",
    "Epoch 56/60 - Batch 300/1875 - Loss: 0.0201",
    "Epoch 56/60 - Batch 400/1875 - Loss: 0.0037",
    "Epoch 56/60 - Batch 500/1875 - Loss: 0.0025",
    "Epoch 56/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 56/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 900/1875 - Loss: 0.0012",
    "Epoch 56/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 56/60 - Batch 1100/1875 - Loss: 0.0809",
    "Epoch 56/60 - Batch 1200/1875 - Loss: 0.0025",
    "Epoch 56/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 1500/1875 - Loss: 0.0174",
    "Epoch 56/60 - Batch 1600/1875 - Loss: 0.0772",
    "Epoch 56/60 - Batch 1700/1875 - Loss: 0.0049",
    "Epoch 56/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 56 completed - Train Acc: 99.62% - Val Acc: 98.15%",
    "Epoch 57/60 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 57/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 57/60 - Batch 200/1875 - Loss: 0.1387",
    "Epoch 57/60 - Batch 300/1875 - Loss: 0.0896",
    "Epoch 57/60 - Batch 400/1875 - Loss: 0.0099",
    "Epoch 57/60 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 57/60 - Batch 600/1875 - Loss: 0.0017",
    "Epoch 57/60 - Batch 700/1875 - Loss: 0.0019",
    "Epoch 57/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 900/1875 - Loss: 0.0851",
    "Epoch 57/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 57/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1300/1875 - Loss: 0.0030",
    "Epoch 57/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1500/1875 - Loss: 0.0128",
    "Epoch 57/60 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 57/60 - Batch 1700/1875 - Loss: 0.0030",
    "Epoch 57/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 57 completed - Train Acc: 99.42% - Val Acc: 97.73%",
    "Epoch 58/60 - Batch 0/1875 - Loss: 0.0121",
    "Epoch 58/60 - Batch 100/1875 - Loss: 0.0441",
    "Epoch 58/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 58/60 - Batch 300/1875 - Loss: 0.0042",
    "Epoch 58/60 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 58/60 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 58/60 - Batch 600/1875 - Loss: 0.0088",
    "Epoch 58/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 58/60 - Batch 800/1875 - Loss: 0.0381",
    "Epoch 58/60 - Batch 900/1875 - Loss: 0.0941",
    "Epoch 58/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 58/60 - Batch 1100/1875 - Loss: 0.0082",
    "Epoch 58/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 58/60 - Batch 1300/1875 - Loss: 0.0173",
    "Epoch 58/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 58/60 - Batch 1500/1875 - Loss: 0.0029",
    "Epoch 58/60 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 58/60 - Batch 1700/1875 - Loss: 0.0050",
    "Epoch 58/60 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 58 completed - Train Acc: 99.57% - Val Acc: 98.00%",
    "Epoch 59/60 - Batch 0/1875 - Loss: 0.0049",
    "Epoch 59/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 200/1875 - Loss: 0.0005",
    "Epoch 59/60 - Batch 300/1875 - Loss: 0.0531",
    "Epoch 59/60 - Batch 400/1875 - Loss: 0.1282",
    "Epoch 59/60 - Batch 500/1875 - Loss: 0.0661",
    "Epoch 59/60 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 59/60 - Batch 700/1875 - Loss: 0.0232",
    "Epoch 59/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 59/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 59/60 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 59/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 59/60 - Batch 1400/1875 - Loss: 0.0689",
    "Epoch 59/60 - Batch 1500/1875 - Loss: 0.0596",
    "Epoch 59/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 1700/1875 - Loss: 0.0517",
    "Epoch 59/60 - Batch 1800/1875 - Loss: 0.0018",
    "Epoch 59 completed - Train Acc: 99.41% - Val Acc: 97.99%",
    "Epoch 60/60 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 60/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 60/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 60/60 - Batch 800/1875 - Loss: 0.0567",
    "Epoch 60/60 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 60/60 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 60/60 - Batch 1100/1875 - Loss: 0.0394",
    "Epoch 60/60 - Batch 1200/1875 - Loss: 0.5227",
    "Epoch 60/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 60/60 - Batch 1400/1875 - Loss: 0.5007",
    "Epoch 60/60 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 60/60 - Batch 1600/1875 - Loss: 0.0025",
    "Epoch 60/60 - Batch 1700/1875 - Loss: 0.0263",
    "Epoch 60/60 - Batch 1800/1875 - Loss: 0.0252",
    "Epoch 60 completed - Train Acc: 99.48% - Val Acc: 97.75%",
    "Training completed. Model saved to models/ZPE-Colab-Sim_hnn_step8.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.28555352091006936,
      "train_accuracy": 91.51333333333334,
      "val_loss": 0.15686515529756062,
      "val_accuracy": 95.57
    },
    {
      "epoch": 2,
      "train_loss": 0.13873042169989397,
      "train_accuracy": 96.23833333333333,
      "val_loss": 0.12742755406700929,
      "val_accuracy": 96.57
    },
    {
      "epoch": 3,
      "train_loss": 0.10289392911697505,
      "train_accuracy": 97.16166666666666,
      "val_loss": 0.12010882839256638,
      "val_accuracy": 96.75
    },
    {
      "epoch": 4,
      "train_loss": 0.09051217392822339,
      "train_accuracy": 97.53166666666667,
      "val_loss": 0.1160540276083722,
      "val_accuracy": 97.04
    },
    {
      "epoch": 5,
      "train_loss": 0.07519451790075594,
      "train_accuracy": 97.98166666666667,
      "val_loss": 0.1036648125754641,
      "val_accuracy": 97.08
    },
    {
      "epoch": 6,
      "train_loss": 0.06615095301134959,
      "train_accuracy": 98.21,
      "val_loss": 0.10999891324972152,
      "val_accuracy": 97.26
    },
    {
      "epoch": 7,
      "train_loss": 0.05965853007447925,
      "train_accuracy": 98.345,
      "val_loss": 0.1006815483966426,
      "val_accuracy": 97.55
    },
    {
      "epoch": 8,
      "train_loss": 0.05766273019260746,
      "train_accuracy": 98.43333333333334,
      "val_loss": 0.12651628512301863,
      "val_accuracy": 97.37
    },
    {
      "epoch": 9,
      "train_loss": 0.04899574707729577,
      "train_accuracy": 98.69833333333334,
      "val_loss": 0.11341947799732902,
      "val_accuracy": 97.53
    },
    {
      "epoch": 10,
      "train_loss": 0.04746369267415624,
      "train_accuracy": 98.68,
      "val_loss": 0.10097270089297414,
      "val_accuracy": 97.59
    },
    {
      "epoch": 11,
      "train_loss": 0.04266958413792484,
      "train_accuracy": 98.87,
      "val_loss": 0.08475884373927375,
      "val_accuracy": 97.98
    },
    {
      "epoch": 12,
      "train_loss": 0.04263113351567851,
      "train_accuracy": 98.885,
      "val_loss": 0.12966319367621534,
      "val_accuracy": 97.15
    },
    {
      "epoch": 13,
      "train_loss": 0.039509801033683595,
      "train_accuracy": 98.935,
      "val_loss": 0.12524480215344594,
      "val_accuracy": 97.5
    },
    {
      "epoch": 14,
      "train_loss": 0.03534668217524583,
      "train_accuracy": 99.08666666666667,
      "val_loss": 0.12555249527613718,
      "val_accuracy": 97.7
    },
    {
      "epoch": 15,
      "train_loss": 0.03782998830260676,
      "train_accuracy": 98.97166666666666,
      "val_loss": 0.11187220714250704,
      "val_accuracy": 97.83
    },
    {
      "epoch": 16,
      "train_loss": 0.03594434655277291,
      "train_accuracy": 99.055,
      "val_loss": 0.1392985007658198,
      "val_accuracy": 97.51
    },
    {
      "epoch": 17,
      "train_loss": 0.03248536455658784,
      "train_accuracy": 99.15666666666667,
      "val_loss": 0.11239386287539505,
      "val_accuracy": 97.37
    },
    {
      "epoch": 18,
      "train_loss": 0.032626321327622304,
      "train_accuracy": 99.12666666666667,
      "val_loss": 0.12381475016134202,
      "val_accuracy": 97.79
    },
    {
      "epoch": 19,
      "train_loss": 0.03272852833029545,
      "train_accuracy": 99.17333333333333,
      "val_loss": 0.12316486824591678,
      "val_accuracy": 97.87
    },
    {
      "epoch": 20,
      "train_loss": 0.03181016606862594,
      "train_accuracy": 99.225,
      "val_loss": 0.12054186651339502,
      "val_accuracy": 97.83
    },
    {
      "epoch": 21,
      "train_loss": 0.028915138441105628,
      "train_accuracy": 99.29333333333334,
      "val_loss": 0.12344090700173034,
      "val_accuracy": 97.84
    },
    {
      "epoch": 22,
      "train_loss": 0.027834345553068063,
      "train_accuracy": 99.30833333333334,
      "val_loss": 0.12008800403931823,
      "val_accuracy": 97.98
    },
    {
      "epoch": 23,
      "train_loss": 0.027892290163618896,
      "train_accuracy": 99.27,
      "val_loss": 0.12380055621434663,
      "val_accuracy": 98.06
    },
    {
      "epoch": 24,
      "train_loss": 0.028516983260520812,
      "train_accuracy": 99.25,
      "val_loss": 0.1458070326704079,
      "val_accuracy": 98.06
    },
    {
      "epoch": 25,
      "train_loss": 0.026619471098947717,
      "train_accuracy": 99.325,
      "val_loss": 0.12087447037627493,
      "val_accuracy": 97.83
    },
    {
      "epoch": 26,
      "train_loss": 0.02678855773647369,
      "train_accuracy": 99.37166666666667,
      "val_loss": 0.09803770758159111,
      "val_accuracy": 98.04
    },
    {
      "epoch": 27,
      "train_loss": 0.028171052849494255,
      "train_accuracy": 99.31333333333333,
      "val_loss": 0.12196669716781479,
      "val_accuracy": 97.76
    },
    {
      "epoch": 28,
      "train_loss": 0.02698200922439846,
      "train_accuracy": 99.33166666666666,
      "val_loss": 0.10754865430101676,
      "val_accuracy": 98.02
    },
    {
      "epoch": 29,
      "train_loss": 0.025631037847317885,
      "train_accuracy": 99.345,
      "val_loss": 0.12835396384846495,
      "val_accuracy": 97.87
    },
    {
      "epoch": 30,
      "train_loss": 0.02353359295333054,
      "train_accuracy": 99.40333333333334,
      "val_loss": 0.0996923445253719,
      "val_accuracy": 98.54
    },
    {
      "epoch": 31,
      "train_loss": 0.028651281987736157,
      "train_accuracy": 99.255,
      "val_loss": 0.1223515965805851,
      "val_accuracy": 97.86
    },
    {
      "epoch": 32,
      "train_loss": 0.02544186695544009,
      "train_accuracy": 99.42666666666666,
      "val_loss": 0.10945359209908948,
      "val_accuracy": 97.98
    },
    {
      "epoch": 33,
      "train_loss": 0.023552050416931446,
      "train_accuracy": 99.425,
      "val_loss": 0.1561886537154322,
      "val_accuracy": 97.97
    },
    {
      "epoch": 34,
      "train_loss": 0.022909492742006547,
      "train_accuracy": 99.445,
      "val_loss": 0.11490340142297281,
      "val_accuracy": 98.15
    },
    {
      "epoch": 35,
      "train_loss": 0.025735148363962117,
      "train_accuracy": 99.35666666666667,
      "val_loss": 0.12418711873212591,
      "val_accuracy": 97.97
    },
    {
      "epoch": 36,
      "train_loss": 0.024432002151723392,
      "train_accuracy": 99.39833333333333,
      "val_loss": 0.11613028244258837,
      "val_accuracy": 98.13
    },
    {
      "epoch": 37,
      "train_loss": 0.02304383873124494,
      "train_accuracy": 99.44666666666667,
      "val_loss": 0.13068402718186223,
      "val_accuracy": 98.15
    },
    {
      "epoch": 38,
      "train_loss": 0.024100418385174474,
      "train_accuracy": 99.425,
      "val_loss": 0.1431759687547234,
      "val_accuracy": 97.84
    },
    {
      "epoch": 39,
      "train_loss": 0.021294506946280583,
      "train_accuracy": 99.50666666666666,
      "val_loss": 0.12521861431600054,
      "val_accuracy": 97.59
    },
    {
      "epoch": 40,
      "train_loss": 0.02686397810253425,
      "train_accuracy": 99.39666666666666,
      "val_loss": 0.1464402662362249,
      "val_accuracy": 97.67
    },
    {
      "epoch": 41,
      "train_loss": 0.02204687542479101,
      "train_accuracy": 99.475,
      "val_loss": 0.1697725375504353,
      "val_accuracy": 97.57
    },
    {
      "epoch": 42,
      "train_loss": 0.02718884881824037,
      "train_accuracy": 99.36333333333333,
      "val_loss": 0.13850113250035823,
      "val_accuracy": 98.24
    },
    {
      "epoch": 43,
      "train_loss": 0.022936194251736215,
      "train_accuracy": 99.50333333333333,
      "val_loss": 0.12606410571610094,
      "val_accuracy": 97.97
    },
    {
      "epoch": 44,
      "train_loss": 0.02452657906498044,
      "train_accuracy": 99.45,
      "val_loss": 0.15850946940275473,
      "val_accuracy": 97.13
    },
    {
      "epoch": 45,
      "train_loss": 0.021336588344833266,
      "train_accuracy": 99.48333333333333,
      "val_loss": 0.12896377942119236,
      "val_accuracy": 98.03
    },
    {
      "epoch": 46,
      "train_loss": 0.024844110272583906,
      "train_accuracy": 99.44333333333333,
      "val_loss": 0.14314573422879082,
      "val_accuracy": 98.01
    },
    {
      "epoch": 47,
      "train_loss": 0.02359914645531475,
      "train_accuracy": 99.45166666666667,
      "val_loss": 0.11087086950297535,
      "val_accuracy": 98.09
    },
    {
      "epoch": 48,
      "train_loss": 0.02188145210004966,
      "train_accuracy": 99.52666666666667,
      "val_loss": 0.13447050104777902,
      "val_accuracy": 97.98
    },
    {
      "epoch": 49,
      "train_loss": 0.02487781776829304,
      "train_accuracy": 99.455,
      "val_loss": 0.12345356570325076,
      "val_accuracy": 98.18
    },
    {
      "epoch": 50,
      "train_loss": 0.02112208712592665,
      "train_accuracy": 99.49666666666667,
      "val_loss": 0.1831157190910398,
      "val_accuracy": 96.81
    },
    {
      "epoch": 51,
      "train_loss": 0.02311372542310339,
      "train_accuracy": 99.45,
      "val_loss": 0.151498182877929,
      "val_accuracy": 98.02
    },
    {
      "epoch": 52,
      "train_loss": 0.02306053840015641,
      "train_accuracy": 99.49833333333333,
      "val_loss": 0.13697271870758243,
      "val_accuracy": 98.01
    },
    {
      "epoch": 53,
      "train_loss": 0.025895461181279937,
      "train_accuracy": 99.395,
      "val_loss": 0.12614573874192417,
      "val_accuracy": 98.04
    },
    {
      "epoch": 54,
      "train_loss": 0.01802758289176316,
      "train_accuracy": 99.49,
      "val_loss": 0.15203984712878083,
      "val_accuracy": 97.78
    },
    {
      "epoch": 55,
      "train_loss": 0.023760729864341912,
      "train_accuracy": 99.48166666666667,
      "val_loss": 0.11596859770726739,
      "val_accuracy": 97.73
    },
    {
      "epoch": 56,
      "train_loss": 0.01752331658544056,
      "train_accuracy": 99.61666666666666,
      "val_loss": 0.14808676930883236,
      "val_accuracy": 98.15
    },
    {
      "epoch": 57,
      "train_loss": 0.026448099125072632,
      "train_accuracy": 99.41833333333334,
      "val_loss": 0.14040346680412594,
      "val_accuracy": 97.73
    },
    {
      "epoch": 58,
      "train_loss": 0.018843865750568078,
      "train_accuracy": 99.57,
      "val_loss": 0.1465101048639721,
      "val_accuracy": 98.0
    },
    {
      "epoch": 59,
      "train_loss": 0.024241831463882983,
      "train_accuracy": 99.40833333333333,
      "val_loss": 0.1231295234312911,
      "val_accuracy": 97.99
    },
    {
      "epoch": 60,
      "train_loss": 0.02377904763893602,
      "train_accuracy": 99.485,
      "val_loss": 0.1631771778907732,
      "val_accuracy": 97.75
    }
  ]
}