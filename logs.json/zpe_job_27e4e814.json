{
  "job_id": "zpe_job_27e4e814",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.0011,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.07
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "baseConfigId": "zpe_job_d243fa84",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-20T05:46:29.440Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3094",
    "Epoch 1/30 - Batch 100/1875 - Loss: 1.1067",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.4499",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.1752",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.2100",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.0732",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.2677",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.1518",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.1505",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2619",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.2203",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.7090",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.0409",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.1694",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.4251",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.1273",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.0471",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.1193",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1141",
    "Epoch 1 completed - Train Acc: 92.16% - Val Acc: 96.05%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.1438",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.2773",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.0041",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.2131",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.0628",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.2422",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.1614",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.0824",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.2661",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0680",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0075",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.0459",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0231",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.1573",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.1094",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.2238",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.2781",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.0734",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0581",
    "Epoch 2 completed - Train Acc: 96.43% - Val Acc: 96.91%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0490",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.3520",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0841",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0150",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.2152",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0245",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.0036",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.0811",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.0217",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0842",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.1157",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0055",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1886",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0393",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0622",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.0042",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0131",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.0543",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.0520",
    "Epoch 3 completed - Train Acc: 97.17% - Val Acc: 97.45%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.1994",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.1978",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0318",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.3146",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.1590",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0236",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0779",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0127",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0053",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.0306",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0526",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.1656",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.1134",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.1454",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.2495",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0670",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0993",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0233",
    "Epoch 4 completed - Train Acc: 97.66% - Val Acc: 97.68%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0233",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.4086",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0323",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.1088",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0031",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0061",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.2330",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0274",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0036",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0048",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0503",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0040",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0105",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0408",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.0037",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0129",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0031",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0210",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0136",
    "Epoch 5 completed - Train Acc: 97.97% - Val Acc: 97.38%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.1838",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0009",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0406",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.0078",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0288",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0103",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.1922",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0045",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0286",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0997",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0195",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.1529",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0202",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0093",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0015",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0111",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0059",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0260",
    "Epoch 6 completed - Train Acc: 98.14% - Val Acc: 97.63%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0019",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0986",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0018",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.1147",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.1193",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0016",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0155",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.1786",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.0068",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0260",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0228",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0689",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0296",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0061",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0486",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0165",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.0219",
    "Epoch 7 completed - Train Acc: 98.32% - Val Acc: 97.37%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0059",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0121",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.0033",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0575",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0104",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0626",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0095",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0277",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0214",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0142",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0014",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0080",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0451",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0811",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.2560",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0048",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0228",
    "Epoch 8 completed - Train Acc: 98.38% - Val Acc: 97.49%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0150",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0066",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0560",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0237",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0170",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0403",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0942",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0160",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.1647",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0349",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.0039",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0099",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0737",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0415",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0516",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0772",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0044",
    "Epoch 9 completed - Train Acc: 98.46% - Val Acc: 98.07%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0043",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0202",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0205",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0208",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.1107",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0087",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0073",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0107",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0022",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0116",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.2100",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.0998",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0022",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.2818",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0046",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0091",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0083",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0977",
    "Epoch 10 completed - Train Acc: 98.61% - Val Acc: 97.29%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0223",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.0038",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0235",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0052",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0157",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.2364",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0036",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.0492",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0075",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0404",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0183",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0012",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.0017",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.1285",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.0523",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.2401",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0050",
    "Epoch 11 completed - Train Acc: 98.67% - Val Acc: 97.54%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0332",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0131",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0232",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.2736",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0084",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.0485",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.0306",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.0140",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0258",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0009",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.0340",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0446",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0125",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0039",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0463",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0691",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0371",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.1952",
    "Epoch 12 completed - Train Acc: 98.64% - Val Acc: 97.93%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0164",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0029",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.0016",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.0558"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.2614064371452977,
      "train_accuracy": 92.155,
      "val_loss": 0.13372244567576105,
      "val_accuracy": 96.05
    },
    {
      "epoch": 2,
      "train_loss": 0.12676783633287997,
      "train_accuracy": 96.42666666666666,
      "val_loss": 0.11184306524191111,
      "val_accuracy": 96.91
    },
    {
      "epoch": 3,
      "train_loss": 0.10296603798680007,
      "train_accuracy": 97.16666666666667,
      "val_loss": 0.09425349787545766,
      "val_accuracy": 97.45
    },
    {
      "epoch": 4,
      "train_loss": 0.08333549755802377,
      "train_accuracy": 97.66166666666666,
      "val_loss": 0.0837820204194842,
      "val_accuracy": 97.68
    },
    {
      "epoch": 5,
      "train_loss": 0.07137864032213886,
      "train_accuracy": 97.965,
      "val_loss": 0.10167721087670831,
      "val_accuracy": 97.38
    },
    {
      "epoch": 6,
      "train_loss": 0.06477229351896094,
      "train_accuracy": 98.135,
      "val_loss": 0.08836986250228042,
      "val_accuracy": 97.63
    },
    {
      "epoch": 7,
      "train_loss": 0.05790111045743106,
      "train_accuracy": 98.32333333333334,
      "val_loss": 0.09839172652137465,
      "val_accuracy": 97.37
    },
    {
      "epoch": 8,
      "train_loss": 0.055573135444490857,
      "train_accuracy": 98.38,
      "val_loss": 0.09258177514050749,
      "val_accuracy": 97.49
    },
    {
      "epoch": 9,
      "train_loss": 0.05139984215580237,
      "train_accuracy": 98.46,
      "val_loss": 0.07468001338060938,
      "val_accuracy": 98.07
    },
    {
      "epoch": 10,
      "train_loss": 0.04935434857046154,
      "train_accuracy": 98.605,
      "val_loss": 0.09608205350226731,
      "val_accuracy": 97.29
    },
    {
      "epoch": 11,
      "train_loss": 0.04630774041668628,
      "train_accuracy": 98.66666666666667,
      "val_loss": 0.10150826030232461,
      "val_accuracy": 97.54
    },
    {
      "epoch": 12,
      "train_loss": 0.04534640966671577,
      "train_accuracy": 98.63666666666667,
      "val_loss": 0.07930836917392055,
      "val_accuracy": 97.93
    }
  ]
}