{
  "job_id": "zpe_job_0947a160",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.0011,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.07
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "baseConfigId": "zpe_job_d243fa84",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-20T04:56:10.995Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.2956",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.5986",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.7265",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.1449",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.1383",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.0677",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.1676",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.0703",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.1939",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2025",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.2736",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.2218",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.1446",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.1308",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.1603",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.0200",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.1072",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.0151",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1860",
    "Epoch 1 completed - Train Acc: 91.69% - Val Acc: 96.23%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.0486",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0859",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.0225",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.0858",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.2462",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.2716",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0381",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.1798",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.1260",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0253",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.1435",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.1076",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0321",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0038",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.0447",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0894",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.0551",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.2352",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.2093",
    "Epoch 2 completed - Train Acc: 96.31% - Val Acc: 96.85%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0150",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.2982",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0516",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0487",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0514",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0026",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.0710",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.4938",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.1089",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.1292",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.0142",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0135",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.0019",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0673",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.2598",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.0511",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.3682",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.0699",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.1046",
    "Epoch 3 completed - Train Acc: 97.11% - Val Acc: 96.02%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0199",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.2144",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0277",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0014",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.1206",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0303",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.1733",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0171",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0585",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0909",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.2696",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.1300",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0873",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0013",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0361",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.0674",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0373",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0042",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 4 completed - Train Acc: 97.59% - Val Acc: 97.24%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0173",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0764",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0310",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.1366",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0092",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0037",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.0113",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0064",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.7022",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0423",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0134",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0297",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0128",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0538",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.0514",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.1155",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0110",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0303",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0156",
    "Epoch 5 completed - Train Acc: 97.89% - Val Acc: 97.05%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.0468",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.1682",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0773",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.1019",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0190",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0206",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0014",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0826",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0050",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.2260",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0863",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0675",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0313",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.1063",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.1476",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.2135",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0113",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0342",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0590",
    "Epoch 6 completed - Train Acc: 98.08% - Val Acc: 97.45%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.1226",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0162",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0183",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0019",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0110",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0718",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0562",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0167",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0462",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.2534",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.0071",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0144",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0648",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0325",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0286",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0063",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0408",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0022",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.1506",
    "Epoch 7 completed - Train Acc: 98.25% - Val Acc: 97.79%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0014",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0196",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0319",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0096",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.1720",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0030",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0291",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0159",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0164",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0189",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.1975",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0023",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.2169",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0110",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.1209",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.1282",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.0325",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.1222",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0076",
    "Epoch 8 completed - Train Acc: 98.44% - Val Acc: 96.74%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0220",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0084",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0269",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.2312",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0080",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0073",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0602",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0202",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.4537",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.1118",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.1647",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0380",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.1130",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0963",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0524",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0427",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0463",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.4680",
    "Epoch 9 completed - Train Acc: 98.43% - Val Acc: 97.94%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0927",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0028",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0096",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0036",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.1292",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.0089",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.1580",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0079",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0077",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0043",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0012",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.3446",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0305",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0110",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0508",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0061",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0070",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.2262",
    "Epoch 10 completed - Train Acc: 98.56% - Val Acc: 97.94%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0096",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.1392",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0291",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0363",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0843",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0061",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0122",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.1020",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0118",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0024",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0221",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0046",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.3167",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0051",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0112",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0030",
    "Epoch 11 completed - Train Acc: 98.74% - Val Acc: 96.76%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0046",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0025",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0826",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0019",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.0820",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.1116",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.0125",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0550",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0010",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.0138",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0226",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0252",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0048",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0286",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0272",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0059",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.0030",
    "Epoch 12 completed - Train Acc: 98.64% - Val Acc: 98.01%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0079",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0621",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.0094",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.2819",
    "Epoch 13/30 - Batch 400/1875 - Loss: 0.0210",
    "Epoch 13/30 - Batch 500/1875 - Loss: 0.1231",
    "Epoch 13/30 - Batch 600/1875 - Loss: 0.0298",
    "Epoch 13/30 - Batch 700/1875 - Loss: 0.2615",
    "Epoch 13/30 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 13/30 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 13/30 - Batch 1000/1875 - Loss: 0.0449",
    "Epoch 13/30 - Batch 1100/1875 - Loss: 0.0072",
    "Epoch 13/30 - Batch 1200/1875 - Loss: 0.0397",
    "Epoch 13/30 - Batch 1300/1875 - Loss: 0.0081",
    "Epoch 13/30 - Batch 1400/1875 - Loss: 0.0145",
    "Epoch 13/30 - Batch 1500/1875 - Loss: 0.0160",
    "Epoch 13/30 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 13/30 - Batch 1700/1875 - Loss: 0.0040",
    "Epoch 13/30 - Batch 1800/1875 - Loss: 0.0935",
    "Epoch 13 completed - Train Acc: 98.75% - Val Acc: 97.89%",
    "Epoch 14/30 - Batch 0/1875 - Loss: 0.0425",
    "Epoch 14/30 - Batch 100/1875 - Loss: 0.0007",
    "Epoch 14/30 - Batch 200/1875 - Loss: 0.0067",
    "Epoch 14/30 - Batch 300/1875 - Loss: 0.1418",
    "Epoch 14/30 - Batch 400/1875 - Loss: 0.2233",
    "Epoch 14/30 - Batch 500/1875 - Loss: 0.0515",
    "Epoch 14/30 - Batch 600/1875 - Loss: 0.0177",
    "Epoch 14/30 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 14/30 - Batch 800/1875 - Loss: 0.2791",
    "Epoch 14/30 - Batch 900/1875 - Loss: 0.1009",
    "Epoch 14/30 - Batch 1000/1875 - Loss: 0.0029",
    "Epoch 14/30 - Batch 1100/1875 - Loss: 0.2114",
    "Epoch 14/30 - Batch 1200/1875 - Loss: 0.0083",
    "Epoch 14/30 - Batch 1300/1875 - Loss: 0.0452",
    "Epoch 14/30 - Batch 1400/1875 - Loss: 0.0139",
    "Epoch 14/30 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 14/30 - Batch 1600/1875 - Loss: 0.0643",
    "Epoch 14/30 - Batch 1700/1875 - Loss: 0.2611",
    "Epoch 14/30 - Batch 1800/1875 - Loss: 0.0152",
    "Epoch 14 completed - Train Acc: 98.78% - Val Acc: 98.00%",
    "Epoch 15/30 - Batch 0/1875 - Loss: 0.0580",
    "Epoch 15/30 - Batch 100/1875 - Loss: 0.0283",
    "Epoch 15/30 - Batch 200/1875 - Loss: 0.0696",
    "Epoch 15/30 - Batch 300/1875 - Loss: 0.0576",
    "Epoch 15/30 - Batch 400/1875 - Loss: 0.0024",
    "Epoch 15/30 - Batch 500/1875 - Loss: 0.1023",
    "Epoch 15/30 - Batch 600/1875 - Loss: 0.0023",
    "Epoch 15/30 - Batch 700/1875 - Loss: 0.1542",
    "Epoch 15/30 - Batch 800/1875 - Loss: 0.0536",
    "Epoch 15/30 - Batch 900/1875 - Loss: 0.0223",
    "Epoch 15/30 - Batch 1000/1875 - Loss: 0.0060",
    "Epoch 15/30 - Batch 1100/1875 - Loss: 0.0174",
    "Epoch 15/30 - Batch 1200/1875 - Loss: 0.0383",
    "Epoch 15/30 - Batch 1300/1875 - Loss: 0.0061",
    "Epoch 15/30 - Batch 1400/1875 - Loss: 0.0072",
    "Epoch 15/30 - Batch 1500/1875 - Loss: 0.1908",
    "Epoch 15/30 - Batch 1600/1875 - Loss: 0.0142",
    "Epoch 15/30 - Batch 1700/1875 - Loss: 0.0739",
    "Epoch 15/30 - Batch 1800/1875 - Loss: 0.0048",
    "Epoch 15 completed - Train Acc: 98.84% - Val Acc: 97.48%",
    "Epoch 16/30 - Batch 0/1875 - Loss: 0.0374",
    "Epoch 16/30 - Batch 100/1875 - Loss: 0.0085",
    "Epoch 16/30 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 16/30 - Batch 300/1875 - Loss: 0.0308",
    "Epoch 16/30 - Batch 400/1875 - Loss: 0.0033",
    "Epoch 16/30 - Batch 500/1875 - Loss: 0.0009",
    "Epoch 16/30 - Batch 600/1875 - Loss: 0.0189",
    "Epoch 16/30 - Batch 700/1875 - Loss: 0.0345",
    "Epoch 16/30 - Batch 800/1875 - Loss: 0.0207",
    "Epoch 16/30 - Batch 900/1875 - Loss: 0.3806",
    "Epoch 16/30 - Batch 1000/1875 - Loss: 0.0010",
    "Epoch 16/30 - Batch 1100/1875 - Loss: 0.0231",
    "Epoch 16/30 - Batch 1200/1875 - Loss: 0.0010",
    "Epoch 16/30 - Batch 1300/1875 - Loss: 0.0051",
    "Epoch 16/30 - Batch 1400/1875 - Loss: 0.0929",
    "Epoch 16/30 - Batch 1500/1875 - Loss: 0.0455",
    "Epoch 16/30 - Batch 1600/1875 - Loss: 0.0012",
    "Epoch 16/30 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 16/30 - Batch 1800/1875 - Loss: 0.0385",
    "Epoch 16 completed - Train Acc: 98.86% - Val Acc: 97.74%",
    "Epoch 17/30 - Batch 0/1875 - Loss: 0.0316",
    "Epoch 17/30 - Batch 100/1875 - Loss: 0.0330",
    "Epoch 17/30 - Batch 200/1875 - Loss: 0.0861",
    "Epoch 17/30 - Batch 300/1875 - Loss: 0.0071",
    "Epoch 17/30 - Batch 400/1875 - Loss: 0.0018",
    "Epoch 17/30 - Batch 500/1875 - Loss: 0.0260",
    "Epoch 17/30 - Batch 600/1875 - Loss: 0.0113",
    "Epoch 17/30 - Batch 700/1875 - Loss: 0.0041",
    "Epoch 17/30 - Batch 800/1875 - Loss: 0.0008",
    "Epoch 17/30 - Batch 900/1875 - Loss: 0.0114",
    "Epoch 17/30 - Batch 1000/1875 - Loss: 0.0521",
    "Epoch 17/30 - Batch 1100/1875 - Loss: 0.0120",
    "Epoch 17/30 - Batch 1200/1875 - Loss: 0.0062",
    "Epoch 17/30 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 17/30 - Batch 1400/1875 - Loss: 0.2683",
    "Epoch 17/30 - Batch 1500/1875 - Loss: 0.1255",
    "Epoch 17/30 - Batch 1600/1875 - Loss: 0.0592",
    "Epoch 17/30 - Batch 1700/1875 - Loss: 0.0035",
    "Epoch 17/30 - Batch 1800/1875 - Loss: 0.0161",
    "Epoch 17 completed - Train Acc: 98.88% - Val Acc: 97.55%",
    "Epoch 18/30 - Batch 0/1875 - Loss: 0.2141",
    "Epoch 18/30 - Batch 100/1875 - Loss: 0.0789",
    "Epoch 18/30 - Batch 200/1875 - Loss: 0.0965",
    "Epoch 18/30 - Batch 300/1875 - Loss: 0.0111",
    "Epoch 18/30 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 18/30 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 18/30 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 18/30 - Batch 700/1875 - Loss: 0.2022",
    "Epoch 18/30 - Batch 800/1875 - Loss: 0.0401",
    "Epoch 18/30 - Batch 900/1875 - Loss: 0.0086",
    "Epoch 18/30 - Batch 1000/1875 - Loss: 0.0085",
    "Epoch 18/30 - Batch 1100/1875 - Loss: 0.0098",
    "Epoch 18/30 - Batch 1200/1875 - Loss: 0.0040",
    "Epoch 18/30 - Batch 1300/1875 - Loss: 0.0019",
    "Epoch 18/30 - Batch 1400/1875 - Loss: 0.0018",
    "Epoch 18/30 - Batch 1500/1875 - Loss: 0.1283",
    "Epoch 18/30 - Batch 1600/1875 - Loss: 0.0109",
    "Epoch 18/30 - Batch 1700/1875 - Loss: 0.0358",
    "Epoch 18/30 - Batch 1800/1875 - Loss: 0.0166",
    "Epoch 18 completed - Train Acc: 98.88% - Val Acc: 97.97%",
    "Epoch 19/30 - Batch 0/1875 - Loss: 0.0054",
    "Epoch 19/30 - Batch 100/1875 - Loss: 0.0102",
    "Epoch 19/30 - Batch 200/1875 - Loss: 0.0744",
    "Epoch 19/30 - Batch 300/1875 - Loss: 0.0038",
    "Epoch 19/30 - Batch 400/1875 - Loss: 0.0923",
    "Epoch 19/30 - Batch 500/1875 - Loss: 0.0329",
    "Epoch 19/30 - Batch 600/1875 - Loss: 0.0304",
    "Epoch 19/30 - Batch 700/1875 - Loss: 0.1439",
    "Epoch 19/30 - Batch 800/1875 - Loss: 0.0022",
    "Epoch 19/30 - Batch 900/1875 - Loss: 0.0083",
    "Epoch 19/30 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 19/30 - Batch 1100/1875 - Loss: 0.0023",
    "Epoch 19/30 - Batch 1200/1875 - Loss: 0.1269",
    "Epoch 19/30 - Batch 1300/1875 - Loss: 0.0146",
    "Epoch 19/30 - Batch 1400/1875 - Loss: 0.0013",
    "Epoch 19/30 - Batch 1500/1875 - Loss: 0.0212",
    "Epoch 19/30 - Batch 1600/1875 - Loss: 0.0039",
    "Epoch 19/30 - Batch 1700/1875 - Loss: 0.0063",
    "Epoch 19/30 - Batch 1800/1875 - Loss: 0.0332",
    "Epoch 19 completed - Train Acc: 98.94% - Val Acc: 97.88%",
    "Epoch 20/30 - Batch 0/1875 - Loss: 0.0482",
    "Epoch 20/30 - Batch 100/1875 - Loss: 0.0062",
    "Epoch 20/30 - Batch 200/1875 - Loss: 0.0108",
    "Epoch 20/30 - Batch 300/1875 - Loss: 0.0082",
    "Epoch 20/30 - Batch 400/1875 - Loss: 0.0834",
    "Epoch 20/30 - Batch 500/1875 - Loss: 0.0423",
    "Epoch 20/30 - Batch 600/1875 - Loss: 0.1131",
    "Epoch 20/30 - Batch 700/1875 - Loss: 0.0360",
    "Epoch 20/30 - Batch 800/1875 - Loss: 0.0605",
    "Epoch 20/30 - Batch 900/1875 - Loss: 0.0107",
    "Epoch 20/30 - Batch 1000/1875 - Loss: 0.0991",
    "Epoch 20/30 - Batch 1100/1875 - Loss: 0.2853",
    "Epoch 20/30 - Batch 1200/1875 - Loss: 0.1201",
    "Epoch 20/30 - Batch 1300/1875 - Loss: 0.1949",
    "Epoch 20/30 - Batch 1400/1875 - Loss: 0.0601",
    "Epoch 20/30 - Batch 1500/1875 - Loss: 0.0291",
    "Epoch 20/30 - Batch 1600/1875 - Loss: 0.2367",
    "Epoch 20/30 - Batch 1700/1875 - Loss: 0.0027",
    "Epoch 20/30 - Batch 1800/1875 - Loss: 0.0024",
    "Epoch 20 completed - Train Acc: 98.93% - Val Acc: 97.70%",
    "Epoch 21/30 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 21/30 - Batch 100/1875 - Loss: 0.0049",
    "Epoch 21/30 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 21/30 - Batch 300/1875 - Loss: 0.0037",
    "Epoch 21/30 - Batch 400/1875 - Loss: 0.0154",
    "Epoch 21/30 - Batch 500/1875 - Loss: 0.0662",
    "Epoch 21/30 - Batch 600/1875 - Loss: 0.0440",
    "Epoch 21/30 - Batch 700/1875 - Loss: 0.0788",
    "Epoch 21/30 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 21/30 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 21/30 - Batch 1000/1875 - Loss: 0.1563",
    "Epoch 21/30 - Batch 1100/1875 - Loss: 0.0697",
    "Epoch 21/30 - Batch 1200/1875 - Loss: 0.0256",
    "Epoch 21/30 - Batch 1300/1875 - Loss: 0.0594",
    "Epoch 21/30 - Batch 1400/1875 - Loss: 0.1068",
    "Epoch 21/30 - Batch 1500/1875 - Loss: 0.0017",
    "Epoch 21/30 - Batch 1600/1875 - Loss: 0.0032",
    "Epoch 21/30 - Batch 1700/1875 - Loss: 0.0266",
    "Epoch 21/30 - Batch 1800/1875 - Loss: 0.0015",
    "Epoch 21 completed - Train Acc: 98.92% - Val Acc: 97.80%",
    "Epoch 22/30 - Batch 0/1875 - Loss: 0.0006",
    "Epoch 22/30 - Batch 100/1875 - Loss: 0.0014",
    "Epoch 22/30 - Batch 200/1875 - Loss: 0.0539",
    "Epoch 22/30 - Batch 300/1875 - Loss: 0.0061",
    "Epoch 22/30 - Batch 400/1875 - Loss: 0.0286",
    "Epoch 22/30 - Batch 500/1875 - Loss: 0.1909",
    "Epoch 22/30 - Batch 600/1875 - Loss: 0.0172",
    "Epoch 22/30 - Batch 700/1875 - Loss: 0.0836",
    "Epoch 22/30 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 22/30 - Batch 900/1875 - Loss: 0.0350",
    "Epoch 22/30 - Batch 1000/1875 - Loss: 0.0005",
    "Epoch 22/30 - Batch 1100/1875 - Loss: 0.0050",
    "Epoch 22/30 - Batch 1200/1875 - Loss: 0.0059",
    "Epoch 22/30 - Batch 1300/1875 - Loss: 0.0804",
    "Epoch 22/30 - Batch 1400/1875 - Loss: 0.0017",
    "Epoch 22/30 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 22/30 - Batch 1600/1875 - Loss: 0.0023",
    "Epoch 22/30 - Batch 1700/1875 - Loss: 0.0010",
    "Epoch 22/30 - Batch 1800/1875 - Loss: 0.1529",
    "Epoch 22 completed - Train Acc: 98.90% - Val Acc: 98.05%",
    "Epoch 23/30 - Batch 0/1875 - Loss: 0.0014",
    "Epoch 23/30 - Batch 100/1875 - Loss: 0.0189",
    "Epoch 23/30 - Batch 200/1875 - Loss: 0.0213",
    "Epoch 23/30 - Batch 300/1875 - Loss: 0.0632",
    "Epoch 23/30 - Batch 400/1875 - Loss: 0.0005",
    "Epoch 23/30 - Batch 500/1875 - Loss: 0.1141",
    "Epoch 23/30 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 23/30 - Batch 700/1875 - Loss: 0.0121",
    "Epoch 23/30 - Batch 800/1875 - Loss: 0.0432",
    "Epoch 23/30 - Batch 900/1875 - Loss: 0.0322",
    "Epoch 23/30 - Batch 1000/1875 - Loss: 0.0208",
    "Epoch 23/30 - Batch 1100/1875 - Loss: 0.0095",
    "Epoch 23/30 - Batch 1200/1875 - Loss: 0.0111",
    "Epoch 23/30 - Batch 1300/1875 - Loss: 0.0115",
    "Epoch 23/30 - Batch 1400/1875 - Loss: 0.0066",
    "Epoch 23/30 - Batch 1500/1875 - Loss: 0.0051",
    "Epoch 23/30 - Batch 1600/1875 - Loss: 0.0066",
    "Epoch 23/30 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 23/30 - Batch 1800/1875 - Loss: 0.0202",
    "Epoch 23 completed - Train Acc: 98.97% - Val Acc: 97.79%",
    "Epoch 24/30 - Batch 0/1875 - Loss: 0.0525",
    "Epoch 24/30 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 24/30 - Batch 200/1875 - Loss: 0.0198",
    "Epoch 24/30 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 24/30 - Batch 400/1875 - Loss: 0.0084",
    "Epoch 24/30 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 24/30 - Batch 600/1875 - Loss: 0.0747",
    "Epoch 24/30 - Batch 700/1875 - Loss: 0.0357",
    "Epoch 24/30 - Batch 800/1875 - Loss: 0.0057",
    "Epoch 24/30 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 24/30 - Batch 1000/1875 - Loss: 0.0026",
    "Epoch 24/30 - Batch 1100/1875 - Loss: 0.0150",
    "Epoch 24/30 - Batch 1200/1875 - Loss: 0.0034",
    "Epoch 24/30 - Batch 1300/1875 - Loss: 0.1413",
    "Epoch 24/30 - Batch 1400/1875 - Loss: 0.0173",
    "Epoch 24/30 - Batch 1500/1875 - Loss: 0.0030",
    "Epoch 24/30 - Batch 1600/1875 - Loss: 0.0075",
    "Epoch 24/30 - Batch 1700/1875 - Loss: 0.0097",
    "Epoch 24/30 - Batch 1800/1875 - Loss: 0.0038",
    "Epoch 24 completed - Train Acc: 98.96% - Val Acc: 97.54%",
    "Epoch 25/30 - Batch 0/1875 - Loss: 0.0186",
    "Epoch 25/30 - Batch 100/1875 - Loss: 0.0050",
    "Epoch 25/30 - Batch 200/1875 - Loss: 0.0029",
    "Epoch 25/30 - Batch 300/1875 - Loss: 0.0014",
    "Epoch 25/30 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 25/30 - Batch 500/1875 - Loss: 0.0023",
    "Epoch 25/30 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 25/30 - Batch 700/1875 - Loss: 0.0020",
    "Epoch 25/30 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 25/30 - Batch 900/1875 - Loss: 0.0021",
    "Epoch 25/30 - Batch 1000/1875 - Loss: 0.0306",
    "Epoch 25/30 - Batch 1100/1875 - Loss: 0.0288",
    "Epoch 25/30 - Batch 1200/1875 - Loss: 0.0245",
    "Epoch 25/30 - Batch 1300/1875 - Loss: 0.0374",
    "Epoch 25/30 - Batch 1400/1875 - Loss: 0.0090",
    "Epoch 25/30 - Batch 1500/1875 - Loss: 0.0071",
    "Epoch 25/30 - Batch 1600/1875 - Loss: 0.0810",
    "Epoch 25/30 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 25/30 - Batch 1800/1875 - Loss: 0.0360",
    "Epoch 25 completed - Train Acc: 99.00% - Val Acc: 98.11%",
    "Epoch 26/30 - Batch 0/1875 - Loss: 0.0090",
    "Epoch 26/30 - Batch 100/1875 - Loss: 0.0010",
    "Epoch 26/30 - Batch 200/1875 - Loss: 0.0595",
    "Epoch 26/30 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 26/30 - Batch 400/1875 - Loss: 0.0829",
    "Epoch 26/30 - Batch 500/1875 - Loss: 0.0513",
    "Epoch 26/30 - Batch 600/1875 - Loss: 0.0020",
    "Epoch 26/30 - Batch 700/1875 - Loss: 0.0043",
    "Epoch 26/30 - Batch 800/1875 - Loss: 0.0048",
    "Epoch 26/30 - Batch 900/1875 - Loss: 0.0054",
    "Epoch 26/30 - Batch 1000/1875 - Loss: 0.0346",
    "Epoch 26/30 - Batch 1100/1875 - Loss: 0.0042",
    "Epoch 26/30 - Batch 1200/1875 - Loss: 0.1325",
    "Epoch 26/30 - Batch 1300/1875 - Loss: 0.0008",
    "Epoch 26/30 - Batch 1400/1875 - Loss: 0.0777",
    "Epoch 26/30 - Batch 1500/1875 - Loss: 0.0025",
    "Epoch 26/30 - Batch 1600/1875 - Loss: 0.0054",
    "Epoch 26/30 - Batch 1700/1875 - Loss: 0.0409",
    "Epoch 26/30 - Batch 1800/1875 - Loss: 0.1515",
    "Epoch 26 completed - Train Acc: 99.05% - Val Acc: 97.70%",
    "Epoch 27/30 - Batch 0/1875 - Loss: 0.2662",
    "Epoch 27/30 - Batch 100/1875 - Loss: 0.0028",
    "Epoch 27/30 - Batch 200/1875 - Loss: 0.1116",
    "Epoch 27/30 - Batch 300/1875 - Loss: 0.0608",
    "Epoch 27/30 - Batch 400/1875 - Loss: 0.0017",
    "Epoch 27/30 - Batch 500/1875 - Loss: 0.0191",
    "Epoch 27/30 - Batch 600/1875 - Loss: 0.0050",
    "Epoch 27/30 - Batch 700/1875 - Loss: 0.0980",
    "Epoch 27/30 - Batch 800/1875 - Loss: 0.0888",
    "Epoch 27/30 - Batch 900/1875 - Loss: 0.0101",
    "Epoch 27/30 - Batch 1000/1875 - Loss: 0.0594",
    "Epoch 27/30 - Batch 1100/1875 - Loss: 0.0058",
    "Epoch 27/30 - Batch 1200/1875 - Loss: 0.1178",
    "Epoch 27/30 - Batch 1300/1875 - Loss: 0.0046",
    "Epoch 27/30 - Batch 1400/1875 - Loss: 0.0116",
    "Epoch 27/30 - Batch 1500/1875 - Loss: 0.0010",
    "Epoch 27/30 - Batch 1600/1875 - Loss: 0.0023",
    "Epoch 27/30 - Batch 1700/1875 - Loss: 0.0121",
    "Epoch 27/30 - Batch 1800/1875 - Loss: 0.1255",
    "Epoch 27 completed - Train Acc: 99.03% - Val Acc: 97.53%",
    "Epoch 28/30 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 28/30 - Batch 100/1875 - Loss: 0.0233",
    "Epoch 28/30 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 28/30 - Batch 300/1875 - Loss: 0.0025",
    "Epoch 28/30 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 28/30 - Batch 500/1875 - Loss: 0.0012",
    "Epoch 28/30 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 28/30 - Batch 700/1875 - Loss: 0.0051",
    "Epoch 28/30 - Batch 800/1875 - Loss: 0.0514",
    "Epoch 28/30 - Batch 900/1875 - Loss: 0.0016",
    "Epoch 28/30 - Batch 1000/1875 - Loss: 0.0007",
    "Epoch 28/30 - Batch 1100/1875 - Loss: 0.0239",
    "Epoch 28/30 - Batch 1200/1875 - Loss: 0.1787",
    "Epoch 28/30 - Batch 1300/1875 - Loss: 0.1994",
    "Epoch 28/30 - Batch 1400/1875 - Loss: 0.0060",
    "Epoch 28/30 - Batch 1500/1875 - Loss: 0.0008",
    "Epoch 28/30 - Batch 1600/1875 - Loss: 0.1025",
    "Epoch 28/30 - Batch 1700/1875 - Loss: 0.0021",
    "Epoch 28/30 - Batch 1800/1875 - Loss: 0.0765",
    "Epoch 28 completed - Train Acc: 99.03% - Val Acc: 97.93%",
    "Epoch 29/30 - Batch 0/1875 - Loss: 0.0016",
    "Epoch 29/30 - Batch 100/1875 - Loss: 0.0151",
    "Epoch 29/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 29/30 - Batch 300/1875 - Loss: 0.0313",
    "Epoch 29/30 - Batch 400/1875 - Loss: 0.0005",
    "Epoch 29/30 - Batch 500/1875 - Loss: 0.0024",
    "Epoch 29/30 - Batch 600/1875 - Loss: 0.0405",
    "Epoch 29/30 - Batch 700/1875 - Loss: 0.0527",
    "Epoch 29/30 - Batch 800/1875 - Loss: 0.1335",
    "Epoch 29/30 - Batch 900/1875 - Loss: 0.0620",
    "Epoch 29/30 - Batch 1000/1875 - Loss: 0.0072",
    "Epoch 29/30 - Batch 1100/1875 - Loss: 0.0094",
    "Epoch 29/30 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 29/30 - Batch 1300/1875 - Loss: 0.0142",
    "Epoch 29/30 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 29/30 - Batch 1500/1875 - Loss: 0.0953",
    "Epoch 29/30 - Batch 1600/1875 - Loss: 0.0125",
    "Epoch 29/30 - Batch 1700/1875 - Loss: 0.0504",
    "Epoch 29/30 - Batch 1800/1875 - Loss: 0.0099",
    "Epoch 29 completed - Train Acc: 99.08% - Val Acc: 98.36%",
    "Epoch 30/30 - Batch 0/1875 - Loss: 0.0750",
    "Epoch 30/30 - Batch 100/1875 - Loss: 0.0078",
    "Epoch 30/30 - Batch 200/1875 - Loss: 0.0181",
    "Epoch 30/30 - Batch 300/1875 - Loss: 0.0126",
    "Epoch 30/30 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 30/30 - Batch 500/1875 - Loss: 0.0475",
    "Epoch 30/30 - Batch 600/1875 - Loss: 0.0061",
    "Epoch 30/30 - Batch 700/1875 - Loss: 0.0035",
    "Epoch 30/30 - Batch 800/1875 - Loss: 0.0650",
    "Epoch 30/30 - Batch 900/1875 - Loss: 0.0062",
    "Epoch 30/30 - Batch 1000/1875 - Loss: 0.0015",
    "Epoch 30/30 - Batch 1100/1875 - Loss: 0.1588",
    "Epoch 30/30 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 30/30 - Batch 1300/1875 - Loss: 0.0223",
    "Epoch 30/30 - Batch 1400/1875 - Loss: 0.0008",
    "Epoch 30/30 - Batch 1500/1875 - Loss: 0.0081",
    "Epoch 30/30 - Batch 1600/1875 - Loss: 0.0634",
    "Epoch 30/30 - Batch 1700/1875 - Loss: 0.0128",
    "Epoch 30/30 - Batch 1800/1875 - Loss: 0.1942",
    "Epoch 30 completed - Train Acc: 99.03% - Val Acc: 97.71%",
    "Training completed. Model saved to models/ZPE-QuantumWeaver-V1.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.26776471349410713,
      "train_accuracy": 91.69333333333333,
      "val_loss": 0.1427261080680647,
      "val_accuracy": 96.23
    },
    {
      "epoch": 2,
      "train_loss": 0.13346118414786956,
      "train_accuracy": 96.315,
      "val_loss": 0.11715301348825548,
      "val_accuracy": 96.85
    },
    {
      "epoch": 3,
      "train_loss": 0.1037327888544649,
      "train_accuracy": 97.115,
      "val_loss": 0.149502972001433,
      "val_accuracy": 96.02
    },
    {
      "epoch": 4,
      "train_loss": 0.0866798087318816,
      "train_accuracy": 97.59333333333333,
      "val_loss": 0.1070930622625226,
      "val_accuracy": 97.24
    },
    {
      "epoch": 5,
      "train_loss": 0.07547057468897353,
      "train_accuracy": 97.88833333333334,
      "val_loss": 0.10701430646422116,
      "val_accuracy": 97.05
    },
    {
      "epoch": 6,
      "train_loss": 0.06766297114893484,
      "train_accuracy": 98.08333333333333,
      "val_loss": 0.09572814966196484,
      "val_accuracy": 97.45
    },
    {
      "epoch": 7,
      "train_loss": 0.06172076108937617,
      "train_accuracy": 98.245,
      "val_loss": 0.08573653790785038,
      "val_accuracy": 97.79
    },
    {
      "epoch": 8,
      "train_loss": 0.055366274268516764,
      "train_accuracy": 98.43666666666667,
      "val_loss": 0.11127029767503294,
      "val_accuracy": 96.74
    },
    {
      "epoch": 9,
      "train_loss": 0.054767057853613126,
      "train_accuracy": 98.43166666666667,
      "val_loss": 0.0810467965720789,
      "val_accuracy": 97.94
    },
    {
      "epoch": 10,
      "train_loss": 0.05050595967720534,
      "train_accuracy": 98.555,
      "val_loss": 0.07645087486275988,
      "val_accuracy": 97.94
    },
    {
      "epoch": 11,
      "train_loss": 0.0441651092878213,
      "train_accuracy": 98.73666666666666,
      "val_loss": 0.14478756157011424,
      "val_accuracy": 96.76
    },
    {
      "epoch": 12,
      "train_loss": 0.045653373199163856,
      "train_accuracy": 98.64166666666667,
      "val_loss": 0.08217638528136673,
      "val_accuracy": 98.01
    },
    {
      "epoch": 13,
      "train_loss": 0.04278698010495088,
      "train_accuracy": 98.755,
      "val_loss": 0.09138779116576548,
      "val_accuracy": 97.89
    },
    {
      "epoch": 14,
      "train_loss": 0.04098488036118603,
      "train_accuracy": 98.78333333333333,
      "val_loss": 0.08615148112706181,
      "val_accuracy": 98.0
    },
    {
      "epoch": 15,
      "train_loss": 0.03940391425002648,
      "train_accuracy": 98.84333333333333,
      "val_loss": 0.10003174906139063,
      "val_accuracy": 97.48
    },
    {
      "epoch": 16,
      "train_loss": 0.03888655041505117,
      "train_accuracy": 98.86333333333333,
      "val_loss": 0.08397653593412263,
      "val_accuracy": 97.74
    },
    {
      "epoch": 17,
      "train_loss": 0.03825454516542959,
      "train_accuracy": 98.875,
      "val_loss": 0.11581587908914573,
      "val_accuracy": 97.55
    },
    {
      "epoch": 18,
      "train_loss": 0.037370863868404804,
      "train_accuracy": 98.88166666666666,
      "val_loss": 0.08620482078210877,
      "val_accuracy": 97.97
    },
    {
      "epoch": 19,
      "train_loss": 0.03581914629906727,
      "train_accuracy": 98.94333333333333,
      "val_loss": 0.08340834369802975,
      "val_accuracy": 97.88
    },
    {
      "epoch": 20,
      "train_loss": 0.035579332858059205,
      "train_accuracy": 98.93,
      "val_loss": 0.09353556050661199,
      "val_accuracy": 97.7
    },
    {
      "epoch": 21,
      "train_loss": 0.034121619684010514,
      "train_accuracy": 98.91666666666667,
      "val_loss": 0.08858626284780928,
      "val_accuracy": 97.8
    },
    {
      "epoch": 22,
      "train_loss": 0.03563643168305765,
      "train_accuracy": 98.90166666666667,
      "val_loss": 0.073378063408185,
      "val_accuracy": 98.05
    },
    {
      "epoch": 23,
      "train_loss": 0.03194974241111292,
      "train_accuracy": 98.975,
      "val_loss": 0.09316043446968966,
      "val_accuracy": 97.79
    },
    {
      "epoch": 24,
      "train_loss": 0.03455548653813312,
      "train_accuracy": 98.95666666666666,
      "val_loss": 0.09435692536618835,
      "val_accuracy": 97.54
    },
    {
      "epoch": 25,
      "train_loss": 0.0322054628253342,
      "train_accuracy": 98.995,
      "val_loss": 0.07341034569322255,
      "val_accuracy": 98.11
    },
    {
      "epoch": 26,
      "train_loss": 0.03133198099778092,
      "train_accuracy": 99.045,
      "val_loss": 0.08420447584670585,
      "val_accuracy": 97.7
    },
    {
      "epoch": 27,
      "train_loss": 0.03120956561298032,
      "train_accuracy": 99.03166666666667,
      "val_loss": 0.10800231026340147,
      "val_accuracy": 97.53
    },
    {
      "epoch": 28,
      "train_loss": 0.03109026949350664,
      "train_accuracy": 99.03166666666667,
      "val_loss": 0.08195727587361197,
      "val_accuracy": 97.93
    },
    {
      "epoch": 29,
      "train_loss": 0.029622430102503858,
      "train_accuracy": 99.08333333333333,
      "val_loss": 0.07326703307459587,
      "val_accuracy": 98.36
    },
    {
      "epoch": 30,
      "train_loss": 0.03151734572642939,
      "train_accuracy": 99.035,
      "val_loss": 0.09537066389402288,
      "val_accuracy": 97.71
    }
  ]
}