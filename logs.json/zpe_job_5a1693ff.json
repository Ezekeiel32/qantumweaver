{
  "job_id": "zpe_job_5a1693ff",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.001,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T22:02:53.244Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.2907",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.4995",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.7016",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.4574",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.3623",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.2349",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.1319",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.1321",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.2006",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.1990",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.1012",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.2836",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.0628",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.1857",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.2635",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.2360",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.2374",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.1100",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1125",
    "Epoch 1 completed - Train Acc: 92.00% - Val Acc: 95.27%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.0190",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.1396",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.0320",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.0572",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.1205",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.0436",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0489",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.0413",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.0538",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.1332",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0396",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.1343",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0056",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.4188",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.1146",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0260",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.1989",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.0478",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.2323",
    "Epoch 2 completed - Train Acc: 96.34% - Val Acc: 96.34%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0046",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0348",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0018",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0340",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.1339",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0350",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.1489",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.2222",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.0666",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0148",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.0985",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.1288",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1846",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0778",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.1081",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.2473",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0443",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.1544",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.0854",
    "Epoch 3 completed - Train Acc: 97.28% - Val Acc: 97.26%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0053",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.0065",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0062",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0289",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.1178",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0075",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.1283",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0338",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0112",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.1119",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.1641",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.2769",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0417",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.1754",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0220",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.2197",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.2237",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0271",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0392",
    "Epoch 4 completed - Train Acc: 97.71% - Val Acc: 97.62%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.1454",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0143",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0033",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.4642",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0382",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0208",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.1199",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0219",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0233",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0072",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.3077",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0836",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0507",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0392",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.1195",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0021",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.1405",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0042",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0286",
    "Epoch 5 completed - Train Acc: 98.01% - Val Acc: 97.72%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.0414",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0363",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0264",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.0144",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.1661",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0315",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0100",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0035",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0099",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0193",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.2329",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0894",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0088",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0013",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0035",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0035",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0277",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.1352",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.2240",
    "Epoch 6 completed - Train Acc: 98.21% - Val Acc: 97.63%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0290",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0259",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0095",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0252",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.4234",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0093",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0372",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0067",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0718",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.0675",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.0064",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0043",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0073",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0940",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0046",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0941",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0154",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.3304",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.0758",
    "Epoch 7 completed - Train Acc: 98.36% - Val Acc: 97.43%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0028",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0862",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0185",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0038",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.0565",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0616",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0445",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.1703",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0112",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.1967",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0265",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0433",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0432",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.1982",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0098",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.1059",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0407",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0348",
    "Epoch 8 completed - Train Acc: 98.47% - Val Acc: 97.69%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0022",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0091",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0667",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0032",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0724",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0045",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0343",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0052",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0508",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.2114",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0067",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0205",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.0402",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0311",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0158",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0912",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0212",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0336",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0049",
    "Epoch 9 completed - Train Acc: 98.60% - Val Acc: 97.85%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0068",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0031",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0215",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0046",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.1463",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.0974",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0018",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0047",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.1406",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0141",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0508",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0222",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.0061",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0024",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0035",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0063",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0121",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0038",
    "Epoch 10 completed - Train Acc: 98.61% - Val Acc: 97.45%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0039",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.1214",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0151",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.1693",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0005",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0823",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0021",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0176",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0162",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.0118",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0214",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.1791",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0623",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0252",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.0558",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0153",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.0149",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0316",
    "Epoch 11 completed - Train Acc: 98.78% - Val Acc: 97.65%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0079",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0065",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0213",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0383",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.1709",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.0015",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.0135",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0033",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0037",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.1040",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0009",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0101",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0321",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0216",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0051",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0183",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.0169",
    "Epoch 12 completed - Train Acc: 98.77% - Val Acc: 97.70%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0017",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0077",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.1285",
    "Epoch 13/30 - Batch 400/1875 - Loss: 0.0121",
    "Epoch 13/30 - Batch 500/1875 - Loss: 0.0041",
    "Epoch 13/30 - Batch 600/1875 - Loss: 0.0077",
    "Epoch 13/30 - Batch 700/1875 - Loss: 0.1313",
    "Epoch 13/30 - Batch 800/1875 - Loss: 0.0863",
    "Epoch 13/30 - Batch 900/1875 - Loss: 0.0525",
    "Epoch 13/30 - Batch 1000/1875 - Loss: 0.1360",
    "Epoch 13/30 - Batch 1100/1875 - Loss: 0.0047",
    "Epoch 13/30 - Batch 1200/1875 - Loss: 0.2183",
    "Epoch 13/30 - Batch 1300/1875 - Loss: 0.0253",
    "Epoch 13/30 - Batch 1400/1875 - Loss: 0.0147",
    "Epoch 13/30 - Batch 1500/1875 - Loss: 0.0435",
    "Epoch 13/30 - Batch 1600/1875 - Loss: 0.0742",
    "Epoch 13/30 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 13/30 - Batch 1800/1875 - Loss: 0.1876",
    "Epoch 13 completed - Train Acc: 98.83% - Val Acc: 97.82%",
    "Epoch 14/30 - Batch 0/1875 - Loss: 0.0022",
    "Epoch 14/30 - Batch 100/1875 - Loss: 0.0061",
    "Epoch 14/30 - Batch 200/1875 - Loss: 0.0070",
    "Epoch 14/30 - Batch 300/1875 - Loss: 0.0126",
    "Epoch 14/30 - Batch 400/1875 - Loss: 0.0050",
    "Epoch 14/30 - Batch 500/1875 - Loss: 0.0858",
    "Epoch 14/30 - Batch 600/1875 - Loss: 0.0233",
    "Epoch 14/30 - Batch 700/1875 - Loss: 0.0249",
    "Epoch 14/30 - Batch 800/1875 - Loss: 0.0129",
    "Epoch 14/30 - Batch 900/1875 - Loss: 0.0160",
    "Epoch 14/30 - Batch 1000/1875 - Loss: 0.0371",
    "Epoch 14/30 - Batch 1100/1875 - Loss: 0.0206",
    "Epoch 14/30 - Batch 1200/1875 - Loss: 0.0977",
    "Epoch 14/30 - Batch 1300/1875 - Loss: 0.0059",
    "Epoch 14/30 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 14/30 - Batch 1500/1875 - Loss: 0.0691",
    "Epoch 14/30 - Batch 1600/1875 - Loss: 0.0207",
    "Epoch 14/30 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 14/30 - Batch 1800/1875 - Loss: 0.0235",
    "Epoch 14 completed - Train Acc: 98.85% - Val Acc: 97.89%",
    "Epoch 15/30 - Batch 0/1875 - Loss: 0.0038",
    "Epoch 15/30 - Batch 100/1875 - Loss: 0.0015",
    "Epoch 15/30 - Batch 200/1875 - Loss: 0.0829",
    "Epoch 15/30 - Batch 300/1875 - Loss: 0.0038",
    "Epoch 15/30 - Batch 400/1875 - Loss: 0.0763",
    "Epoch 15/30 - Batch 500/1875 - Loss: 0.0035",
    "Epoch 15/30 - Batch 600/1875 - Loss: 0.0035",
    "Epoch 15/30 - Batch 700/1875 - Loss: 0.0346",
    "Epoch 15/30 - Batch 800/1875 - Loss: 0.0038",
    "Epoch 15/30 - Batch 900/1875 - Loss: 0.0192",
    "Epoch 15/30 - Batch 1000/1875 - Loss: 0.0058",
    "Epoch 15/30 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 15/30 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 15/30 - Batch 1300/1875 - Loss: 0.1642",
    "Epoch 15/30 - Batch 1400/1875 - Loss: 0.0052",
    "Epoch 15/30 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 15/30 - Batch 1600/1875 - Loss: 0.0240",
    "Epoch 15/30 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 15/30 - Batch 1800/1875 - Loss: 0.0048",
    "Epoch 15 completed - Train Acc: 98.87% - Val Acc: 97.73%",
    "Epoch 16/30 - Batch 0/1875 - Loss: 0.0239",
    "Epoch 16/30 - Batch 100/1875 - Loss: 0.0168",
    "Epoch 16/30 - Batch 200/1875 - Loss: 0.1055",
    "Epoch 16/30 - Batch 300/1875 - Loss: 0.0036",
    "Epoch 16/30 - Batch 400/1875 - Loss: 0.0027",
    "Epoch 16/30 - Batch 500/1875 - Loss: 0.0004",
    "Epoch 16/30 - Batch 600/1875 - Loss: 0.0086",
    "Epoch 16/30 - Batch 700/1875 - Loss: 0.0043",
    "Epoch 16/30 - Batch 800/1875 - Loss: 0.0536",
    "Epoch 16/30 - Batch 900/1875 - Loss: 0.0918",
    "Epoch 16/30 - Batch 1000/1875 - Loss: 0.0120",
    "Epoch 16/30 - Batch 1100/1875 - Loss: 0.0209",
    "Epoch 16/30 - Batch 1200/1875 - Loss: 0.0063",
    "Epoch 16/30 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 16/30 - Batch 1400/1875 - Loss: 0.0012",
    "Epoch 16/30 - Batch 1500/1875 - Loss: 0.0370",
    "Epoch 16/30 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 16/30 - Batch 1700/1875 - Loss: 0.0015",
    "Epoch 16/30 - Batch 1800/1875 - Loss: 0.0684",
    "Epoch 16 completed - Train Acc: 98.99% - Val Acc: 97.45%",
    "Epoch 17/30 - Batch 0/1875 - Loss: 0.0444",
    "Epoch 17/30 - Batch 100/1875 - Loss: 0.0337",
    "Epoch 17/30 - Batch 200/1875 - Loss: 0.0063",
    "Epoch 17/30 - Batch 300/1875 - Loss: 0.0088",
    "Epoch 17/30 - Batch 400/1875 - Loss: 0.0062"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.2638431134104729,
      "train_accuracy": 92.00166666666667,
      "val_loss": 0.16580912713913587,
      "val_accuracy": 95.27
    },
    {
      "epoch": 2,
      "train_loss": 0.12675037288268406,
      "train_accuracy": 96.34333333333333,
      "val_loss": 0.12696410293853394,
      "val_accuracy": 96.34
    },
    {
      "epoch": 3,
      "train_loss": 0.09817586986639848,
      "train_accuracy": 97.28166666666667,
      "val_loss": 0.09318060540149141,
      "val_accuracy": 97.26
    },
    {
      "epoch": 4,
      "train_loss": 0.08298046815150106,
      "train_accuracy": 97.71166666666667,
      "val_loss": 0.08464537754280346,
      "val_accuracy": 97.62
    },
    {
      "epoch": 5,
      "train_loss": 0.07134962903082681,
      "train_accuracy": 98.00833333333334,
      "val_loss": 0.0890312793761058,
      "val_accuracy": 97.72
    },
    {
      "epoch": 6,
      "train_loss": 0.06051112129442238,
      "train_accuracy": 98.21333333333334,
      "val_loss": 0.09082184579560948,
      "val_accuracy": 97.63
    },
    {
      "epoch": 7,
      "train_loss": 0.05867134064454586,
      "train_accuracy": 98.36333333333333,
      "val_loss": 0.10442738278334072,
      "val_accuracy": 97.43
    },
    {
      "epoch": 8,
      "train_loss": 0.05270667186833161,
      "train_accuracy": 98.47166666666666,
      "val_loss": 0.09366749548417501,
      "val_accuracy": 97.69
    },
    {
      "epoch": 9,
      "train_loss": 0.04926823566967505,
      "train_accuracy": 98.60166666666667,
      "val_loss": 0.08586664044227169,
      "val_accuracy": 97.85
    },
    {
      "epoch": 10,
      "train_loss": 0.04849062650950703,
      "train_accuracy": 98.605,
      "val_loss": 0.10149326025201591,
      "val_accuracy": 97.45
    },
    {
      "epoch": 11,
      "train_loss": 0.04297728562872702,
      "train_accuracy": 98.78333333333333,
      "val_loss": 0.0894390136258176,
      "val_accuracy": 97.65
    },
    {
      "epoch": 12,
      "train_loss": 0.04294195028770288,
      "train_accuracy": 98.77,
      "val_loss": 0.08537155916013009,
      "val_accuracy": 97.7
    },
    {
      "epoch": 13,
      "train_loss": 0.03934914935949394,
      "train_accuracy": 98.82833333333333,
      "val_loss": 0.08478745926569069,
      "val_accuracy": 97.82
    },
    {
      "epoch": 14,
      "train_loss": 0.039559802165753596,
      "train_accuracy": 98.85333333333334,
      "val_loss": 0.07762392635847097,
      "val_accuracy": 97.89
    },
    {
      "epoch": 15,
      "train_loss": 0.0371323371861974,
      "train_accuracy": 98.87333333333333,
      "val_loss": 0.09643922371129991,
      "val_accuracy": 97.73
    },
    {
      "epoch": 16,
      "train_loss": 0.03451225903237161,
      "train_accuracy": 98.99166666666666,
      "val_loss": 0.10369524506193711,
      "val_accuracy": 97.45
    }
  ]
}