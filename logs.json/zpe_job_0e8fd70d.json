{
  "job_id": "zpe_job_0e8fd70d",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.001,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T20:49:30.802Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3016",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.4221",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.4896",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.5373",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.1165",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.3033",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.2439",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.1386",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.4568",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2377",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.2519",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.3578",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.2268",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.0898",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.2800",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.2049",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.0576",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.0087",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1750",
    "Epoch 1 completed - Train Acc: 91.69% - Val Acc: 95.33%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.1930",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0808",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.1829",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.0191",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.0686",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.0979",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0712",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.0291",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.0126",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0942",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0429",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.1076",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.1895",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0257",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.1598",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.4875",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.1697",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.0102",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0095",
    "Epoch 2 completed - Train Acc: 96.36% - Val Acc: 96.39%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.1093",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0215",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0111",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0698",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.1769",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.1222",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.1101",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.4133",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.1281",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0087",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.1877",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0592",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.0926",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0823",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0633",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.1232",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0150",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.1644",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.2841",
    "Epoch 3 completed - Train Acc: 97.19% - Val Acc: 97.01%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.1176",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.0901",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0123",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0688",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0359",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0021",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0065",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0218",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0016",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0295",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.1508",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.1412",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0462",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0335",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0680",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.1561",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.4046",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0229",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0291",
    "Epoch 4 completed - Train Acc: 97.72% - Val Acc: 97.48%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0397",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0547",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.0025",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.2041",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0979",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.1008",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.4456",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0273",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.1538",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0272",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0296",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0214",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.0204",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0880",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0128",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0416",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0124",
    "Epoch 5 completed - Train Acc: 98.01% - Val Acc: 97.51%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.0064",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0528",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0707",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.1516",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0009",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.1682",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.2351",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0144",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.1824",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0026",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0315",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.4120",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0903",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0626",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.1091",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0152",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0583",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0053",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0502",
    "Epoch 6 completed - Train Acc: 98.19% - Val Acc: 97.79%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0593",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.2277",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0022",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0219",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0299",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0546",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0032",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0172",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0084",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.1527",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.0155",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.1518",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0554",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0921",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0135",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0045",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0146",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0780",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.1795",
    "Epoch 7 completed - Train Acc: 98.34% - Val Acc: 97.41%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0467",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0187",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0758",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.1130",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0756",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0118",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0052",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.1067",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0380",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0184",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0607",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0029",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0378",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0436",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0029",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.0048",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0436",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0040",
    "Epoch 8 completed - Train Acc: 98.53% - Val Acc: 97.95%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0633",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0197",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0407",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0183",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0074",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0116",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0048",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0085",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.0140",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0183",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0125",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.3390",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0440",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0627",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0065",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0994",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0157",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0259",
    "Epoch 9 completed - Train Acc: 98.58% - Val Acc: 97.71%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0025",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0027",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0193",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0595",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0831",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.0259",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0423",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0013",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0050",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0041",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0328",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0168",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.0199",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0984",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0028",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0021",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0196",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0200",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0405",
    "Epoch 10 completed - Train Acc: 98.62% - Val Acc: 97.87%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0017",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.0038",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0044",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0123",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0023",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0059",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0088",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0063",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0046",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.0017",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0005",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0250",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0206",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.1480",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.0116",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.2559",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.1271",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 11 completed - Train Acc: 98.75% - Val Acc: 97.46%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0018",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0242",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0982",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0441",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0183",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.0009",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.0053",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.1059",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0026",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0351",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.0183",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0075",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0867",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0501",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0044",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.1546",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0028",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.1603",
    "Epoch 12 completed - Train Acc: 98.83% - Val Acc: 97.74%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0074",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.0192",
    "Epoch 13/30 - Batch 400/1875 - Loss: 0.0024",
    "Epoch 13/30 - Batch 500/1875 - Loss: 0.0229",
    "Epoch 13/30 - Batch 600/1875 - Loss: 0.0282",
    "Epoch 13/30 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 13/30 - Batch 800/1875 - Loss: 0.1759",
    "Epoch 13/30 - Batch 900/1875 - Loss: 0.0048",
    "Epoch 13/30 - Batch 1000/1875 - Loss: 0.0012",
    "Epoch 13/30 - Batch 1100/1875 - Loss: 0.0732",
    "Epoch 13/30 - Batch 1200/1875 - Loss: 0.0082",
    "Epoch 13/30 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 13/30 - Batch 1400/1875 - Loss: 0.0022",
    "Epoch 13/30 - Batch 1500/1875 - Loss: 0.2116",
    "Epoch 13/30 - Batch 1600/1875 - Loss: 0.0016",
    "Epoch 13/30 - Batch 1700/1875 - Loss: 0.0573",
    "Epoch 13/30 - Batch 1800/1875 - Loss: 0.0799",
    "Epoch 13 completed - Train Acc: 98.81% - Val Acc: 97.65%",
    "Epoch 14/30 - Batch 0/1875 - Loss: 0.0837",
    "Epoch 14/30 - Batch 100/1875 - Loss: 0.0164",
    "Epoch 14/30 - Batch 200/1875 - Loss: 0.0116",
    "Epoch 14/30 - Batch 300/1875 - Loss: 0.2970",
    "Epoch 14/30 - Batch 400/1875 - Loss: 0.0128",
    "Epoch 14/30 - Batch 500/1875 - Loss: 0.0672",
    "Epoch 14/30 - Batch 600/1875 - Loss: 0.0643",
    "Epoch 14/30 - Batch 700/1875 - Loss: 0.0038",
    "Epoch 14/30 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 14/30 - Batch 900/1875 - Loss: 0.0167",
    "Epoch 14/30 - Batch 1000/1875 - Loss: 0.0027",
    "Epoch 14/30 - Batch 1100/1875 - Loss: 0.0175",
    "Epoch 14/30 - Batch 1200/1875 - Loss: 0.3629",
    "Epoch 14/30 - Batch 1300/1875 - Loss: 0.0061",
    "Epoch 14/30 - Batch 1400/1875 - Loss: 0.0110",
    "Epoch 14/30 - Batch 1500/1875 - Loss: 0.0277",
    "Epoch 14/30 - Batch 1600/1875 - Loss: 0.1659",
    "Epoch 14/30 - Batch 1700/1875 - Loss: 0.0089",
    "Epoch 14/30 - Batch 1800/1875 - Loss: 0.0287",
    "Epoch 14 completed - Train Acc: 98.86% - Val Acc: 97.24%",
    "Epoch 15/30 - Batch 0/1875 - Loss: 0.0597",
    "Epoch 15/30 - Batch 100/1875 - Loss: 0.0307",
    "Epoch 15/30 - Batch 200/1875 - Loss: 0.0043",
    "Epoch 15/30 - Batch 300/1875 - Loss: 0.0007",
    "Epoch 15/30 - Batch 400/1875 - Loss: 0.1622",
    "Epoch 15/30 - Batch 500/1875 - Loss: 0.3285",
    "Epoch 15/30 - Batch 600/1875 - Loss: 0.1313",
    "Epoch 15/30 - Batch 700/1875 - Loss: 0.0024",
    "Epoch 15/30 - Batch 800/1875 - Loss: 0.0394",
    "Epoch 15/30 - Batch 900/1875 - Loss: 0.0062",
    "Epoch 15/30 - Batch 1000/1875 - Loss: 0.0446",
    "Epoch 15/30 - Batch 1100/1875 - Loss: 0.5406",
    "Epoch 15/30 - Batch 1200/1875 - Loss: 0.0307",
    "Epoch 15/30 - Batch 1300/1875 - Loss: 0.0191",
    "Epoch 15/30 - Batch 1400/1875 - Loss: 0.0102",
    "Epoch 15/30 - Batch 1500/1875 - Loss: 0.0035",
    "Epoch 15/30 - Batch 1600/1875 - Loss: 0.0031",
    "Epoch 15/30 - Batch 1700/1875 - Loss: 0.0012",
    "Epoch 15/30 - Batch 1800/1875 - Loss: 0.0076",
    "Epoch 15 completed - Train Acc: 98.80% - Val Acc: 97.69%",
    "Epoch 16/30 - Batch 0/1875 - Loss: 0.0267",
    "Epoch 16/30 - Batch 100/1875 - Loss: 0.0082",
    "Epoch 16/30 - Batch 200/1875 - Loss: 0.1670",
    "Epoch 16/30 - Batch 300/1875 - Loss: 0.0029",
    "Epoch 16/30 - Batch 400/1875 - Loss: 0.0129",
    "Epoch 16/30 - Batch 500/1875 - Loss: 0.0051",
    "Epoch 16/30 - Batch 600/1875 - Loss: 0.0145",
    "Epoch 16/30 - Batch 700/1875 - Loss: 0.0634",
    "Epoch 16/30 - Batch 800/1875 - Loss: 0.0344",
    "Epoch 16/30 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 16/30 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 16/30 - Batch 1100/1875 - Loss: 0.0866",
    "Epoch 16/30 - Batch 1200/1875 - Loss: 0.0016",
    "Epoch 16/30 - Batch 1300/1875 - Loss: 0.0009",
    "Epoch 16/30 - Batch 1400/1875 - Loss: 0.0192",
    "Epoch 16/30 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 16/30 - Batch 1600/1875 - Loss: 0.0012",
    "Epoch 16/30 - Batch 1700/1875 - Loss: 0.0040",
    "Epoch 16/30 - Batch 1800/1875 - Loss: 0.0087",
    "Epoch 16 completed - Train Acc: 98.89% - Val Acc: 97.90%",
    "Epoch 17/30 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 17/30 - Batch 100/1875 - Loss: 0.0018",
    "Epoch 17/30 - Batch 200/1875 - Loss: 0.0037",
    "Epoch 17/30 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 17/30 - Batch 400/1875 - Loss: 0.0502",
    "Epoch 17/30 - Batch 500/1875 - Loss: 0.0013",
    "Epoch 17/30 - Batch 600/1875 - Loss: 0.0718",
    "Epoch 17/30 - Batch 700/1875 - Loss: 0.0073",
    "Epoch 17/30 - Batch 800/1875 - Loss: 0.0154",
    "Epoch 17/30 - Batch 900/1875 - Loss: 0.0644",
    "Epoch 17/30 - Batch 1000/1875 - Loss: 0.0033",
    "Epoch 17/30 - Batch 1100/1875 - Loss: 0.0942",
    "Epoch 17/30 - Batch 1200/1875 - Loss: 0.0042",
    "Epoch 17/30 - Batch 1300/1875 - Loss: 0.0621",
    "Epoch 17/30 - Batch 1400/1875 - Loss: 0.0014",
    "Epoch 17/30 - Batch 1500/1875 - Loss: 0.0484",
    "Epoch 17/30 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 17/30 - Batch 1700/1875 - Loss: 0.0509",
    "Epoch 17/30 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 17 completed - Train Acc: 98.94% - Val Acc: 97.93%",
    "Epoch 18/30 - Batch 0/1875 - Loss: 0.0116",
    "Epoch 18/30 - Batch 100/1875 - Loss: 0.0232",
    "Epoch 18/30 - Batch 200/1875 - Loss: 0.0038",
    "Epoch 18/30 - Batch 300/1875 - Loss: 0.0115",
    "Epoch 18/30 - Batch 400/1875 - Loss: 0.4225",
    "Epoch 18/30 - Batch 500/1875 - Loss: 0.0179",
    "Epoch 18/30 - Batch 600/1875 - Loss: 0.0291",
    "Epoch 18/30 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 18/30 - Batch 800/1875 - Loss: 0.1288",
    "Epoch 18/30 - Batch 900/1875 - Loss: 0.1839",
    "Epoch 18/30 - Batch 1000/1875 - Loss: 0.1874",
    "Epoch 18/30 - Batch 1100/1875 - Loss: 0.0055",
    "Epoch 18/30 - Batch 1200/1875 - Loss: 0.1264",
    "Epoch 18/30 - Batch 1300/1875 - Loss: 0.0006",
    "Epoch 18/30 - Batch 1400/1875 - Loss: 0.0240",
    "Epoch 18/30 - Batch 1500/1875 - Loss: 0.0403",
    "Epoch 18/30 - Batch 1600/1875 - Loss: 0.0056",
    "Epoch 18/30 - Batch 1700/1875 - Loss: 0.0228",
    "Epoch 18/30 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 18 completed - Train Acc: 98.91% - Val Acc: 98.02%",
    "Epoch 19/30 - Batch 0/1875 - Loss: 0.0011",
    "Epoch 19/30 - Batch 100/1875 - Loss: 0.0047",
    "Epoch 19/30 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 19/30 - Batch 300/1875 - Loss: 0.0753",
    "Epoch 19/30 - Batch 400/1875 - Loss: 0.2758",
    "Epoch 19/30 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 19/30 - Batch 600/1875 - Loss: 0.0043",
    "Epoch 19/30 - Batch 700/1875 - Loss: 0.0065",
    "Epoch 19/30 - Batch 800/1875 - Loss: 0.0291",
    "Epoch 19/30 - Batch 900/1875 - Loss: 0.0631",
    "Epoch 19/30 - Batch 1000/1875 - Loss: 0.0041",
    "Epoch 19/30 - Batch 1100/1875 - Loss: 0.0781",
    "Epoch 19/30 - Batch 1200/1875 - Loss: 0.0009",
    "Epoch 19/30 - Batch 1300/1875 - Loss: 0.0230",
    "Epoch 19/30 - Batch 1400/1875 - Loss: 0.0064",
    "Epoch 19/30 - Batch 1500/1875 - Loss: 0.0033",
    "Epoch 19/30 - Batch 1600/1875 - Loss: 0.0009",
    "Epoch 19/30 - Batch 1700/1875 - Loss: 0.0183",
    "Epoch 19/30 - Batch 1800/1875 - Loss: 0.0068",
    "Epoch 19 completed - Train Acc: 99.00% - Val Acc: 97.82%",
    "Epoch 20/30 - Batch 0/1875 - Loss: 0.0536",
    "Epoch 20/30 - Batch 100/1875 - Loss: 0.0048",
    "Epoch 20/30 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 20/30 - Batch 300/1875 - Loss: 0.0193",
    "Epoch 20/30 - Batch 400/1875 - Loss: 0.1277",
    "Epoch 20/30 - Batch 500/1875 - Loss: 0.0556",
    "Epoch 20/30 - Batch 600/1875 - Loss: 0.0012",
    "Epoch 20/30 - Batch 700/1875 - Loss: 0.0143",
    "Epoch 20/30 - Batch 800/1875 - Loss: 0.0216",
    "Epoch 20/30 - Batch 900/1875 - Loss: 0.0051",
    "Epoch 20/30 - Batch 1000/1875 - Loss: 0.0021",
    "Epoch 20/30 - Batch 1100/1875 - Loss: 0.0349",
    "Epoch 20/30 - Batch 1200/1875 - Loss: 0.1152",
    "Epoch 20/30 - Batch 1300/1875 - Loss: 0.0233",
    "Epoch 20/30 - Batch 1400/1875 - Loss: 0.0239",
    "Epoch 20/30 - Batch 1500/1875 - Loss: 0.0456",
    "Epoch 20/30 - Batch 1600/1875 - Loss: 0.0009",
    "Epoch 20/30 - Batch 1700/1875 - Loss: 0.0592",
    "Epoch 20/30 - Batch 1800/1875 - Loss: 0.0170",
    "Epoch 20 completed - Train Acc: 99.02% - Val Acc: 97.84%",
    "Epoch 21/30 - Batch 0/1875 - Loss: 0.0022",
    "Epoch 21/30 - Batch 100/1875 - Loss: 0.0143",
    "Epoch 21/30 - Batch 200/1875 - Loss: 0.0711",
    "Epoch 21/30 - Batch 300/1875 - Loss: 0.0047",
    "Epoch 21/30 - Batch 400/1875 - Loss: 0.0207",
    "Epoch 21/30 - Batch 500/1875 - Loss: 0.0845",
    "Epoch 21/30 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 21/30 - Batch 700/1875 - Loss: 0.0089",
    "Epoch 21/30 - Batch 800/1875 - Loss: 0.3219",
    "Epoch 21/30 - Batch 900/1875 - Loss: 0.0684",
    "Epoch 21/30 - Batch 1000/1875 - Loss: 0.0056",
    "Epoch 21/30 - Batch 1100/1875 - Loss: 0.0672",
    "Epoch 21/30 - Batch 1200/1875 - Loss: 0.0183",
    "Epoch 21/30 - Batch 1300/1875 - Loss: 0.0608",
    "Epoch 21/30 - Batch 1400/1875 - Loss: 0.0511",
    "Epoch 21/30 - Batch 1500/1875 - Loss: 0.0799",
    "Epoch 21/30 - Batch 1600/1875 - Loss: 0.0338",
    "Epoch 21/30 - Batch 1700/1875 - Loss: 0.0170",
    "Epoch 21/30 - Batch 1800/1875 - Loss: 0.0126",
    "Epoch 21 completed - Train Acc: 99.04% - Val Acc: 97.53%",
    "Epoch 22/30 - Batch 0/1875 - Loss: 0.0222",
    "Epoch 22/30 - Batch 100/1875 - Loss: 0.0237",
    "Epoch 22/30 - Batch 200/1875 - Loss: 0.0033",
    "Epoch 22/30 - Batch 300/1875 - Loss: 0.0088",
    "Epoch 22/30 - Batch 400/1875 - Loss: 0.0013",
    "Epoch 22/30 - Batch 500/1875 - Loss: 0.0039",
    "Epoch 22/30 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 22/30 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 22/30 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 22/30 - Batch 900/1875 - Loss: 0.1822",
    "Epoch 22/30 - Batch 1000/1875 - Loss: 0.0012",
    "Epoch 22/30 - Batch 1100/1875 - Loss: 0.0643",
    "Epoch 22/30 - Batch 1200/1875 - Loss: 0.0153",
    "Epoch 22/30 - Batch 1300/1875 - Loss: 0.0565",
    "Epoch 22/30 - Batch 1400/1875 - Loss: 0.0102",
    "Epoch 22/30 - Batch 1500/1875 - Loss: 0.0089",
    "Epoch 22/30 - Batch 1600/1875 - Loss: 0.0136",
    "Epoch 22/30 - Batch 1700/1875 - Loss: 0.0452",
    "Epoch 22/30 - Batch 1800/1875 - Loss: 0.1772",
    "Epoch 22 completed - Train Acc: 99.03% - Val Acc: 97.96%",
    "Epoch 23/30 - Batch 0/1875 - Loss: 0.0018",
    "Epoch 23/30 - Batch 100/1875 - Loss: 0.0014",
    "Epoch 23/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 23/30 - Batch 300/1875 - Loss: 0.0265",
    "Epoch 23/30 - Batch 400/1875 - Loss: 0.0852",
    "Epoch 23/30 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 23/30 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 23/30 - Batch 700/1875 - Loss: 0.0051",
    "Epoch 23/30 - Batch 800/1875 - Loss: 0.0148",
    "Epoch 23/30 - Batch 900/1875 - Loss: 0.0514",
    "Epoch 23/30 - Batch 1000/1875 - Loss: 0.0020",
    "Epoch 23/30 - Batch 1100/1875 - Loss: 0.0014",
    "Epoch 23/30 - Batch 1200/1875 - Loss: 0.0663",
    "Epoch 23/30 - Batch 1300/1875 - Loss: 0.0054",
    "Epoch 23/30 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 23/30 - Batch 1500/1875 - Loss: 0.0209",
    "Epoch 23/30 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 23/30 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 23/30 - Batch 1800/1875 - Loss: 0.2548",
    "Epoch 23 completed - Train Acc: 99.10% - Val Acc: 97.92%",
    "Epoch 24/30 - Batch 0/1875 - Loss: 0.0030",
    "Epoch 24/30 - Batch 100/1875 - Loss: 0.0735",
    "Epoch 24/30 - Batch 200/1875 - Loss: 0.0195",
    "Epoch 24/30 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 24/30 - Batch 400/1875 - Loss: 0.1082",
    "Epoch 24/30 - Batch 500/1875 - Loss: 0.0290",
    "Epoch 24/30 - Batch 600/1875 - Loss: 0.0294",
    "Epoch 24/30 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 24/30 - Batch 800/1875 - Loss: 0.0070",
    "Epoch 24/30 - Batch 900/1875 - Loss: 0.0015",
    "Epoch 24/30 - Batch 1000/1875 - Loss: 0.0027",
    "Epoch 24/30 - Batch 1100/1875 - Loss: 0.0020",
    "Epoch 24/30 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 24/30 - Batch 1300/1875 - Loss: 0.0651",
    "Epoch 24/30 - Batch 1400/1875 - Loss: 0.0101",
    "Epoch 24/30 - Batch 1500/1875 - Loss: 0.0020",
    "Epoch 24/30 - Batch 1600/1875 - Loss: 0.0018",
    "Epoch 24/30 - Batch 1700/1875 - Loss: 0.0852",
    "Epoch 24/30 - Batch 1800/1875 - Loss: 0.0121",
    "Epoch 24 completed - Train Acc: 99.13% - Val Acc: 97.55%",
    "Epoch 25/30 - Batch 0/1875 - Loss: 0.0769",
    "Epoch 25/30 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 25/30 - Batch 200/1875 - Loss: 0.0103",
    "Epoch 25/30 - Batch 300/1875 - Loss: 0.0343",
    "Epoch 25/30 - Batch 400/1875 - Loss: 0.0900",
    "Epoch 25/30 - Batch 500/1875 - Loss: 0.0054",
    "Epoch 25/30 - Batch 600/1875 - Loss: 0.0031",
    "Epoch 25/30 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 25/30 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 25/30 - Batch 900/1875 - Loss: 0.0257",
    "Epoch 25/30 - Batch 1000/1875 - Loss: 0.0287",
    "Epoch 25/30 - Batch 1100/1875 - Loss: 0.1521",
    "Epoch 25/30 - Batch 1200/1875 - Loss: 0.1631",
    "Epoch 25/30 - Batch 1300/1875 - Loss: 0.0176",
    "Epoch 25/30 - Batch 1400/1875 - Loss: 0.3172",
    "Epoch 25/30 - Batch 1500/1875 - Loss: 0.0035",
    "Epoch 25/30 - Batch 1600/1875 - Loss: 0.0294",
    "Epoch 25/30 - Batch 1700/1875 - Loss: 0.0203",
    "Epoch 25/30 - Batch 1800/1875 - Loss: 0.0164",
    "Epoch 25 completed - Train Acc: 99.08% - Val Acc: 98.03%",
    "Epoch 26/30 - Batch 0/1875 - Loss: 0.0157",
    "Epoch 26/30 - Batch 100/1875 - Loss: 0.0035",
    "Epoch 26/30 - Batch 200/1875 - Loss: 0.0040",
    "Epoch 26/30 - Batch 300/1875 - Loss: 0.0023",
    "Epoch 26/30 - Batch 400/1875 - Loss: 0.0709",
    "Epoch 26/30 - Batch 500/1875 - Loss: 0.0239",
    "Epoch 26/30 - Batch 600/1875 - Loss: 0.0037",
    "Epoch 26/30 - Batch 700/1875 - Loss: 0.0015",
    "Epoch 26/30 - Batch 800/1875 - Loss: 0.0088",
    "Epoch 26/30 - Batch 900/1875 - Loss: 0.0032",
    "Epoch 26/30 - Batch 1000/1875 - Loss: 0.0530",
    "Epoch 26/30 - Batch 1100/1875 - Loss: 0.0094",
    "Epoch 26/30 - Batch 1200/1875 - Loss: 0.0025",
    "Epoch 26/30 - Batch 1300/1875 - Loss: 0.0551",
    "Epoch 26/30 - Batch 1400/1875 - Loss: 0.0698",
    "Epoch 26/30 - Batch 1500/1875 - Loss: 0.0145",
    "Epoch 26/30 - Batch 1600/1875 - Loss: 0.0936",
    "Epoch 26/30 - Batch 1700/1875 - Loss: 0.0664",
    "Epoch 26/30 - Batch 1800/1875 - Loss: 0.0030",
    "Epoch 26 completed - Train Acc: 99.16% - Val Acc: 97.92%",
    "Epoch 27/30 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 27/30 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 27/30 - Batch 200/1875 - Loss: 0.0091",
    "Epoch 27/30 - Batch 300/1875 - Loss: 0.0024",
    "Epoch 27/30 - Batch 400/1875 - Loss: 0.0087",
    "Epoch 27/30 - Batch 500/1875 - Loss: 0.0252",
    "Epoch 27/30 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 27/30 - Batch 700/1875 - Loss: 0.0285",
    "Epoch 27/30 - Batch 800/1875 - Loss: 0.0027",
    "Epoch 27/30 - Batch 900/1875 - Loss: 0.0648",
    "Epoch 27/30 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 27/30 - Batch 1100/1875 - Loss: 0.0257",
    "Epoch 27/30 - Batch 1200/1875 - Loss: 0.0011",
    "Epoch 27/30 - Batch 1300/1875 - Loss: 0.0262",
    "Epoch 27/30 - Batch 1400/1875 - Loss: 0.0051",
    "Epoch 27/30 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 27/30 - Batch 1600/1875 - Loss: 0.0012",
    "Epoch 27/30 - Batch 1700/1875 - Loss: 0.0344",
    "Epoch 27/30 - Batch 1800/1875 - Loss: 0.0064",
    "Epoch 27 completed - Train Acc: 99.15% - Val Acc: 97.44%",
    "Epoch 28/30 - Batch 0/1875 - Loss: 0.0015",
    "Epoch 28/30 - Batch 100/1875 - Loss: 0.0018",
    "Epoch 28/30 - Batch 200/1875 - Loss: 0.0031",
    "Epoch 28/30 - Batch 300/1875 - Loss: 0.0148",
    "Epoch 28/30 - Batch 400/1875 - Loss: 0.0005",
    "Epoch 28/30 - Batch 500/1875 - Loss: 0.5469",
    "Epoch 28/30 - Batch 600/1875 - Loss: 0.0174",
    "Epoch 28/30 - Batch 700/1875 - Loss: 0.0402",
    "Epoch 28/30 - Batch 800/1875 - Loss: 0.0379",
    "Epoch 28/30 - Batch 900/1875 - Loss: 0.0130",
    "Epoch 28/30 - Batch 1000/1875 - Loss: 0.0038",
    "Epoch 28/30 - Batch 1100/1875 - Loss: 0.0053",
    "Epoch 28/30 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 28/30 - Batch 1300/1875 - Loss: 0.0028",
    "Epoch 28/30 - Batch 1400/1875 - Loss: 0.0241",
    "Epoch 28/30 - Batch 1500/1875 - Loss: 0.1094",
    "Epoch 28/30 - Batch 1600/1875 - Loss: 0.0347",
    "Epoch 28/30 - Batch 1700/1875 - Loss: 0.0067",
    "Epoch 28/30 - Batch 1800/1875 - Loss: 0.0635",
    "Epoch 28 completed - Train Acc: 99.09% - Val Acc: 97.80%",
    "Epoch 29/30 - Batch 0/1875 - Loss: 0.0216",
    "Epoch 29/30 - Batch 100/1875 - Loss: 0.0079",
    "Epoch 29/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 29/30 - Batch 300/1875 - Loss: 0.0510",
    "Epoch 29/30 - Batch 400/1875 - Loss: 0.0020",
    "Epoch 29/30 - Batch 500/1875 - Loss: 0.0576",
    "Epoch 29/30 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 29/30 - Batch 700/1875 - Loss: 0.1550",
    "Epoch 29/30 - Batch 800/1875 - Loss: 0.0118",
    "Epoch 29/30 - Batch 900/1875 - Loss: 0.0538",
    "Epoch 29/30 - Batch 1000/1875 - Loss: 0.0021",
    "Epoch 29/30 - Batch 1100/1875 - Loss: 0.0015",
    "Epoch 29/30 - Batch 1200/1875 - Loss: 0.0035",
    "Epoch 29/30 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 29/30 - Batch 1400/1875 - Loss: 0.0138",
    "Epoch 29/30 - Batch 1500/1875 - Loss: 0.0106",
    "Epoch 29/30 - Batch 1600/1875 - Loss: 0.0240",
    "Epoch 29/30 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 29/30 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 29 completed - Train Acc: 99.13% - Val Acc: 97.83%",
    "Epoch 30/30 - Batch 0/1875 - Loss: 0.0161",
    "Epoch 30/30 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 30/30 - Batch 200/1875 - Loss: 0.0037",
    "Epoch 30/30 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 30/30 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 30/30 - Batch 500/1875 - Loss: 0.0078",
    "Epoch 30/30 - Batch 600/1875 - Loss: 0.1303",
    "Epoch 30/30 - Batch 700/1875 - Loss: 0.0580",
    "Epoch 30/30 - Batch 800/1875 - Loss: 0.0656",
    "Epoch 30/30 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 30/30 - Batch 1000/1875 - Loss: 0.0489",
    "Epoch 30/30 - Batch 1100/1875 - Loss: 0.0273",
    "Epoch 30/30 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 30/30 - Batch 1300/1875 - Loss: 0.0149",
    "Epoch 30/30 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 30/30 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 30/30 - Batch 1600/1875 - Loss: 0.0011",
    "Epoch 30/30 - Batch 1700/1875 - Loss: 0.0024",
    "Epoch 30/30 - Batch 1800/1875 - Loss: 0.0024",
    "Epoch 30 completed - Train Acc: 99.19% - Val Acc: 97.88%",
    "Training completed. Model saved to models/ZPE-QuantumWeaver-V1.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.27213536508033676,
      "train_accuracy": 91.69166666666666,
      "val_loss": 0.15251209455082557,
      "val_accuracy": 95.33
    },
    {
      "epoch": 2,
      "train_loss": 0.12822863440737128,
      "train_accuracy": 96.355,
      "val_loss": 0.13638505745151366,
      "val_accuracy": 96.39
    },
    {
      "epoch": 3,
      "train_loss": 0.1000637354457751,
      "train_accuracy": 97.19333333333333,
      "val_loss": 0.10169479123204381,
      "val_accuracy": 97.01
    },
    {
      "epoch": 4,
      "train_loss": 0.08060383818006764,
      "train_accuracy": 97.715,
      "val_loss": 0.09259538642710684,
      "val_accuracy": 97.48
    },
    {
      "epoch": 5,
      "train_loss": 0.07152997391140088,
      "train_accuracy": 98.01166666666667,
      "val_loss": 0.09431264317383768,
      "val_accuracy": 97.51
    },
    {
      "epoch": 6,
      "train_loss": 0.06289708329279674,
      "train_accuracy": 98.18666666666667,
      "val_loss": 0.0895386523780049,
      "val_accuracy": 97.79
    },
    {
      "epoch": 7,
      "train_loss": 0.059125906327083554,
      "train_accuracy": 98.34,
      "val_loss": 0.0927551246998144,
      "val_accuracy": 97.41
    },
    {
      "epoch": 8,
      "train_loss": 0.05348626880086376,
      "train_accuracy": 98.52666666666667,
      "val_loss": 0.07704983243496327,
      "val_accuracy": 97.95
    },
    {
      "epoch": 9,
      "train_loss": 0.05076220918778175,
      "train_accuracy": 98.58333333333333,
      "val_loss": 0.09219405832299672,
      "val_accuracy": 97.71
    },
    {
      "epoch": 10,
      "train_loss": 0.046703044236633774,
      "train_accuracy": 98.62,
      "val_loss": 0.08855702081192297,
      "val_accuracy": 97.87
    },
    {
      "epoch": 11,
      "train_loss": 0.044421452842322955,
      "train_accuracy": 98.75,
      "val_loss": 0.0986475633935552,
      "val_accuracy": 97.46
    },
    {
      "epoch": 12,
      "train_loss": 0.04159364921221665,
      "train_accuracy": 98.83333333333333,
      "val_loss": 0.09199859332112303,
      "val_accuracy": 97.74
    },
    {
      "epoch": 13,
      "train_loss": 0.039199233933039555,
      "train_accuracy": 98.81166666666667,
      "val_loss": 0.0948120490845268,
      "val_accuracy": 97.65
    },
    {
      "epoch": 14,
      "train_loss": 0.038271757093360066,
      "train_accuracy": 98.855,
      "val_loss": 0.1139455288575439,
      "val_accuracy": 97.24
    },
    {
      "epoch": 15,
      "train_loss": 0.03893042102965604,
      "train_accuracy": 98.8,
      "val_loss": 0.08643585969894803,
      "val_accuracy": 97.69
    },
    {
      "epoch": 16,
      "train_loss": 0.03568578211204343,
      "train_accuracy": 98.895,
      "val_loss": 0.08207230207870034,
      "val_accuracy": 97.9
    },
    {
      "epoch": 17,
      "train_loss": 0.03501320391448971,
      "train_accuracy": 98.945,
      "val_loss": 0.0789590847389176,
      "val_accuracy": 97.93
    },
    {
      "epoch": 18,
      "train_loss": 0.03583857198964882,
      "train_accuracy": 98.91,
      "val_loss": 0.08308818633604198,
      "val_accuracy": 98.02
    },
    {
      "epoch": 19,
      "train_loss": 0.032370323446567636,
      "train_accuracy": 98.99833333333333,
      "val_loss": 0.08209855374686846,
      "val_accuracy": 97.82
    },
    {
      "epoch": 20,
      "train_loss": 0.03299134559790788,
      "train_accuracy": 99.01833333333333,
      "val_loss": 0.08993722462525712,
      "val_accuracy": 97.84
    },
    {
      "epoch": 21,
      "train_loss": 0.03273046323905388,
      "train_accuracy": 99.03833333333333,
      "val_loss": 0.10139542778607849,
      "val_accuracy": 97.53
    },
    {
      "epoch": 22,
      "train_loss": 0.03190176259399765,
      "train_accuracy": 99.02666666666667,
      "val_loss": 0.08267538056292434,
      "val_accuracy": 97.96
    },
    {
      "epoch": 23,
      "train_loss": 0.02896968896800439,
      "train_accuracy": 99.09833333333333,
      "val_loss": 0.08432425402716558,
      "val_accuracy": 97.92
    },
    {
      "epoch": 24,
      "train_loss": 0.030866564505723244,
      "train_accuracy": 99.12833333333333,
      "val_loss": 0.0936604002704153,
      "val_accuracy": 97.55
    },
    {
      "epoch": 25,
      "train_loss": 0.030568140985860373,
      "train_accuracy": 99.085,
      "val_loss": 0.08967283687774799,
      "val_accuracy": 98.03
    },
    {
      "epoch": 26,
      "train_loss": 0.027872810341267904,
      "train_accuracy": 99.16333333333333,
      "val_loss": 0.08106437436324083,
      "val_accuracy": 97.92
    },
    {
      "epoch": 27,
      "train_loss": 0.028460946680337655,
      "train_accuracy": 99.15,
      "val_loss": 0.10043568144825409,
      "val_accuracy": 97.44
    },
    {
      "epoch": 28,
      "train_loss": 0.030482505818665958,
      "train_accuracy": 99.095,
      "val_loss": 0.09396973542545554,
      "val_accuracy": 97.8
    },
    {
      "epoch": 29,
      "train_loss": 0.028366600625378973,
      "train_accuracy": 99.13166666666666,
      "val_loss": 0.08951487488704266,
      "val_accuracy": 97.83
    },
    {
      "epoch": 30,
      "train_loss": 0.026469847277071676,
      "train_accuracy": 99.18666666666667,
      "val_loss": 0.09892243782957294,
      "val_accuracy": 97.88
    }
  ]
}