{
  "job_id": "zpe_job_1deccd81",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-Colab-Sim_hnn_step8",
    "totalEpochs": 80,
    "batchSize": 32,
    "learningRate": 0.00135,
    "weightDecay": 1e-05,
    "momentumParams": [
      0.965,
      0.855,
      0.785,
      0.725,
      0.415,
      0.755
    ],
    "strengthParams": [
      0.185,
      0.545,
      0.405,
      0.595,
      0.365,
      0.435
    ],
    "noiseParams": [
      0.165,
      0.265,
      0.245,
      0.435,
      0.285,
      0.235
    ],
    "couplingParams": [
      0.78,
      0.83,
      0.8,
      0.77,
      0.74,
      0.63
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.07,
    "quantumMode": true,
    "baseConfigId": "zpe_job_86f76b88",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T20:45:07.043Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/80 - Batch 0/1875 - Loss: 2.2963",
    "Epoch 1/80 - Batch 100/1875 - Loss: 0.4594",
    "Epoch 1/80 - Batch 200/1875 - Loss: 0.5895",
    "Epoch 1/80 - Batch 300/1875 - Loss: 0.0807",
    "Epoch 1/80 - Batch 400/1875 - Loss: 0.2979",
    "Epoch 1/80 - Batch 500/1875 - Loss: 0.3876",
    "Epoch 1/80 - Batch 600/1875 - Loss: 0.3142",
    "Epoch 1/80 - Batch 700/1875 - Loss: 0.1973",
    "Epoch 1/80 - Batch 800/1875 - Loss: 0.0690",
    "Epoch 1/80 - Batch 900/1875 - Loss: 0.1213",
    "Epoch 1/80 - Batch 1000/1875 - Loss: 0.2032",
    "Epoch 1/80 - Batch 1100/1875 - Loss: 0.0421",
    "Epoch 1/80 - Batch 1200/1875 - Loss: 0.0386",
    "Epoch 1/80 - Batch 1300/1875 - Loss: 0.1873",
    "Epoch 1/80 - Batch 1400/1875 - Loss: 0.0522",
    "Epoch 1/80 - Batch 1500/1875 - Loss: 0.2208",
    "Epoch 1/80 - Batch 1600/1875 - Loss: 0.1371",
    "Epoch 1/80 - Batch 1700/1875 - Loss: 0.0989",
    "Epoch 1/80 - Batch 1800/1875 - Loss: 0.0195",
    "Epoch 1 completed - Train Acc: 91.47% - Val Acc: 94.60%",
    "Epoch 2/80 - Batch 0/1875 - Loss: 0.0830",
    "Epoch 2/80 - Batch 100/1875 - Loss: 0.1487",
    "Epoch 2/80 - Batch 200/1875 - Loss: 0.2040",
    "Epoch 2/80 - Batch 300/1875 - Loss: 0.1271",
    "Epoch 2/80 - Batch 400/1875 - Loss: 0.0367",
    "Epoch 2/80 - Batch 500/1875 - Loss: 0.1050",
    "Epoch 2/80 - Batch 600/1875 - Loss: 0.0181",
    "Epoch 2/80 - Batch 700/1875 - Loss: 0.2123",
    "Epoch 2/80 - Batch 800/1875 - Loss: 0.2652",
    "Epoch 2/80 - Batch 900/1875 - Loss: 0.0658",
    "Epoch 2/80 - Batch 1000/1875 - Loss: 0.1062",
    "Epoch 2/80 - Batch 1100/1875 - Loss: 0.0304",
    "Epoch 2/80 - Batch 1200/1875 - Loss: 0.1749",
    "Epoch 2/80 - Batch 1300/1875 - Loss: 0.1032",
    "Epoch 2/80 - Batch 1400/1875 - Loss: 0.0434",
    "Epoch 2/80 - Batch 1500/1875 - Loss: 0.0086",
    "Epoch 2/80 - Batch 1600/1875 - Loss: 0.0927",
    "Epoch 2/80 - Batch 1700/1875 - Loss: 0.0141",
    "Epoch 2/80 - Batch 1800/1875 - Loss: 0.0476",
    "Epoch 2 completed - Train Acc: 96.23% - Val Acc: 96.31%",
    "Epoch 3/80 - Batch 0/1875 - Loss: 0.3281",
    "Epoch 3/80 - Batch 100/1875 - Loss: 0.1153",
    "Epoch 3/80 - Batch 200/1875 - Loss: 0.0266",
    "Epoch 3/80 - Batch 300/1875 - Loss: 0.0029",
    "Epoch 3/80 - Batch 400/1875 - Loss: 0.0090",
    "Epoch 3/80 - Batch 500/1875 - Loss: 0.0200",
    "Epoch 3/80 - Batch 600/1875 - Loss: 0.1035",
    "Epoch 3/80 - Batch 700/1875 - Loss: 0.1279",
    "Epoch 3/80 - Batch 800/1875 - Loss: 0.0460",
    "Epoch 3/80 - Batch 900/1875 - Loss: 0.0161",
    "Epoch 3/80 - Batch 1000/1875 - Loss: 0.0168",
    "Epoch 3/80 - Batch 1100/1875 - Loss: 0.1597",
    "Epoch 3/80 - Batch 1200/1875 - Loss: 0.0121",
    "Epoch 3/80 - Batch 1300/1875 - Loss: 0.2308",
    "Epoch 3/80 - Batch 1400/1875 - Loss: 0.1087",
    "Epoch 3/80 - Batch 1500/1875 - Loss: 0.0450",
    "Epoch 3/80 - Batch 1600/1875 - Loss: 0.0052",
    "Epoch 3/80 - Batch 1700/1875 - Loss: 0.0578",
    "Epoch 3/80 - Batch 1800/1875 - Loss: 0.1009",
    "Epoch 3 completed - Train Acc: 97.11% - Val Acc: 96.44%",
    "Epoch 4/80 - Batch 0/1875 - Loss: 0.0223",
    "Epoch 4/80 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 4/80 - Batch 200/1875 - Loss: 0.1533",
    "Epoch 4/80 - Batch 300/1875 - Loss: 0.0091",
    "Epoch 4/80 - Batch 400/1875 - Loss: 0.0083",
    "Epoch 4/80 - Batch 500/1875 - Loss: 0.1313",
    "Epoch 4/80 - Batch 600/1875 - Loss: 0.0317",
    "Epoch 4/80 - Batch 700/1875 - Loss: 0.0185",
    "Epoch 4/80 - Batch 800/1875 - Loss: 0.0137",
    "Epoch 4/80 - Batch 900/1875 - Loss: 0.0034",
    "Epoch 4/80 - Batch 1000/1875 - Loss: 0.0192",
    "Epoch 4/80 - Batch 1100/1875 - Loss: 0.2168",
    "Epoch 4/80 - Batch 1200/1875 - Loss: 0.0976",
    "Epoch 4/80 - Batch 1300/1875 - Loss: 0.0256",
    "Epoch 4/80 - Batch 1400/1875 - Loss: 0.1288",
    "Epoch 4/80 - Batch 1500/1875 - Loss: 0.1036",
    "Epoch 4/80 - Batch 1600/1875 - Loss: 0.1531",
    "Epoch 4/80 - Batch 1700/1875 - Loss: 0.0022",
    "Epoch 4/80 - Batch 1800/1875 - Loss: 0.3973",
    "Epoch 4 completed - Train Acc: 97.70% - Val Acc: 97.26%",
    "Epoch 5/80 - Batch 0/1875 - Loss: 0.0935",
    "Epoch 5/80 - Batch 100/1875 - Loss: 0.0045",
    "Epoch 5/80 - Batch 200/1875 - Loss: 0.0500",
    "Epoch 5/80 - Batch 300/1875 - Loss: 0.1369",
    "Epoch 5/80 - Batch 400/1875 - Loss: 0.0472",
    "Epoch 5/80 - Batch 500/1875 - Loss: 0.0422",
    "Epoch 5/80 - Batch 600/1875 - Loss: 0.0381",
    "Epoch 5/80 - Batch 700/1875 - Loss: 0.0267",
    "Epoch 5/80 - Batch 800/1875 - Loss: 0.0162",
    "Epoch 5/80 - Batch 900/1875 - Loss: 0.0252",
    "Epoch 5/80 - Batch 1000/1875 - Loss: 0.0315",
    "Epoch 5/80 - Batch 1100/1875 - Loss: 0.0538",
    "Epoch 5/80 - Batch 1200/1875 - Loss: 0.2195",
    "Epoch 5/80 - Batch 1300/1875 - Loss: 0.2587",
    "Epoch 5/80 - Batch 1400/1875 - Loss: 0.0096",
    "Epoch 5/80 - Batch 1500/1875 - Loss: 0.0351",
    "Epoch 5/80 - Batch 1600/1875 - Loss: 0.0377",
    "Epoch 5/80 - Batch 1700/1875 - Loss: 0.0695",
    "Epoch 5/80 - Batch 1800/1875 - Loss: 0.1844",
    "Epoch 5 completed - Train Acc: 97.96% - Val Acc: 96.97%",
    "Epoch 6/80 - Batch 0/1875 - Loss: 0.0027",
    "Epoch 6/80 - Batch 100/1875 - Loss: 0.0785",
    "Epoch 6/80 - Batch 200/1875 - Loss: 0.0252",
    "Epoch 6/80 - Batch 300/1875 - Loss: 0.0029",
    "Epoch 6/80 - Batch 400/1875 - Loss: 0.0046",
    "Epoch 6/80 - Batch 500/1875 - Loss: 0.0323",
    "Epoch 6/80 - Batch 600/1875 - Loss: 0.0030",
    "Epoch 6/80 - Batch 700/1875 - Loss: 0.0110",
    "Epoch 6/80 - Batch 800/1875 - Loss: 0.0049",
    "Epoch 6/80 - Batch 900/1875 - Loss: 0.0014",
    "Epoch 6/80 - Batch 1000/1875 - Loss: 0.1386",
    "Epoch 6/80 - Batch 1100/1875 - Loss: 0.0048",
    "Epoch 6/80 - Batch 1200/1875 - Loss: 0.0948",
    "Epoch 6/80 - Batch 1300/1875 - Loss: 0.0027",
    "Epoch 6/80 - Batch 1400/1875 - Loss: 0.0797",
    "Epoch 6/80 - Batch 1500/1875 - Loss: 0.0533",
    "Epoch 6/80 - Batch 1600/1875 - Loss: 0.0529",
    "Epoch 6/80 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 6/80 - Batch 1800/1875 - Loss: 0.1658",
    "Epoch 6 completed - Train Acc: 98.18% - Val Acc: 97.33%",
    "Epoch 7/80 - Batch 0/1875 - Loss: 0.3422",
    "Epoch 7/80 - Batch 100/1875 - Loss: 0.0071",
    "Epoch 7/80 - Batch 200/1875 - Loss: 0.0233",
    "Epoch 7/80 - Batch 300/1875 - Loss: 0.1051",
    "Epoch 7/80 - Batch 400/1875 - Loss: 0.0118",
    "Epoch 7/80 - Batch 500/1875 - Loss: 0.0902",
    "Epoch 7/80 - Batch 600/1875 - Loss: 0.0115",
    "Epoch 7/80 - Batch 700/1875 - Loss: 0.0118",
    "Epoch 7/80 - Batch 800/1875 - Loss: 0.0170",
    "Epoch 7/80 - Batch 900/1875 - Loss: 0.0023",
    "Epoch 7/80 - Batch 1000/1875 - Loss: 0.0756",
    "Epoch 7/80 - Batch 1100/1875 - Loss: 0.0359",
    "Epoch 7/80 - Batch 1200/1875 - Loss: 0.0426",
    "Epoch 7/80 - Batch 1300/1875 - Loss: 0.0191",
    "Epoch 7/80 - Batch 1400/1875 - Loss: 0.0187",
    "Epoch 7/80 - Batch 1500/1875 - Loss: 0.0074",
    "Epoch 7/80 - Batch 1600/1875 - Loss: 0.0149",
    "Epoch 7/80 - Batch 1700/1875 - Loss: 0.0031",
    "Epoch 7/80 - Batch 1800/1875 - Loss: 0.0233",
    "Epoch 7 completed - Train Acc: 98.43% - Val Acc: 97.34%",
    "Epoch 8/80 - Batch 0/1875 - Loss: 0.0909",
    "Epoch 8/80 - Batch 100/1875 - Loss: 0.0426",
    "Epoch 8/80 - Batch 200/1875 - Loss: 0.0159",
    "Epoch 8/80 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 8/80 - Batch 400/1875 - Loss: 0.0602",
    "Epoch 8/80 - Batch 500/1875 - Loss: 0.0235",
    "Epoch 8/80 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 8/80 - Batch 700/1875 - Loss: 0.0932",
    "Epoch 8/80 - Batch 800/1875 - Loss: 0.0088",
    "Epoch 8/80 - Batch 900/1875 - Loss: 0.0645",
    "Epoch 8/80 - Batch 1000/1875 - Loss: 0.0317",
    "Epoch 8/80 - Batch 1100/1875 - Loss: 0.0964",
    "Epoch 8/80 - Batch 1200/1875 - Loss: 0.0197",
    "Epoch 8/80 - Batch 1300/1875 - Loss: 0.0347",
    "Epoch 8/80 - Batch 1400/1875 - Loss: 0.0033",
    "Epoch 8/80 - Batch 1500/1875 - Loss: 0.2207",
    "Epoch 8/80 - Batch 1600/1875 - Loss: 0.0639",
    "Epoch 8/80 - Batch 1700/1875 - Loss: 0.1650",
    "Epoch 8/80 - Batch 1800/1875 - Loss: 0.0114",
    "Epoch 8 completed - Train Acc: 98.54% - Val Acc: 97.93%",
    "Epoch 9/80 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 9/80 - Batch 100/1875 - Loss: 0.0142",
    "Epoch 9/80 - Batch 200/1875 - Loss: 0.3216",
    "Epoch 9/80 - Batch 300/1875 - Loss: 0.1784",
    "Epoch 9/80 - Batch 400/1875 - Loss: 0.0572",
    "Epoch 9/80 - Batch 500/1875 - Loss: 0.0584",
    "Epoch 9/80 - Batch 600/1875 - Loss: 0.0009",
    "Epoch 9/80 - Batch 700/1875 - Loss: 0.0025",
    "Epoch 9/80 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 9/80 - Batch 900/1875 - Loss: 0.1387",
    "Epoch 9/80 - Batch 1000/1875 - Loss: 0.0130",
    "Epoch 9/80 - Batch 1100/1875 - Loss: 0.0090",
    "Epoch 9/80 - Batch 1200/1875 - Loss: 0.0268",
    "Epoch 9/80 - Batch 1300/1875 - Loss: 0.1191",
    "Epoch 9/80 - Batch 1400/1875 - Loss: 0.0018",
    "Epoch 9/80 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 9/80 - Batch 1600/1875 - Loss: 0.0012",
    "Epoch 9/80 - Batch 1700/1875 - Loss: 0.0026",
    "Epoch 9/80 - Batch 1800/1875 - Loss: 0.0542",
    "Epoch 9 completed - Train Acc: 98.58% - Val Acc: 97.79%",
    "Epoch 10/80 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 10/80 - Batch 100/1875 - Loss: 0.0416",
    "Epoch 10/80 - Batch 200/1875 - Loss: 0.0727",
    "Epoch 10/80 - Batch 300/1875 - Loss: 0.0100",
    "Epoch 10/80 - Batch 400/1875 - Loss: 0.0055",
    "Epoch 10/80 - Batch 500/1875 - Loss: 0.0305",
    "Epoch 10/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 10/80 - Batch 700/1875 - Loss: 0.0394",
    "Epoch 10/80 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 10/80 - Batch 900/1875 - Loss: 0.0006",
    "Epoch 10/80 - Batch 1000/1875 - Loss: 0.0542",
    "Epoch 10/80 - Batch 1100/1875 - Loss: 0.0051",
    "Epoch 10/80 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 10/80 - Batch 1300/1875 - Loss: 0.0107",
    "Epoch 10/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 10/80 - Batch 1500/1875 - Loss: 0.0028",
    "Epoch 10/80 - Batch 1600/1875 - Loss: 0.0583",
    "Epoch 10/80 - Batch 1700/1875 - Loss: 0.0108",
    "Epoch 10/80 - Batch 1800/1875 - Loss: 0.1464",
    "Epoch 10 completed - Train Acc: 98.72% - Val Acc: 98.01%",
    "Epoch 11/80 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 11/80 - Batch 100/1875 - Loss: 0.0182",
    "Epoch 11/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 11/80 - Batch 300/1875 - Loss: 0.2238",
    "Epoch 11/80 - Batch 400/1875 - Loss: 0.0204",
    "Epoch 11/80 - Batch 500/1875 - Loss: 0.0018",
    "Epoch 11/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 11/80 - Batch 700/1875 - Loss: 0.0823",
    "Epoch 11/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 11/80 - Batch 900/1875 - Loss: 0.0160",
    "Epoch 11/80 - Batch 1000/1875 - Loss: 0.0206",
    "Epoch 11/80 - Batch 1100/1875 - Loss: 0.0175",
    "Epoch 11/80 - Batch 1200/1875 - Loss: 0.0109",
    "Epoch 11/80 - Batch 1300/1875 - Loss: 0.0225",
    "Epoch 11/80 - Batch 1400/1875 - Loss: 0.0027",
    "Epoch 11/80 - Batch 1500/1875 - Loss: 0.0392",
    "Epoch 11/80 - Batch 1600/1875 - Loss: 0.0067",
    "Epoch 11/80 - Batch 1700/1875 - Loss: 0.0010",
    "Epoch 11/80 - Batch 1800/1875 - Loss: 0.0031",
    "Epoch 11 completed - Train Acc: 98.83% - Val Acc: 97.51%",
    "Epoch 12/80 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 12/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 12/80 - Batch 200/1875 - Loss: 0.0036",
    "Epoch 12/80 - Batch 300/1875 - Loss: 0.0048",
    "Epoch 12/80 - Batch 400/1875 - Loss: 0.0357",
    "Epoch 12/80 - Batch 500/1875 - Loss: 0.0027",
    "Epoch 12/80 - Batch 600/1875 - Loss: 0.0040",
    "Epoch 12/80 - Batch 700/1875 - Loss: 0.0219",
    "Epoch 12/80 - Batch 800/1875 - Loss: 0.0019",
    "Epoch 12/80 - Batch 900/1875 - Loss: 0.0008",
    "Epoch 12/80 - Batch 1000/1875 - Loss: 0.0881",
    "Epoch 12/80 - Batch 1100/1875 - Loss: 0.2504",
    "Epoch 12/80 - Batch 1200/1875 - Loss: 0.0277",
    "Epoch 12/80 - Batch 1300/1875 - Loss: 0.0812",
    "Epoch 12/80 - Batch 1400/1875 - Loss: 0.0524",
    "Epoch 12/80 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 12/80 - Batch 1600/1875 - Loss: 0.0624",
    "Epoch 12/80 - Batch 1700/1875 - Loss: 0.1537",
    "Epoch 12/80 - Batch 1800/1875 - Loss: 0.1699",
    "Epoch 12 completed - Train Acc: 98.85% - Val Acc: 97.88%",
    "Epoch 13/80 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 13/80 - Batch 100/1875 - Loss: 0.0335",
    "Epoch 13/80 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 13/80 - Batch 300/1875 - Loss: 0.0063",
    "Epoch 13/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 13/80 - Batch 500/1875 - Loss: 0.1210",
    "Epoch 13/80 - Batch 600/1875 - Loss: 0.0499",
    "Epoch 13/80 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 13/80 - Batch 800/1875 - Loss: 0.0038",
    "Epoch 13/80 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 13/80 - Batch 1000/1875 - Loss: 0.0097",
    "Epoch 13/80 - Batch 1100/1875 - Loss: 0.0473",
    "Epoch 13/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 13/80 - Batch 1300/1875 - Loss: 0.0726",
    "Epoch 13/80 - Batch 1400/1875 - Loss: 0.0014",
    "Epoch 13/80 - Batch 1500/1875 - Loss: 0.0691",
    "Epoch 13/80 - Batch 1600/1875 - Loss: 0.0136",
    "Epoch 13/80 - Batch 1700/1875 - Loss: 0.0626",
    "Epoch 13/80 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 13 completed - Train Acc: 98.93% - Val Acc: 97.94%",
    "Epoch 14/80 - Batch 0/1875 - Loss: 0.0394",
    "Epoch 14/80 - Batch 100/1875 - Loss: 0.0966",
    "Epoch 14/80 - Batch 200/1875 - Loss: 0.0210",
    "Epoch 14/80 - Batch 300/1875 - Loss: 0.0139",
    "Epoch 14/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 14/80 - Batch 500/1875 - Loss: 0.0031",
    "Epoch 14/80 - Batch 600/1875 - Loss: 0.0192",
    "Epoch 14/80 - Batch 700/1875 - Loss: 0.0017",
    "Epoch 14/80 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 14/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 14/80 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 14/80 - Batch 1100/1875 - Loss: 0.1297",
    "Epoch 14/80 - Batch 1200/1875 - Loss: 0.0058",
    "Epoch 14/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 14/80 - Batch 1400/1875 - Loss: 0.0328",
    "Epoch 14/80 - Batch 1500/1875 - Loss: 0.0018",
    "Epoch 14/80 - Batch 1600/1875 - Loss: 0.0339",
    "Epoch 14/80 - Batch 1700/1875 - Loss: 0.0544",
    "Epoch 14/80 - Batch 1800/1875 - Loss: 0.0033",
    "Epoch 14 completed - Train Acc: 98.98% - Val Acc: 97.53%",
    "Epoch 15/80 - Batch 0/1875 - Loss: 0.0154",
    "Epoch 15/80 - Batch 100/1875 - Loss: 0.0272",
    "Epoch 15/80 - Batch 200/1875 - Loss: 0.0018",
    "Epoch 15/80 - Batch 300/1875 - Loss: 0.0497",
    "Epoch 15/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 15/80 - Batch 500/1875 - Loss: 0.0404",
    "Epoch 15/80 - Batch 600/1875 - Loss: 0.0858",
    "Epoch 15/80 - Batch 700/1875 - Loss: 0.0873",
    "Epoch 15/80 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 15/80 - Batch 900/1875 - Loss: 0.0098",
    "Epoch 15/80 - Batch 1000/1875 - Loss: 0.0018",
    "Epoch 15/80 - Batch 1100/1875 - Loss: 0.0151",
    "Epoch 15/80 - Batch 1200/1875 - Loss: 0.0040",
    "Epoch 15/80 - Batch 1300/1875 - Loss: 0.0117",
    "Epoch 15/80 - Batch 1400/1875 - Loss: 0.0101",
    "Epoch 15/80 - Batch 1500/1875 - Loss: 0.0079",
    "Epoch 15/80 - Batch 1600/1875 - Loss: 0.0283",
    "Epoch 15/80 - Batch 1700/1875 - Loss: 0.0332",
    "Epoch 15/80 - Batch 1800/1875 - Loss: 0.0313",
    "Epoch 15 completed - Train Acc: 99.00% - Val Acc: 97.62%",
    "Epoch 16/80 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 16/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 16/80 - Batch 200/1875 - Loss: 0.0865",
    "Epoch 16/80 - Batch 300/1875 - Loss: 0.0652",
    "Epoch 16/80 - Batch 400/1875 - Loss: 0.0006",
    "Epoch 16/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 16/80 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 16/80 - Batch 700/1875 - Loss: 0.1501",
    "Epoch 16/80 - Batch 800/1875 - Loss: 0.0580",
    "Epoch 16/80 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 16/80 - Batch 1000/1875 - Loss: 0.0345",
    "Epoch 16/80 - Batch 1100/1875 - Loss: 0.0757",
    "Epoch 16/80 - Batch 1200/1875 - Loss: 0.0123",
    "Epoch 16/80 - Batch 1300/1875 - Loss: 0.0849",
    "Epoch 16/80 - Batch 1400/1875 - Loss: 0.1694",
    "Epoch 16/80 - Batch 1500/1875 - Loss: 0.0654",
    "Epoch 16/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 16/80 - Batch 1700/1875 - Loss: 0.0094",
    "Epoch 16/80 - Batch 1800/1875 - Loss: 0.0107",
    "Epoch 16 completed - Train Acc: 99.01% - Val Acc: 97.75%",
    "Epoch 17/80 - Batch 0/1875 - Loss: 0.0258",
    "Epoch 17/80 - Batch 100/1875 - Loss: 0.0014",
    "Epoch 17/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 17/80 - Batch 300/1875 - Loss: 0.0666",
    "Epoch 17/80 - Batch 400/1875 - Loss: 0.0427",
    "Epoch 17/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 17/80 - Batch 600/1875 - Loss: 0.0040",
    "Epoch 17/80 - Batch 700/1875 - Loss: 0.0033",
    "Epoch 17/80 - Batch 800/1875 - Loss: 0.0194",
    "Epoch 17/80 - Batch 900/1875 - Loss: 0.1012",
    "Epoch 17/80 - Batch 1000/1875 - Loss: 0.0052",
    "Epoch 17/80 - Batch 1100/1875 - Loss: 0.0032",
    "Epoch 17/80 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 17/80 - Batch 1300/1875 - Loss: 0.0020",
    "Epoch 17/80 - Batch 1400/1875 - Loss: 0.0048",
    "Epoch 17/80 - Batch 1500/1875 - Loss: 0.0065",
    "Epoch 17/80 - Batch 1600/1875 - Loss: 0.0038",
    "Epoch 17/80 - Batch 1700/1875 - Loss: 0.2010",
    "Epoch 17/80 - Batch 1800/1875 - Loss: 0.0206",
    "Epoch 17 completed - Train Acc: 99.14% - Val Acc: 97.75%",
    "Epoch 18/80 - Batch 0/1875 - Loss: 0.0069",
    "Epoch 18/80 - Batch 100/1875 - Loss: 0.0107",
    "Epoch 18/80 - Batch 200/1875 - Loss: 0.1393",
    "Epoch 18/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 18/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 18/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 700/1875 - Loss: 0.0375",
    "Epoch 18/80 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 18/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 18/80 - Batch 1000/1875 - Loss: 0.0024",
    "Epoch 18/80 - Batch 1100/1875 - Loss: 0.0286",
    "Epoch 18/80 - Batch 1200/1875 - Loss: 0.1451",
    "Epoch 18/80 - Batch 1300/1875 - Loss: 0.1475",
    "Epoch 18/80 - Batch 1400/1875 - Loss: 0.0043",
    "Epoch 18/80 - Batch 1500/1875 - Loss: 0.0344",
    "Epoch 18/80 - Batch 1600/1875 - Loss: 0.0175",
    "Epoch 18/80 - Batch 1700/1875 - Loss: 0.0037",
    "Epoch 18/80 - Batch 1800/1875 - Loss: 0.0085",
    "Epoch 18 completed - Train Acc: 99.15% - Val Acc: 97.92%",
    "Epoch 19/80 - Batch 0/1875 - Loss: 0.0611",
    "Epoch 19/80 - Batch 100/1875 - Loss: 0.0023",
    "Epoch 19/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 19/80 - Batch 300/1875 - Loss: 0.0080",
    "Epoch 19/80 - Batch 400/1875 - Loss: 0.0881",
    "Epoch 19/80 - Batch 500/1875 - Loss: 0.3269",
    "Epoch 19/80 - Batch 600/1875 - Loss: 0.0015",
    "Epoch 19/80 - Batch 700/1875 - Loss: 0.0091",
    "Epoch 19/80 - Batch 800/1875 - Loss: 0.1273",
    "Epoch 19/80 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 19/80 - Batch 1000/1875 - Loss: 0.0524",
    "Epoch 19/80 - Batch 1100/1875 - Loss: 0.0010",
    "Epoch 19/80 - Batch 1200/1875 - Loss: 0.0256",
    "Epoch 19/80 - Batch 1300/1875 - Loss: 0.0073",
    "Epoch 19/80 - Batch 1400/1875 - Loss: 0.0018",
    "Epoch 19/80 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 19/80 - Batch 1600/1875 - Loss: 0.0023",
    "Epoch 19/80 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 19/80 - Batch 1800/1875 - Loss: 0.0039",
    "Epoch 19 completed - Train Acc: 99.19% - Val Acc: 97.81%",
    "Epoch 20/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 20/80 - Batch 100/1875 - Loss: 0.0123",
    "Epoch 20/80 - Batch 200/1875 - Loss: 0.0010",
    "Epoch 20/80 - Batch 300/1875 - Loss: 0.0044",
    "Epoch 20/80 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 20/80 - Batch 500/1875 - Loss: 0.0080",
    "Epoch 20/80 - Batch 600/1875 - Loss: 0.0341",
    "Epoch 20/80 - Batch 700/1875 - Loss: 0.0347",
    "Epoch 20/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 20/80 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 20/80 - Batch 1000/1875 - Loss: 0.1083",
    "Epoch 20/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 20/80 - Batch 1200/1875 - Loss: 0.0004",
    "Epoch 20/80 - Batch 1300/1875 - Loss: 0.0510",
    "Epoch 20/80 - Batch 1400/1875 - Loss: 0.0772",
    "Epoch 20/80 - Batch 1500/1875 - Loss: 0.0586",
    "Epoch 20/80 - Batch 1600/1875 - Loss: 0.2095",
    "Epoch 20/80 - Batch 1700/1875 - Loss: 0.0021",
    "Epoch 20/80 - Batch 1800/1875 - Loss: 0.0066",
    "Epoch 20 completed - Train Acc: 99.13% - Val Acc: 97.95%",
    "Epoch 21/80 - Batch 0/1875 - Loss: 0.0167",
    "Epoch 21/80 - Batch 100/1875 - Loss: 0.0473",
    "Epoch 21/80 - Batch 200/1875 - Loss: 0.0109",
    "Epoch 21/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 400/1875 - Loss: 0.0062",
    "Epoch 21/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 21/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 700/1875 - Loss: 0.0625",
    "Epoch 21/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 900/1875 - Loss: 0.0170",
    "Epoch 21/80 - Batch 1000/1875 - Loss: 0.0175",
    "Epoch 21/80 - Batch 1100/1875 - Loss: 0.0325",
    "Epoch 21/80 - Batch 1200/1875 - Loss: 0.0072",
    "Epoch 21/80 - Batch 1300/1875 - Loss: 0.0062",
    "Epoch 21/80 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 21/80 - Batch 1500/1875 - Loss: 0.0009",
    "Epoch 21/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 21/80 - Batch 1700/1875 - Loss: 0.2472",
    "Epoch 21/80 - Batch 1800/1875 - Loss: 0.0099",
    "Epoch 21 completed - Train Acc: 99.20% - Val Acc: 97.81%",
    "Epoch 22/80 - Batch 0/1875 - Loss: 0.1175",
    "Epoch 22/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 22/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 22/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 22/80 - Batch 400/1875 - Loss: 0.7371",
    "Epoch 22/80 - Batch 500/1875 - Loss: 0.0348",
    "Epoch 22/80 - Batch 600/1875 - Loss: 0.0745",
    "Epoch 22/80 - Batch 700/1875 - Loss: 0.0017",
    "Epoch 22/80 - Batch 800/1875 - Loss: 0.0281",
    "Epoch 22/80 - Batch 900/1875 - Loss: 0.1075",
    "Epoch 22/80 - Batch 1000/1875 - Loss: 0.1364",
    "Epoch 22/80 - Batch 1100/1875 - Loss: 0.0697",
    "Epoch 22/80 - Batch 1200/1875 - Loss: 0.0070",
    "Epoch 22/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 22/80 - Batch 1400/1875 - Loss: 0.0030",
    "Epoch 22/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 22/80 - Batch 1600/1875 - Loss: 0.0426",
    "Epoch 22/80 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 22/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 22 completed - Train Acc: 99.27% - Val Acc: 97.95%",
    "Epoch 23/80 - Batch 0/1875 - Loss: 0.0128",
    "Epoch 23/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 23/80 - Batch 200/1875 - Loss: 0.2255",
    "Epoch 23/80 - Batch 300/1875 - Loss: 0.0013",
    "Epoch 23/80 - Batch 400/1875 - Loss: 0.0032",
    "Epoch 23/80 - Batch 500/1875 - Loss: 0.0010",
    "Epoch 23/80 - Batch 600/1875 - Loss: 0.0653",
    "Epoch 23/80 - Batch 700/1875 - Loss: 0.0042",
    "Epoch 23/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 23/80 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 23/80 - Batch 1000/1875 - Loss: 0.0019",
    "Epoch 23/80 - Batch 1100/1875 - Loss: 0.0007",
    "Epoch 23/80 - Batch 1200/1875 - Loss: 0.2260",
    "Epoch 23/80 - Batch 1300/1875 - Loss: 0.0063",
    "Epoch 23/80 - Batch 1400/1875 - Loss: 0.0046",
    "Epoch 23/80 - Batch 1500/1875 - Loss: 0.0298",
    "Epoch 23/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 23/80 - Batch 1700/1875 - Loss: 0.0328",
    "Epoch 23/80 - Batch 1800/1875 - Loss: 0.0029",
    "Epoch 23 completed - Train Acc: 99.28% - Val Acc: 98.04%",
    "Epoch 24/80 - Batch 0/1875 - Loss: 0.0075",
    "Epoch 24/80 - Batch 100/1875 - Loss: 0.0726",
    "Epoch 24/80 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 24/80 - Batch 300/1875 - Loss: 0.0046",
    "Epoch 24/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 24/80 - Batch 500/1875 - Loss: 0.0046",
    "Epoch 24/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 24/80 - Batch 700/1875 - Loss: 0.0031",
    "Epoch 24/80 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 24/80 - Batch 900/1875 - Loss: 0.0319",
    "Epoch 24/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 24/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 24/80 - Batch 1500/1875 - Loss: 0.0054",
    "Epoch 24/80 - Batch 1600/1875 - Loss: 0.0014",
    "Epoch 24/80 - Batch 1700/1875 - Loss: 0.0020",
    "Epoch 24/80 - Batch 1800/1875 - Loss: 0.0973",
    "Epoch 24 completed - Train Acc: 99.32% - Val Acc: 97.98%",
    "Epoch 25/80 - Batch 0/1875 - Loss: 0.0111",
    "Epoch 25/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 25/80 - Batch 200/1875 - Loss: 0.0056",
    "Epoch 25/80 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 25/80 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 25/80 - Batch 500/1875 - Loss: 0.0981",
    "Epoch 25/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 25/80 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 25/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 25/80 - Batch 900/1875 - Loss: 0.2658",
    "Epoch 25/80 - Batch 1000/1875 - Loss: 0.0087",
    "Epoch 25/80 - Batch 1100/1875 - Loss: 0.0717",
    "Epoch 25/80 - Batch 1200/1875 - Loss: 0.0725",
    "Epoch 25/80 - Batch 1300/1875 - Loss: 0.0075",
    "Epoch 25/80 - Batch 1400/1875 - Loss: 0.2777",
    "Epoch 25/80 - Batch 1500/1875 - Loss: 0.0047",
    "Epoch 25/80 - Batch 1600/1875 - Loss: 0.0195",
    "Epoch 25/80 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 25/80 - Batch 1800/1875 - Loss: 0.1653",
    "Epoch 25 completed - Train Acc: 99.21% - Val Acc: 97.93%",
    "Epoch 26/80 - Batch 0/1875 - Loss: 0.0044",
    "Epoch 26/80 - Batch 100/1875 - Loss: 0.0333",
    "Epoch 26/80 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 26/80 - Batch 300/1875 - Loss: 0.0012",
    "Epoch 26/80 - Batch 400/1875 - Loss: 0.0034",
    "Epoch 26/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 26/80 - Batch 600/1875 - Loss: 0.0023",
    "Epoch 26/80 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 26/80 - Batch 800/1875 - Loss: 0.0073",
    "Epoch 26/80 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 26/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 26/80 - Batch 1100/1875 - Loss: 0.0082",
    "Epoch 26/80 - Batch 1200/1875 - Loss: 0.1690",
    "Epoch 26/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 26/80 - Batch 1400/1875 - Loss: 0.0208",
    "Epoch 26/80 - Batch 1500/1875 - Loss: 0.0626",
    "Epoch 26/80 - Batch 1600/1875 - Loss: 0.0108",
    "Epoch 26/80 - Batch 1700/1875 - Loss: 0.0029",
    "Epoch 26/80 - Batch 1800/1875 - Loss: 0.0145",
    "Epoch 26 completed - Train Acc: 99.34% - Val Acc: 98.06%",
    "Epoch 27/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 27/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 27/80 - Batch 200/1875 - Loss: 0.1156",
    "Epoch 27/80 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 27/80 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 27/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 27/80 - Batch 600/1875 - Loss: 0.0092",
    "Epoch 27/80 - Batch 700/1875 - Loss: 0.1391",
    "Epoch 27/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 27/80 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 27/80 - Batch 1000/1875 - Loss: 0.1329",
    "Epoch 27/80 - Batch 1100/1875 - Loss: 0.0014",
    "Epoch 27/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 27/80 - Batch 1300/1875 - Loss: 0.2061",
    "Epoch 27/80 - Batch 1400/1875 - Loss: 0.0325",
    "Epoch 27/80 - Batch 1500/1875 - Loss: 0.0604",
    "Epoch 27/80 - Batch 1600/1875 - Loss: 0.0036",
    "Epoch 27/80 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 27/80 - Batch 1800/1875 - Loss: 0.0013",
    "Epoch 27 completed - Train Acc: 99.25% - Val Acc: 97.76%",
    "Epoch 28/80 - Batch 0/1875 - Loss: 0.0085",
    "Epoch 28/80 - Batch 100/1875 - Loss: 0.0012",
    "Epoch 28/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 28/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 28/80 - Batch 400/1875 - Loss: 0.0091",
    "Epoch 28/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 28/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 28/80 - Batch 700/1875 - Loss: 0.0024",
    "Epoch 28/80 - Batch 800/1875 - Loss: 0.0117",
    "Epoch 28/80 - Batch 900/1875 - Loss: 0.0042",
    "Epoch 28/80 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 28/80 - Batch 1100/1875 - Loss: 0.0049",
    "Epoch 28/80 - Batch 1200/1875 - Loss: 0.0077",
    "Epoch 28/80 - Batch 1300/1875 - Loss: 0.2572",
    "Epoch 28/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 28/80 - Batch 1500/1875 - Loss: 0.0108",
    "Epoch 28/80 - Batch 1600/1875 - Loss: 0.0020",
    "Epoch 28/80 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 28/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 28 completed - Train Acc: 99.42% - Val Acc: 97.96%",
    "Epoch 29/80 - Batch 0/1875 - Loss: 0.0216",
    "Epoch 29/80 - Batch 100/1875 - Loss: 0.0268",
    "Epoch 29/80 - Batch 200/1875 - Loss: 0.0880",
    "Epoch 29/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 29/80 - Batch 500/1875 - Loss: 0.0054",
    "Epoch 29/80 - Batch 600/1875 - Loss: 0.0147",
    "Epoch 29/80 - Batch 700/1875 - Loss: 0.0010",
    "Epoch 29/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 29/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 29/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 1100/1875 - Loss: 0.0184",
    "Epoch 29/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 29/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 1500/1875 - Loss: 0.0067",
    "Epoch 29/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 29/80 - Batch 1700/1875 - Loss: 0.0036",
    "Epoch 29/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 29 completed - Train Acc: 99.31% - Val Acc: 97.89%",
    "Epoch 30/80 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 30/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 30/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 30/80 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 30/80 - Batch 400/1875 - Loss: 0.4862",
    "Epoch 30/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 30/80 - Batch 600/1875 - Loss: 0.2125",
    "Epoch 30/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 30/80 - Batch 800/1875 - Loss: 0.0191",
    "Epoch 30/80 - Batch 900/1875 - Loss: 0.0357",
    "Epoch 30/80 - Batch 1000/1875 - Loss: 0.0071",
    "Epoch 30/80 - Batch 1100/1875 - Loss: 0.0564",
    "Epoch 30/80 - Batch 1200/1875 - Loss: 0.0070",
    "Epoch 30/80 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 30/80 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 30/80 - Batch 1500/1875 - Loss: 0.1402",
    "Epoch 30/80 - Batch 1600/1875 - Loss: 0.0129",
    "Epoch 30/80 - Batch 1700/1875 - Loss: 0.2148",
    "Epoch 30/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 30 completed - Train Acc: 99.31% - Val Acc: 98.14%",
    "Epoch 31/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 31/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 31/80 - Batch 200/1875 - Loss: 0.1695",
    "Epoch 31/80 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 31/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 31/80 - Batch 500/1875 - Loss: 0.0033",
    "Epoch 31/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 31/80 - Batch 700/1875 - Loss: 0.0168",
    "Epoch 31/80 - Batch 800/1875 - Loss: 0.0129",
    "Epoch 31/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 31/80 - Batch 1000/1875 - Loss: 0.0018",
    "Epoch 31/80 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 31/80 - Batch 1200/1875 - Loss: 0.0846",
    "Epoch 31/80 - Batch 1300/1875 - Loss: 0.0036",
    "Epoch 31/80 - Batch 1400/1875 - Loss: 0.1598",
    "Epoch 31/80 - Batch 1500/1875 - Loss: 0.0028",
    "Epoch 31/80 - Batch 1600/1875 - Loss: 0.0102",
    "Epoch 31/80 - Batch 1700/1875 - Loss: 0.0051",
    "Epoch 31/80 - Batch 1800/1875 - Loss: 0.0085",
    "Epoch 31 completed - Train Acc: 99.28% - Val Acc: 97.59%",
    "Epoch 32/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 100/1875 - Loss: 0.0188",
    "Epoch 32/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 32/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 32/80 - Batch 500/1875 - Loss: 0.0125",
    "Epoch 32/80 - Batch 600/1875 - Loss: 0.1675",
    "Epoch 32/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 32/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 1000/1875 - Loss: 0.0030",
    "Epoch 32/80 - Batch 1100/1875 - Loss: 0.0070",
    "Epoch 32/80 - Batch 1200/1875 - Loss: 0.0085",
    "Epoch 32/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 1400/1875 - Loss: 0.0670",
    "Epoch 32/80 - Batch 1500/1875 - Loss: 0.0073",
    "Epoch 32/80 - Batch 1600/1875 - Loss: 0.0098",
    "Epoch 32/80 - Batch 1700/1875 - Loss: 0.1593",
    "Epoch 32/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 32 completed - Train Acc: 99.34% - Val Acc: 97.84%",
    "Epoch 33/80 - Batch 0/1875 - Loss: 0.0162",
    "Epoch 33/80 - Batch 100/1875 - Loss: 0.0827",
    "Epoch 33/80 - Batch 200/1875 - Loss: 0.0449",
    "Epoch 33/80 - Batch 300/1875 - Loss: 0.1753",
    "Epoch 33/80 - Batch 400/1875 - Loss: 0.0006",
    "Epoch 33/80 - Batch 500/1875 - Loss: 0.0008",
    "Epoch 33/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 33/80 - Batch 700/1875 - Loss: 0.0102",
    "Epoch 33/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 33/80 - Batch 900/1875 - Loss: 0.0081",
    "Epoch 33/80 - Batch 1000/1875 - Loss: 0.0007",
    "Epoch 33/80 - Batch 1100/1875 - Loss: 0.0028",
    "Epoch 33/80 - Batch 1200/1875 - Loss: 0.1156",
    "Epoch 33/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 33/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 33/80 - Batch 1500/1875 - Loss: 0.0062",
    "Epoch 33/80 - Batch 1600/1875 - Loss: 0.0020",
    "Epoch 33/80 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 33/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 33 completed - Train Acc: 99.47% - Val Acc: 97.88%",
    "Epoch 34/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 200/1875 - Loss: 0.0269",
    "Epoch 34/80 - Batch 300/1875 - Loss: 0.0285",
    "Epoch 34/80 - Batch 400/1875 - Loss: 0.2165",
    "Epoch 34/80 - Batch 500/1875 - Loss: 0.1690",
    "Epoch 34/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 700/1875 - Loss: 0.0067",
    "Epoch 34/80 - Batch 800/1875 - Loss: 0.0115",
    "Epoch 34/80 - Batch 900/1875 - Loss: 0.0104",
    "Epoch 34/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 1200/1875 - Loss: 0.5131",
    "Epoch 34/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 1500/1875 - Loss: 0.0340",
    "Epoch 34/80 - Batch 1600/1875 - Loss: 0.0634",
    "Epoch 34/80 - Batch 1700/1875 - Loss: 0.0113",
    "Epoch 34/80 - Batch 1800/1875 - Loss: 0.0130",
    "Epoch 34 completed - Train Acc: 99.33% - Val Acc: 97.77%",
    "Epoch 35/80 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 35/80 - Batch 100/1875 - Loss: 0.0122",
    "Epoch 35/80 - Batch 200/1875 - Loss: 0.0017",
    "Epoch 35/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 35/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 35/80 - Batch 500/1875 - Loss: 0.0006",
    "Epoch 35/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 35/80 - Batch 700/1875 - Loss: 0.0025",
    "Epoch 35/80 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 35/80 - Batch 900/1875 - Loss: 1.0159",
    "Epoch 35/80 - Batch 1000/1875 - Loss: 0.0009",
    "Epoch 35/80 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 35/80 - Batch 1200/1875 - Loss: 0.0042",
    "Epoch 35/80 - Batch 1300/1875 - Loss: 0.0208",
    "Epoch 35/80 - Batch 1400/1875 - Loss: 0.0031",
    "Epoch 35/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 35/80 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 35/80 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 35/80 - Batch 1800/1875 - Loss: 0.0236",
    "Epoch 35 completed - Train Acc: 99.48% - Val Acc: 97.83%",
    "Epoch 36/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 36/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 36/80 - Batch 200/1875 - Loss: 0.0026",
    "Epoch 36/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 36/80 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 36/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 700/1875 - Loss: 0.0048",
    "Epoch 36/80 - Batch 800/1875 - Loss: 0.0054",
    "Epoch 36/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 36/80 - Batch 1000/1875 - Loss: 0.0007",
    "Epoch 36/80 - Batch 1100/1875 - Loss: 0.0028",
    "Epoch 36/80 - Batch 1200/1875 - Loss: 0.0024",
    "Epoch 36/80 - Batch 1300/1875 - Loss: 0.0577",
    "Epoch 36/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 1600/1875 - Loss: 0.0238",
    "Epoch 36/80 - Batch 1700/1875 - Loss: 0.0168",
    "Epoch 36/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 36 completed - Train Acc: 99.43% - Val Acc: 97.90%",
    "Epoch 37/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 37/80 - Batch 100/1875 - Loss: 0.0017",
    "Epoch 37/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 37/80 - Batch 300/1875 - Loss: 0.0140",
    "Epoch 37/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 37/80 - Batch 500/1875 - Loss: 0.0912",
    "Epoch 37/80 - Batch 600/1875 - Loss: 0.0032",
    "Epoch 37/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 37/80 - Batch 800/1875 - Loss: 0.1512",
    "Epoch 37/80 - Batch 900/1875 - Loss: 0.0017",
    "Epoch 37/80 - Batch 1000/1875 - Loss: 0.0041",
    "Epoch 37/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 37/80 - Batch 1200/1875 - Loss: 0.0009",
    "Epoch 37/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 37/80 - Batch 1400/1875 - Loss: 0.0166",
    "Epoch 37/80 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 37/80 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 37/80 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 37/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 37 completed - Train Acc: 99.45% - Val Acc: 97.63%",
    "Epoch 38/80 - Batch 0/1875 - Loss: 0.0297",
    "Epoch 38/80 - Batch 100/1875 - Loss: 0.0015",
    "Epoch 38/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 38/80 - Batch 300/1875 - Loss: 0.0304",
    "Epoch 38/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 38/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 38/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 38/80 - Batch 700/1875 - Loss: 0.0844",
    "Epoch 38/80 - Batch 800/1875 - Loss: 0.0225",
    "Epoch 38/80 - Batch 900/1875 - Loss: 0.0006",
    "Epoch 38/80 - Batch 1000/1875 - Loss: 0.0005",
    "Epoch 38/80 - Batch 1100/1875 - Loss: 0.0205",
    "Epoch 38/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 38/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 38/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 38/80 - Batch 1500/1875 - Loss: 0.0065",
    "Epoch 38/80 - Batch 1600/1875 - Loss: 0.1898",
    "Epoch 38/80 - Batch 1700/1875 - Loss: 0.1234",
    "Epoch 38/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 38 completed - Train Acc: 99.44% - Val Acc: 97.91%",
    "Epoch 39/80 - Batch 0/1875 - Loss: 0.0389",
    "Epoch 39/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 300/1875 - Loss: 0.0087",
    "Epoch 39/80 - Batch 400/1875 - Loss: 0.0047",
    "Epoch 39/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 39/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 39/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 900/1875 - Loss: 0.0016",
    "Epoch 39/80 - Batch 1000/1875 - Loss: 0.0031",
    "Epoch 39/80 - Batch 1100/1875 - Loss: 0.0010",
    "Epoch 39/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 1400/1875 - Loss: 0.0027",
    "Epoch 39/80 - Batch 1500/1875 - Loss: 0.0153",
    "Epoch 39/80 - Batch 1600/1875 - Loss: 0.0461",
    "Epoch 39/80 - Batch 1700/1875 - Loss: 0.1073",
    "Epoch 39/80 - Batch 1800/1875 - Loss: 0.0033",
    "Epoch 39 completed - Train Acc: 99.33% - Val Acc: 97.64%",
    "Epoch 40/80 - Batch 0/1875 - Loss: 0.1595",
    "Epoch 40/80 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 40/80 - Batch 200/1875 - Loss: 0.2754",
    "Epoch 40/80 - Batch 300/1875 - Loss: 0.0058",
    "Epoch 40/80 - Batch 400/1875 - Loss: 0.0494",
    "Epoch 40/80 - Batch 500/1875 - Loss: 0.0245",
    "Epoch 40/80 - Batch 600/1875 - Loss: 0.0044",
    "Epoch 40/80 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 40/80 - Batch 800/1875 - Loss: 0.1271",
    "Epoch 40/80 - Batch 900/1875 - Loss: 0.3296",
    "Epoch 40/80 - Batch 1000/1875 - Loss: 0.0472",
    "Epoch 40/80 - Batch 1100/1875 - Loss: 0.0043",
    "Epoch 40/80 - Batch 1200/1875 - Loss: 0.0009",
    "Epoch 40/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 40/80 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 40/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 40/80 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 40/80 - Batch 1700/1875 - Loss: 0.1942",
    "Epoch 40/80 - Batch 1800/1875 - Loss: 0.2994",
    "Epoch 40 completed - Train Acc: 99.53% - Val Acc: 98.03%",
    "Epoch 41/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 41/80 - Batch 100/1875 - Loss: 0.0053",
    "Epoch 41/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 41/80 - Batch 300/1875 - Loss: 0.3241",
    "Epoch 41/80 - Batch 400/1875 - Loss: 0.1058",
    "Epoch 41/80 - Batch 500/1875 - Loss: 0.0048",
    "Epoch 41/80 - Batch 600/1875 - Loss: 0.0049",
    "Epoch 41/80 - Batch 700/1875 - Loss: 0.0016",
    "Epoch 41/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 41/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 41/80 - Batch 1000/1875 - Loss: 0.0009",
    "Epoch 41/80 - Batch 1100/1875 - Loss: 0.0078",
    "Epoch 41/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 41/80 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 41/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 41/80 - Batch 1500/1875 - Loss: 0.0045",
    "Epoch 41/80 - Batch 1600/1875 - Loss: 0.5268",
    "Epoch 41/80 - Batch 1700/1875 - Loss: 0.1025",
    "Epoch 41/80 - Batch 1800/1875 - Loss: 0.0032",
    "Epoch 41 completed - Train Acc: 99.39% - Val Acc: 97.82%",
    "Epoch 42/80 - Batch 0/1875 - Loss: 0.0036",
    "Epoch 42/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 42/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 42/80 - Batch 500/1875 - Loss: 0.0082",
    "Epoch 42/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 42/80 - Batch 700/1875 - Loss: 0.0143",
    "Epoch 42/80 - Batch 800/1875 - Loss: 0.9471",
    "Epoch 42/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 42/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 42/80 - Batch 1100/1875 - Loss: 0.0173",
    "Epoch 42/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 42/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 42/80 - Batch 1600/1875 - Loss: 0.0015",
    "Epoch 42/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 42/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 42 completed - Train Acc: 99.47% - Val Acc: 97.69%",
    "Epoch 43/80 - Batch 0/1875 - Loss: 0.0011",
    "Epoch 43/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 43/80 - Batch 200/1875 - Loss: 0.0125",
    "Epoch 43/80 - Batch 300/1875 - Loss: 0.0050",
    "Epoch 43/80 - Batch 400/1875 - Loss: 0.0016",
    "Epoch 43/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 43/80 - Batch 600/1875 - Loss: 0.1565",
    "Epoch 43/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 43/80 - Batch 800/1875 - Loss: 0.1545",
    "Epoch 43/80 - Batch 900/1875 - Loss: 0.0714",
    "Epoch 43/80 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 43/80 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 43/80 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 43/80 - Batch 1300/1875 - Loss: 0.0900",
    "Epoch 43/80 - Batch 1400/1875 - Loss: 0.0262",
    "Epoch 43/80 - Batch 1500/1875 - Loss: 0.0163",
    "Epoch 43/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 43/80 - Batch 1700/1875 - Loss: 0.0039",
    "Epoch 43/80 - Batch 1800/1875 - Loss: 0.0964",
    "Epoch 43 completed - Train Acc: 99.43% - Val Acc: 98.15%",
    "Epoch 44/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 44/80 - Batch 200/1875 - Loss: 0.0367",
    "Epoch 44/80 - Batch 300/1875 - Loss: 0.1038",
    "Epoch 44/80 - Batch 400/1875 - Loss: 0.0061",
    "Epoch 44/80 - Batch 500/1875 - Loss: 0.0033",
    "Epoch 44/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 44/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 44/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 44/80 - Batch 1200/1875 - Loss: 0.2268",
    "Epoch 44/80 - Batch 1300/1875 - Loss: 0.0155",
    "Epoch 44/80 - Batch 1400/1875 - Loss: 0.0028",
    "Epoch 44/80 - Batch 1500/1875 - Loss: 0.0162",
    "Epoch 44/80 - Batch 1600/1875 - Loss: 0.0069",
    "Epoch 44/80 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 44/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 44 completed - Train Acc: 99.48% - Val Acc: 97.90%",
    "Epoch 45/80 - Batch 0/1875 - Loss: 0.0186",
    "Epoch 45/80 - Batch 100/1875 - Loss: 0.0073",
    "Epoch 45/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 45/80 - Batch 300/1875 - Loss: 0.0250",
    "Epoch 45/80 - Batch 400/1875 - Loss: 0.1090",
    "Epoch 45/80 - Batch 500/1875 - Loss: 0.0085",
    "Epoch 45/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 45/80 - Batch 900/1875 - Loss: 0.1325",
    "Epoch 45/80 - Batch 1000/1875 - Loss: 0.0517",
    "Epoch 45/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 1200/1875 - Loss: 0.0015",
    "Epoch 45/80 - Batch 1300/1875 - Loss: 0.0008",
    "Epoch 45/80 - Batch 1400/1875 - Loss: 0.0009",
    "Epoch 45/80 - Batch 1500/1875 - Loss: 0.0008",
    "Epoch 45/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 1700/1875 - Loss: 0.0282",
    "Epoch 45/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 45 completed - Train Acc: 99.42% - Val Acc: 98.04%",
    "Epoch 46/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 100/1875 - Loss: 0.0967",
    "Epoch 46/80 - Batch 200/1875 - Loss: 0.0019",
    "Epoch 46/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 46/80 - Batch 600/1875 - Loss: 0.0022",
    "Epoch 46/80 - Batch 700/1875 - Loss: 0.0137",
    "Epoch 46/80 - Batch 800/1875 - Loss: 0.0118",
    "Epoch 46/80 - Batch 900/1875 - Loss: 0.0012",
    "Epoch 46/80 - Batch 1000/1875 - Loss: 0.0017",
    "Epoch 46/80 - Batch 1100/1875 - Loss: 0.0496",
    "Epoch 46/80 - Batch 1200/1875 - Loss: 0.0024",
    "Epoch 46/80 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 46/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 46/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 46/80 - Batch 1600/1875 - Loss: 0.0432",
    "Epoch 46/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 1800/1875 - Loss: 0.0068",
    "Epoch 46 completed - Train Acc: 99.53% - Val Acc: 97.61%",
    "Epoch 47/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 47/80 - Batch 100/1875 - Loss: 0.0021",
    "Epoch 47/80 - Batch 200/1875 - Loss: 0.0329",
    "Epoch 47/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 47/80 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 47/80 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 47/80 - Batch 600/1875 - Loss: 0.0109",
    "Epoch 47/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 900/1875 - Loss: 0.0090",
    "Epoch 47/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 47/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 47/80 - Batch 1200/1875 - Loss: 0.0024",
    "Epoch 47/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 1400/1875 - Loss: 0.0099",
    "Epoch 47/80 - Batch 1500/1875 - Loss: 0.1013",
    "Epoch 47/80 - Batch 1600/1875 - Loss: 0.0850",
    "Epoch 47/80 - Batch 1700/1875 - Loss: 0.3276",
    "Epoch 47/80 - Batch 1800/1875 - Loss: 0.0052",
    "Epoch 47 completed - Train Acc: 99.51% - Val Acc: 97.16%",
    "Epoch 48/80 - Batch 0/1875 - Loss: 0.0823",
    "Epoch 48/80 - Batch 100/1875 - Loss: 0.0077",
    "Epoch 48/80 - Batch 200/1875 - Loss: 0.5152",
    "Epoch 48/80 - Batch 300/1875 - Loss: 0.2314",
    "Epoch 48/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 48/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 48/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 800/1875 - Loss: 0.0122",
    "Epoch 48/80 - Batch 900/1875 - Loss: 0.0008",
    "Epoch 48/80 - Batch 1000/1875 - Loss: 0.2004",
    "Epoch 48/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 48/80 - Batch 1200/1875 - Loss: 0.0008",
    "Epoch 48/80 - Batch 1300/1875 - Loss: 0.0138",
    "Epoch 48/80 - Batch 1400/1875 - Loss: 0.0011",
    "Epoch 48/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 48/80 - Batch 1700/1875 - Loss: 0.0112",
    "Epoch 48/80 - Batch 1800/1875 - Loss: 0.0010",
    "Epoch 48 completed - Train Acc: 99.42% - Val Acc: 98.11%",
    "Epoch 49/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 49/80 - Batch 100/1875 - Loss: 0.0375",
    "Epoch 49/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 49/80 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 49/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 49/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 49/80 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 49/80 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 49/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 49/80 - Batch 900/1875 - Loss: 0.0140",
    "Epoch 49/80 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 49/80 - Batch 1100/1875 - Loss: 0.0090",
    "Epoch 49/80 - Batch 1200/1875 - Loss: 0.0021",
    "Epoch 49/80 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 49/80 - Batch 1400/1875 - Loss: 0.0313",
    "Epoch 49/80 - Batch 1500/1875 - Loss: 0.0054",
    "Epoch 49/80 - Batch 1600/1875 - Loss: 0.0166",
    "Epoch 49/80 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 49/80 - Batch 1800/1875 - Loss: 0.0469",
    "Epoch 49 completed - Train Acc: 99.47% - Val Acc: 97.94%",
    "Epoch 50/80 - Batch 0/1875 - Loss: 0.1083",
    "Epoch 50/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 200/1875 - Loss: 0.0020",
    "Epoch 50/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 400/1875 - Loss: 0.0016",
    "Epoch 50/80 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 50/80 - Batch 600/1875 - Loss: 0.0066",
    "Epoch 50/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 800/1875 - Loss: 0.0819",
    "Epoch 50/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1200/1875 - Loss: 0.0093",
    "Epoch 50/80 - Batch 1300/1875 - Loss: 0.0031",
    "Epoch 50/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1500/1875 - Loss: 0.0065",
    "Epoch 50/80 - Batch 1600/1875 - Loss: 0.0016",
    "Epoch 50/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 50 completed - Train Acc: 99.48% - Val Acc: 97.63%",
    "Epoch 51/80 - Batch 0/1875 - Loss: 0.0033",
    "Epoch 51/80 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 51/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 300/1875 - Loss: 0.0102",
    "Epoch 51/80 - Batch 400/1875 - Loss: 0.1014",
    "Epoch 51/80 - Batch 500/1875 - Loss: 0.4772",
    "Epoch 51/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 700/1875 - Loss: 0.0084",
    "Epoch 51/80 - Batch 800/1875 - Loss: 0.0022",
    "Epoch 51/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 1000/1875 - Loss: 0.0021",
    "Epoch 51/80 - Batch 1100/1875 - Loss: 0.0948",
    "Epoch 51/80 - Batch 1200/1875 - Loss: 0.0322",
    "Epoch 51/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 1500/1875 - Loss: 0.0116",
    "Epoch 51/80 - Batch 1600/1875 - Loss: 0.0018",
    "Epoch 51/80 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 51/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 51 completed - Train Acc: 99.52% - Val Acc: 97.91%",
    "Epoch 52/80 - Batch 0/1875 - Loss: 0.0032",
    "Epoch 52/80 - Batch 100/1875 - Loss: 0.0148",
    "Epoch 52/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 52/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 52/80 - Batch 500/1875 - Loss: 0.0004",
    "Epoch 52/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 52/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 1000/1875 - Loss: 0.0135",
    "Epoch 52/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 52/80 - Batch 1200/1875 - Loss: 0.0163",
    "Epoch 52/80 - Batch 1300/1875 - Loss: 0.0911",
    "Epoch 52/80 - Batch 1400/1875 - Loss: 0.0030",
    "Epoch 52/80 - Batch 1500/1875 - Loss: 0.0090",
    "Epoch 52/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 1700/1875 - Loss: 0.1997",
    "Epoch 52/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 52 completed - Train Acc: 99.52% - Val Acc: 98.05%",
    "Epoch 53/80 - Batch 0/1875 - Loss: 0.0285",
    "Epoch 53/80 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 53/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 53/80 - Batch 500/1875 - Loss: 0.0036",
    "Epoch 53/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 53/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 53/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 53/80 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 53/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 53/80 - Batch 1300/1875 - Loss: 0.0638",
    "Epoch 53/80 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 53/80 - Batch 1500/1875 - Loss: 0.0871",
    "Epoch 53/80 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 53/80 - Batch 1700/1875 - Loss: 0.0050",
    "Epoch 53/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 53 completed - Train Acc: 99.48% - Val Acc: 98.16%",
    "Epoch 54/80 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 54/80 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 54/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 300/1875 - Loss: 0.0619",
    "Epoch 54/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 54/80 - Batch 700/1875 - Loss: 0.0040",
    "Epoch 54/80 - Batch 800/1875 - Loss: 0.0038",
    "Epoch 54/80 - Batch 900/1875 - Loss: 0.0028",
    "Epoch 54/80 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 54/80 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 54/80 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 54/80 - Batch 1300/1875 - Loss: 0.0613",
    "Epoch 54/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 54/80 - Batch 1600/1875 - Loss: 0.0007",
    "Epoch 54/80 - Batch 1700/1875 - Loss: 0.0153",
    "Epoch 54/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 54 completed - Train Acc: 99.56% - Val Acc: 97.78%",
    "Epoch 55/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 55/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 55/80 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 55/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 55/80 - Batch 600/1875 - Loss: 0.1651",
    "Epoch 55/80 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 55/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 900/1875 - Loss: 0.0024",
    "Epoch 55/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 1200/1875 - Loss: 0.0164",
    "Epoch 55/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 1500/1875 - Loss: 0.0043",
    "Epoch 55/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 55/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 55 completed - Train Acc: 99.51% - Val Acc: 97.75%",
    "Epoch 56/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 56/80 - Batch 100/1875 - Loss: 0.0105",
    "Epoch 56/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 300/1875 - Loss: 0.0014",
    "Epoch 56/80 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 56/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 56/80 - Batch 800/1875 - Loss: 0.0058",
    "Epoch 56/80 - Batch 900/1875 - Loss: 0.0021",
    "Epoch 56/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 1100/1875 - Loss: 0.0034",
    "Epoch 56/80 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 56/80 - Batch 1300/1875 - Loss: 0.0039",
    "Epoch 56/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 56/80 - Batch 1500/1875 - Loss: 0.0370",
    "Epoch 56/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 56/80 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 56/80 - Batch 1800/1875 - Loss: 0.0076",
    "Epoch 56 completed - Train Acc: 99.58% - Val Acc: 98.22%",
    "Epoch 57/80 - Batch 0/1875 - Loss: 0.0027",
    "Epoch 57/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 57/80 - Batch 200/1875 - Loss: 0.0755",
    "Epoch 57/80 - Batch 300/1875 - Loss: 0.0029",
    "Epoch 57/80 - Batch 400/1875 - Loss: 0.0017",
    "Epoch 57/80 - Batch 500/1875 - Loss: 0.1768",
    "Epoch 57/80 - Batch 600/1875 - Loss: 0.0192",
    "Epoch 57/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 57/80 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 57/80 - Batch 900/1875 - Loss: 0.0094",
    "Epoch 57/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 57/80 - Batch 1100/1875 - Loss: 0.0097",
    "Epoch 57/80 - Batch 1200/1875 - Loss: 0.1664",
    "Epoch 57/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 57/80 - Batch 1400/1875 - Loss: 0.0010",
    "Epoch 57/80 - Batch 1500/1875 - Loss: 0.0083",
    "Epoch 57/80 - Batch 1600/1875 - Loss: 0.0011",
    "Epoch 57/80 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 57/80 - Batch 1800/1875 - Loss: 0.0033",
    "Epoch 57 completed - Train Acc: 99.53% - Val Acc: 98.01%",
    "Epoch 58/80 - Batch 0/1875 - Loss: 0.0169",
    "Epoch 58/80 - Batch 100/1875 - Loss: 0.0046",
    "Epoch 58/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 58/80 - Batch 300/1875 - Loss: 0.0139",
    "Epoch 58/80 - Batch 400/1875 - Loss: 0.0709",
    "Epoch 58/80 - Batch 500/1875 - Loss: 0.0010",
    "Epoch 58/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 58/80 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 58/80 - Batch 800/1875 - Loss: 0.0276",
    "Epoch 58/80 - Batch 900/1875 - Loss: 0.0046",
    "Epoch 58/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 58/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 58/80 - Batch 1200/1875 - Loss: 0.0526",
    "Epoch 58/80 - Batch 1300/1875 - Loss: 0.0023",
    "Epoch 58/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 58/80 - Batch 1500/1875 - Loss: 0.0083",
    "Epoch 58/80 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 58/80 - Batch 1700/1875 - Loss: 0.0098",
    "Epoch 58/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 58 completed - Train Acc: 99.47% - Val Acc: 97.89%",
    "Epoch 59/80 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 59/80 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 59/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 600/1875 - Loss: 1.6075",
    "Epoch 59/80 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 59/80 - Batch 800/1875 - Loss: 0.0306",
    "Epoch 59/80 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 59/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 59/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 1200/1875 - Loss: 0.2021",
    "Epoch 59/80 - Batch 1300/1875 - Loss: 0.0093",
    "Epoch 59/80 - Batch 1400/1875 - Loss: 0.0005",
    "Epoch 59/80 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 59/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 59/80 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 59/80 - Batch 1800/1875 - Loss: 0.0009",
    "Epoch 59 completed - Train Acc: 99.54% - Val Acc: 97.93%",
    "Epoch 60/80 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 60/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 60/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 60/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 700/1875 - Loss: 0.0011",
    "Epoch 60/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 900/1875 - Loss: 0.0136",
    "Epoch 60/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 1100/1875 - Loss: 0.2054",
    "Epoch 60/80 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 60/80 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 60/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 1500/1875 - Loss: 0.0081",
    "Epoch 60/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 60 completed - Train Acc: 99.56% - Val Acc: 98.15%",
    "Epoch 61/80 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 61/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 61/80 - Batch 300/1875 - Loss: 0.1619",
    "Epoch 61/80 - Batch 400/1875 - Loss: 0.0215",
    "Epoch 61/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 600/1875 - Loss: 0.0065",
    "Epoch 61/80 - Batch 700/1875 - Loss: 0.0406",
    "Epoch 61/80 - Batch 800/1875 - Loss: 0.2841",
    "Epoch 61/80 - Batch 900/1875 - Loss: 0.0559",
    "Epoch 61/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 61/80 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 61/80 - Batch 1200/1875 - Loss: 0.0537",
    "Epoch 61/80 - Batch 1300/1875 - Loss: 0.0463",
    "Epoch 61/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 61/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 61 completed - Train Acc: 99.53% - Val Acc: 97.98%",
    "Epoch 62/80 - Batch 0/1875 - Loss: 0.0006",
    "Epoch 62/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 200/1875 - Loss: 0.0017",
    "Epoch 62/80 - Batch 300/1875 - Loss: 0.0012",
    "Epoch 62/80 - Batch 400/1875 - Loss: 0.1050",
    "Epoch 62/80 - Batch 500/1875 - Loss: 0.0191",
    "Epoch 62/80 - Batch 600/1875 - Loss: 0.0041",
    "Epoch 62/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 62/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 62/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 62/80 - Batch 1100/1875 - Loss: 0.0049",
    "Epoch 62/80 - Batch 1200/1875 - Loss: 0.0396",
    "Epoch 62/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 1400/1875 - Loss: 0.0038",
    "Epoch 62/80 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 62/80 - Batch 1600/1875 - Loss: 0.0201",
    "Epoch 62/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 62 completed - Train Acc: 99.57% - Val Acc: 97.62%",
    "Epoch 63/80 - Batch 0/1875 - Loss: 0.0864",
    "Epoch 63/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 63/80 - Batch 300/1875 - Loss: 0.0187",
    "Epoch 63/80 - Batch 400/1875 - Loss: 0.1102",
    "Epoch 63/80 - Batch 500/1875 - Loss: 0.0032",
    "Epoch 63/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 700/1875 - Loss: 0.0040",
    "Epoch 63/80 - Batch 800/1875 - Loss: 0.0010",
    "Epoch 63/80 - Batch 900/1875 - Loss: 0.0485",
    "Epoch 63/80 - Batch 1000/1875 - Loss: 0.0062",
    "Epoch 63/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1200/1875 - Loss: 0.0034",
    "Epoch 63/80 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 63/80 - Batch 1400/1875 - Loss: 0.0017",
    "Epoch 63/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 63/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1800/1875 - Loss: 0.1328",
    "Epoch 63 completed - Train Acc: 99.53% - Val Acc: 97.92%",
    "Epoch 64/80 - Batch 0/1875 - Loss: 0.0143",
    "Epoch 64/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 64/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 64/80 - Batch 300/1875 - Loss: 0.0072",
    "Epoch 64/80 - Batch 400/1875 - Loss: 0.0674",
    "Epoch 64/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 64/80 - Batch 700/1875 - Loss: 0.0297",
    "Epoch 64/80 - Batch 800/1875 - Loss: 0.0049",
    "Epoch 64/80 - Batch 900/1875 - Loss: 0.0028",
    "Epoch 64/80 - Batch 1000/1875 - Loss: 0.0841",
    "Epoch 64/80 - Batch 1100/1875 - Loss: 0.0834",
    "Epoch 64/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 64/80 - Batch 1300/1875 - Loss: 0.0003",
    "Epoch 64/80 - Batch 1400/1875 - Loss: 0.0151",
    "Epoch 64/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 64/80 - Batch 1600/1875 - Loss: 0.0117",
    "Epoch 64/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 64 completed - Train Acc: 99.47% - Val Acc: 97.84%",
    "Epoch 65/80 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 65/80 - Batch 100/1875 - Loss: 0.0089",
    "Epoch 65/80 - Batch 200/1875 - Loss: 0.0029",
    "Epoch 65/80 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 65/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 500/1875 - Loss: 0.0012",
    "Epoch 65/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 65/80 - Batch 800/1875 - Loss: 0.0491",
    "Epoch 65/80 - Batch 900/1875 - Loss: 0.4341",
    "Epoch 65/80 - Batch 1000/1875 - Loss: 0.5013",
    "Epoch 65/80 - Batch 1100/1875 - Loss: 0.0031",
    "Epoch 65/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 65/80 - Batch 1300/1875 - Loss: 0.0392",
    "Epoch 65/80 - Batch 1400/1875 - Loss: 0.0022",
    "Epoch 65/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 65/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 1800/1875 - Loss: 0.0697",
    "Epoch 65 completed - Train Acc: 99.53% - Val Acc: 98.11%",
    "Epoch 66/80 - Batch 0/1875 - Loss: 0.1074",
    "Epoch 66/80 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 66/80 - Batch 200/1875 - Loss: 0.0017",
    "Epoch 66/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 66/80 - Batch 400/1875 - Loss: 0.0034",
    "Epoch 66/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 600/1875 - Loss: 0.0010",
    "Epoch 66/80 - Batch 700/1875 - Loss: 0.0068",
    "Epoch 66/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 66/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 66/80 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 66/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 1500/1875 - Loss: 0.0298",
    "Epoch 66/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 66/80 - Batch 1700/1875 - Loss: 0.0012",
    "Epoch 66/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 66 completed - Train Acc: 99.55% - Val Acc: 97.60%",
    "Epoch 67/80 - Batch 0/1875 - Loss: 0.0161",
    "Epoch 67/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 200/1875 - Loss: 0.0042",
    "Epoch 67/80 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 67/80 - Batch 400/1875 - Loss: 0.0111",
    "Epoch 67/80 - Batch 500/1875 - Loss: 0.0004",
    "Epoch 67/80 - Batch 600/1875 - Loss: 0.0027",
    "Epoch 67/80 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 67/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 67/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 67/80 - Batch 1000/1875 - Loss: 0.0030",
    "Epoch 67/80 - Batch 1100/1875 - Loss: 0.0015",
    "Epoch 67/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 67/80 - Batch 1300/1875 - Loss: 0.0143",
    "Epoch 67/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 67/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 67/80 - Batch 1700/1875 - Loss: 0.0590",
    "Epoch 67/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 67 completed - Train Acc: 99.54% - Val Acc: 97.88%",
    "Epoch 68/80 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 68/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 68/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 68/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 68/80 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 68/80 - Batch 500/1875 - Loss: 0.0347",
    "Epoch 68/80 - Batch 600/1875 - Loss: 0.0448",
    "Epoch 68/80 - Batch 700/1875 - Loss: 0.0091",
    "Epoch 68/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 68/80 - Batch 900/1875 - Loss: 0.0024",
    "Epoch 68/80 - Batch 1000/1875 - Loss: 0.0062",
    "Epoch 68/80 - Batch 1100/1875 - Loss: 0.6506",
    "Epoch 68/80 - Batch 1200/1875 - Loss: 0.0047",
    "Epoch 68/80 - Batch 1300/1875 - Loss: 0.0009",
    "Epoch 68/80 - Batch 1400/1875 - Loss: 0.0030",
    "Epoch 68/80 - Batch 1500/1875 - Loss: 0.0009",
    "Epoch 68/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 68/80 - Batch 1700/1875 - Loss: 0.0021",
    "Epoch 68/80 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 68 completed - Train Acc: 99.52% - Val Acc: 97.25%",
    "Epoch 69/80 - Batch 0/1875 - Loss: 0.0015",
    "Epoch 69/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 69/80 - Batch 200/1875 - Loss: 0.0140",
    "Epoch 69/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 69/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 69/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 900/1875 - Loss: 0.0006",
    "Epoch 69/80 - Batch 1000/1875 - Loss: 0.0185",
    "Epoch 69/80 - Batch 1100/1875 - Loss: 0.3684",
    "Epoch 69/80 - Batch 1200/1875 - Loss: 0.0279",
    "Epoch 69/80 - Batch 1300/1875 - Loss: 0.0471",
    "Epoch 69/80 - Batch 1400/1875 - Loss: 0.0030",
    "Epoch 69/80 - Batch 1500/1875 - Loss: 0.0039",
    "Epoch 69/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 69/80 - Batch 1700/1875 - Loss: 0.0094",
    "Epoch 69/80 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 69 completed - Train Acc: 99.55% - Val Acc: 97.62%",
    "Epoch 70/80 - Batch 0/1875 - Loss: 0.0049",
    "Epoch 70/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 70/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 70/80 - Batch 300/1875 - Loss: 0.0652",
    "Epoch 70/80 - Batch 400/1875 - Loss: 0.0105",
    "Epoch 70/80 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 70/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 70/80 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 70/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 70/80 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 70/80 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 70/80 - Batch 1100/1875 - Loss: 0.0029",
    "Epoch 70/80 - Batch 1200/1875 - Loss: 0.0197",
    "Epoch 70/80 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 70/80 - Batch 1400/1875 - Loss: 0.0021",
    "Epoch 70/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 70/80 - Batch 1600/1875 - Loss: 0.0608",
    "Epoch 70/80 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 70/80 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 70 completed - Train Acc: 99.63% - Val Acc: 98.19%",
    "Epoch 71/80 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 71/80 - Batch 100/1875 - Loss: 0.0402",
    "Epoch 71/80 - Batch 200/1875 - Loss: 0.0158",
    "Epoch 71/80 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 71/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 71/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 600/1875 - Loss: 0.0110",
    "Epoch 71/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 71/80 - Batch 900/1875 - Loss: 0.0201",
    "Epoch 71/80 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 71/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 71/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 1300/1875 - Loss: 0.0104",
    "Epoch 71/80 - Batch 1400/1875 - Loss: 0.0008",
    "Epoch 71/80 - Batch 1500/1875 - Loss: 0.0125",
    "Epoch 71/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 71/80 - Batch 1700/1875 - Loss: 0.0075",
    "Epoch 71/80 - Batch 1800/1875 - Loss: 0.0686",
    "Epoch 71 completed - Train Acc: 99.59% - Val Acc: 97.91%",
    "Epoch 72/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 72/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 200/1875 - Loss: 0.0005",
    "Epoch 72/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 72/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 72/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 700/1875 - Loss: 0.0017",
    "Epoch 72/80 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 72/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 72/80 - Batch 1000/1875 - Loss: 0.0020",
    "Epoch 72/80 - Batch 1100/1875 - Loss: 0.2176",
    "Epoch 72/80 - Batch 1200/1875 - Loss: 0.0035",
    "Epoch 72/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 1500/1875 - Loss: 0.0615",
    "Epoch 72/80 - Batch 1600/1875 - Loss: 0.1429",
    "Epoch 72/80 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 72/80 - Batch 1800/1875 - Loss: 0.0006",
    "Epoch 72 completed - Train Acc: 99.48% - Val Acc: 97.98%",
    "Epoch 73/80 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 73/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 300/1875 - Loss: 0.0380",
    "Epoch 73/80 - Batch 400/1875 - Loss: 0.0041",
    "Epoch 73/80 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 73/80 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 73/80 - Batch 700/1875 - Loss: 0.0227",
    "Epoch 73/80 - Batch 800/1875 - Loss: 0.1225",
    "Epoch 73/80 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 73/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 1100/1875 - Loss: 0.0022",
    "Epoch 73/80 - Batch 1200/1875 - Loss: 0.0012",
    "Epoch 73/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 73/80 - Batch 1400/1875 - Loss: 0.0428",
    "Epoch 73/80 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 73/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 73/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 1800/1875 - Loss: 0.0613",
    "Epoch 73 completed - Train Acc: 99.54% - Val Acc: 97.72%",
    "Epoch 74/80 - Batch 0/1875 - Loss: 0.0006",
    "Epoch 74/80 - Batch 100/1875 - Loss: 0.0362",
    "Epoch 74/80 - Batch 200/1875 - Loss: 0.0037",
    "Epoch 74/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 74/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 74/80 - Batch 500/1875 - Loss: 0.0049",
    "Epoch 74/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 800/1875 - Loss: 0.1928",
    "Epoch 74/80 - Batch 900/1875 - Loss: 0.0057",
    "Epoch 74/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 1100/1875 - Loss: 0.1266",
    "Epoch 74/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 1400/1875 - Loss: 0.0923",
    "Epoch 74/80 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 74/80 - Batch 1600/1875 - Loss: 0.0017",
    "Epoch 74/80 - Batch 1700/1875 - Loss: 0.0027",
    "Epoch 74/80 - Batch 1800/1875 - Loss: 0.0151",
    "Epoch 74 completed - Train Acc: 99.67% - Val Acc: 97.93%",
    "Epoch 75/80 - Batch 0/1875 - Loss: 0.0038",
    "Epoch 75/80 - Batch 100/1875 - Loss: 0.0023",
    "Epoch 75/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 75/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 75/80 - Batch 400/1875 - Loss: 0.5448",
    "Epoch 75/80 - Batch 500/1875 - Loss: 0.0035",
    "Epoch 75/80 - Batch 600/1875 - Loss: 0.0037",
    "Epoch 75/80 - Batch 700/1875 - Loss: 0.0111",
    "Epoch 75/80 - Batch 800/1875 - Loss: 0.0069",
    "Epoch 75/80 - Batch 900/1875 - Loss: 0.0798",
    "Epoch 75/80 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 75/80 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 75/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 1300/1875 - Loss: 0.0006",
    "Epoch 75/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 1500/1875 - Loss: 0.0246",
    "Epoch 75/80 - Batch 1600/1875 - Loss: 0.0010",
    "Epoch 75/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 75 completed - Train Acc: 99.57% - Val Acc: 97.33%",
    "Epoch 76/80 - Batch 0/1875 - Loss: 0.0054",
    "Epoch 76/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 200/1875 - Loss: 0.1616",
    "Epoch 76/80 - Batch 300/1875 - Loss: 0.0007",
    "Epoch 76/80 - Batch 400/1875 - Loss: 0.1097",
    "Epoch 76/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 700/1875 - Loss: 0.0224",
    "Epoch 76/80 - Batch 800/1875 - Loss: 0.0309",
    "Epoch 76/80 - Batch 900/1875 - Loss: 0.0022",
    "Epoch 76/80 - Batch 1000/1875 - Loss: 0.0024",
    "Epoch 76/80 - Batch 1100/1875 - Loss: 0.0010",
    "Epoch 76/80 - Batch 1200/1875 - Loss: 0.0030",
    "Epoch 76/80 - Batch 1300/1875 - Loss: 0.0034",
    "Epoch 76/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 1700/1875 - Loss: 0.1838",
    "Epoch 76/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 76 completed - Train Acc: 99.61% - Val Acc: 97.82%",
    "Epoch 77/80 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 77/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 77/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 77/80 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 77/80 - Batch 400/1875 - Loss: 0.0194",
    "Epoch 77/80 - Batch 500/1875 - Loss: 0.0284",
    "Epoch 77/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 77/80 - Batch 700/1875 - Loss: 0.0018",
    "Epoch 77/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 77/80 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 77/80 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 77/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 77/80 - Batch 1500/1875 - Loss: 0.0396",
    "Epoch 77/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 77/80 - Batch 1800/1875 - Loss: 0.0032",
    "Epoch 77 completed - Train Acc: 99.50% - Val Acc: 97.87%",
    "Epoch 78/80 - Batch 0/1875 - Loss: 0.1054",
    "Epoch 78/80 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 78/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 78/80 - Batch 500/1875 - Loss: 0.0062",
    "Epoch 78/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 700/1875 - Loss: 0.0978",
    "Epoch 78/80 - Batch 800/1875 - Loss: 0.0043",
    "Epoch 78/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1200/1875 - Loss: 0.0087",
    "Epoch 78/80 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 78/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1500/1875 - Loss: 0.1042",
    "Epoch 78/80 - Batch 1600/1875 - Loss: 0.0020",
    "Epoch 78/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1800/1875 - Loss: 0.0115",
    "Epoch 78 completed - Train Acc: 99.58% - Val Acc: 97.97%",
    "Epoch 79/80 - Batch 0/1875 - Loss: 0.0754",
    "Epoch 79/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 400/1875 - Loss: 0.2971",
    "Epoch 79/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 600/1875 - Loss: 0.0014",
    "Epoch 79/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 900/1875 - Loss: 0.0145",
    "Epoch 79/80 - Batch 1000/1875 - Loss: 0.0656",
    "Epoch 79/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1200/1875 - Loss: 0.0144",
    "Epoch 79/80 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 79/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 79/80 - Batch 1500/1875 - Loss: 0.0131",
    "Epoch 79/80 - Batch 1600/1875 - Loss: 0.0018",
    "Epoch 79/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1800/1875 - Loss: 0.1782",
    "Epoch 79 completed - Train Acc: 99.62% - Val Acc: 97.64%",
    "Epoch 80/80 - Batch 0/1875 - Loss: 0.0636",
    "Epoch 80/80 - Batch 100/1875 - Loss: 0.0012",
    "Epoch 80/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 300/1875 - Loss: 0.1738",
    "Epoch 80/80 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 80/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 600/1875 - Loss: 0.0146",
    "Epoch 80/80 - Batch 700/1875 - Loss: 0.0508",
    "Epoch 80/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 900/1875 - Loss: 0.0063",
    "Epoch 80/80 - Batch 1000/1875 - Loss: 0.0016",
    "Epoch 80/80 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 80/80 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 80/80 - Batch 1300/1875 - Loss: 0.0346",
    "Epoch 80/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 80/80 - Batch 1500/1875 - Loss: 0.0057",
    "Epoch 80/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 80/80 - Batch 1700/1875 - Loss: 0.0037",
    "Epoch 80/80 - Batch 1800/1875 - Loss: 0.0053",
    "Epoch 80 completed - Train Acc: 99.56% - Val Acc: 97.93%",
    "Training completed. Model saved to models/ZPE-Colab-Sim_hnn_step8.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.28633718376023076,
      "train_accuracy": 91.46666666666667,
      "val_loss": 0.18373610994580705,
      "val_accuracy": 94.6
    },
    {
      "epoch": 2,
      "train_loss": 0.13822813457741093,
      "train_accuracy": 96.23166666666667,
      "val_loss": 0.1386190891266241,
      "val_accuracy": 96.31
    },
    {
      "epoch": 3,
      "train_loss": 0.1067763568361445,
      "train_accuracy": 97.11166666666666,
      "val_loss": 0.1303401794336653,
      "val_accuracy": 96.44
    },
    {
      "epoch": 4,
      "train_loss": 0.0853350010983219,
      "train_accuracy": 97.69666666666667,
      "val_loss": 0.10623753133000056,
      "val_accuracy": 97.26
    },
    {
      "epoch": 5,
      "train_loss": 0.07552595541894358,
      "train_accuracy": 97.95666666666666,
      "val_loss": 0.11271848604685826,
      "val_accuracy": 96.97
    },
    {
      "epoch": 6,
      "train_loss": 0.0663274136904627,
      "train_accuracy": 98.17666666666666,
      "val_loss": 0.106422206053578,
      "val_accuracy": 97.33
    },
    {
      "epoch": 7,
      "train_loss": 0.05932975804645442,
      "train_accuracy": 98.43166666666667,
      "val_loss": 0.11398148743770659,
      "val_accuracy": 97.34
    },
    {
      "epoch": 8,
      "train_loss": 0.05672073821564845,
      "train_accuracy": 98.53833333333333,
      "val_loss": 0.10376296310976496,
      "val_accuracy": 97.93
    },
    {
      "epoch": 9,
      "train_loss": 0.0542739072738681,
      "train_accuracy": 98.58,
      "val_loss": 0.09210014080152167,
      "val_accuracy": 97.79
    },
    {
      "epoch": 10,
      "train_loss": 0.047397483997295785,
      "train_accuracy": 98.715,
      "val_loss": 0.08432706097957675,
      "val_accuracy": 98.01
    },
    {
      "epoch": 11,
      "train_loss": 0.04276019296504874,
      "train_accuracy": 98.835,
      "val_loss": 0.10303066626796406,
      "val_accuracy": 97.51
    },
    {
      "epoch": 12,
      "train_loss": 0.04278148254954334,
      "train_accuracy": 98.85333333333334,
      "val_loss": 0.10729713935626886,
      "val_accuracy": 97.88
    },
    {
      "epoch": 13,
      "train_loss": 0.040173560495791356,
      "train_accuracy": 98.93166666666667,
      "val_loss": 0.10432403048414594,
      "val_accuracy": 97.94
    },
    {
      "epoch": 14,
      "train_loss": 0.03960420816740798,
      "train_accuracy": 98.98,
      "val_loss": 0.17398880483044807,
      "val_accuracy": 97.53
    },
    {
      "epoch": 15,
      "train_loss": 0.03850866047022167,
      "train_accuracy": 98.99833333333333,
      "val_loss": 0.10965113932954959,
      "val_accuracy": 97.62
    },
    {
      "epoch": 16,
      "train_loss": 0.03957792948350824,
      "train_accuracy": 99.00666666666666,
      "val_loss": 0.12194903050621238,
      "val_accuracy": 97.75
    },
    {
      "epoch": 17,
      "train_loss": 0.034993971230842164,
      "train_accuracy": 99.13666666666667,
      "val_loss": 0.13886345209281004,
      "val_accuracy": 97.75
    },
    {
      "epoch": 18,
      "train_loss": 0.03510217617656425,
      "train_accuracy": 99.14833333333333,
      "val_loss": 0.10410687160367758,
      "val_accuracy": 97.92
    },
    {
      "epoch": 19,
      "train_loss": 0.029991731557110678,
      "train_accuracy": 99.19,
      "val_loss": 0.11467689248794859,
      "val_accuracy": 97.81
    },
    {
      "epoch": 20,
      "train_loss": 0.034851957466233416,
      "train_accuracy": 99.12666666666667,
      "val_loss": 0.09844562264350562,
      "val_accuracy": 97.95
    },
    {
      "epoch": 21,
      "train_loss": 0.030906040585660946,
      "train_accuracy": 99.205,
      "val_loss": 0.11673711172819003,
      "val_accuracy": 97.81
    },
    {
      "epoch": 22,
      "train_loss": 0.02466150175920317,
      "train_accuracy": 99.27,
      "val_loss": 0.15105292694213124,
      "val_accuracy": 97.95
    },
    {
      "epoch": 23,
      "train_loss": 0.027797558156164616,
      "train_accuracy": 99.285,
      "val_loss": 0.12838553576501452,
      "val_accuracy": 98.04
    },
    {
      "epoch": 24,
      "train_loss": 0.02883556539561624,
      "train_accuracy": 99.31833333333333,
      "val_loss": 0.1255400069621914,
      "val_accuracy": 97.98
    },
    {
      "epoch": 25,
      "train_loss": 0.032325053923403645,
      "train_accuracy": 99.20666666666666,
      "val_loss": 0.122566225686833,
      "val_accuracy": 97.93
    },
    {
      "epoch": 26,
      "train_loss": 0.026004567163469484,
      "train_accuracy": 99.345,
      "val_loss": 0.13606245608846787,
      "val_accuracy": 98.06
    },
    {
      "epoch": 27,
      "train_loss": 0.030573940952108625,
      "train_accuracy": 99.245,
      "val_loss": 0.13179239814830712,
      "val_accuracy": 97.76
    },
    {
      "epoch": 28,
      "train_loss": 0.023065285342728763,
      "train_accuracy": 99.41833333333334,
      "val_loss": 0.13787287242978843,
      "val_accuracy": 97.96
    },
    {
      "epoch": 29,
      "train_loss": 0.0312582832646242,
      "train_accuracy": 99.30666666666667,
      "val_loss": 0.1215071637006426,
      "val_accuracy": 97.89
    },
    {
      "epoch": 30,
      "train_loss": 0.026558880972863186,
      "train_accuracy": 99.31333333333333,
      "val_loss": 0.11602312591357962,
      "val_accuracy": 98.14
    },
    {
      "epoch": 31,
      "train_loss": 0.03221070462219281,
      "train_accuracy": 99.285,
      "val_loss": 0.19346223554153347,
      "val_accuracy": 97.59
    },
    {
      "epoch": 32,
      "train_loss": 0.02842614509744284,
      "train_accuracy": 99.34166666666667,
      "val_loss": 0.13844809571406447,
      "val_accuracy": 97.84
    },
    {
      "epoch": 33,
      "train_loss": 0.022407469899926882,
      "train_accuracy": 99.46666666666667,
      "val_loss": 0.13472892526116845,
      "val_accuracy": 97.88
    },
    {
      "epoch": 34,
      "train_loss": 0.029779564834564613,
      "train_accuracy": 99.325,
      "val_loss": 0.12307401740284976,
      "val_accuracy": 97.77
    },
    {
      "epoch": 35,
      "train_loss": 0.023180686199743013,
      "train_accuracy": 99.47833333333334,
      "val_loss": 0.14419146819780057,
      "val_accuracy": 97.83
    },
    {
      "epoch": 36,
      "train_loss": 0.023378451114166886,
      "train_accuracy": 99.43333333333334,
      "val_loss": 0.15261063046063691,
      "val_accuracy": 97.9
    },
    {
      "epoch": 37,
      "train_loss": 0.022619763468793443,
      "train_accuracy": 99.44666666666667,
      "val_loss": 0.18949318296561718,
      "val_accuracy": 97.63
    },
    {
      "epoch": 38,
      "train_loss": 0.025831649689731073,
      "train_accuracy": 99.44,
      "val_loss": 0.18025135890535238,
      "val_accuracy": 97.91
    },
    {
      "epoch": 39,
      "train_loss": 0.028639312856341737,
      "train_accuracy": 99.335,
      "val_loss": 0.14945374464694894,
      "val_accuracy": 97.64
    },
    {
      "epoch": 40,
      "train_loss": 0.02014396801384494,
      "train_accuracy": 99.535,
      "val_loss": 0.17457162062764708,
      "val_accuracy": 98.03
    },
    {
      "epoch": 41,
      "train_loss": 0.026830380993201713,
      "train_accuracy": 99.385,
      "val_loss": 0.18227747999350166,
      "val_accuracy": 97.82
    },
    {
      "epoch": 42,
      "train_loss": 0.023572724611413975,
      "train_accuracy": 99.47,
      "val_loss": 0.18115325884283762,
      "val_accuracy": 97.69
    },
    {
      "epoch": 43,
      "train_loss": 0.024930132612561426,
      "train_accuracy": 99.43333333333334,
      "val_loss": 0.11951426341071203,
      "val_accuracy": 98.15
    },
    {
      "epoch": 44,
      "train_loss": 0.021395719700480154,
      "train_accuracy": 99.48,
      "val_loss": 0.15167114879271468,
      "val_accuracy": 97.9
    },
    {
      "epoch": 45,
      "train_loss": 0.025027207283572838,
      "train_accuracy": 99.42333333333333,
      "val_loss": 0.14311388871804712,
      "val_accuracy": 98.04
    },
    {
      "epoch": 46,
      "train_loss": 0.022467793985057823,
      "train_accuracy": 99.525,
      "val_loss": 0.15143544430698377,
      "val_accuracy": 97.61
    },
    {
      "epoch": 47,
      "train_loss": 0.02292593146546676,
      "train_accuracy": 99.51333333333334,
      "val_loss": 0.16163930992537293,
      "val_accuracy": 97.16
    },
    {
      "epoch": 48,
      "train_loss": 0.023568224124035177,
      "train_accuracy": 99.42333333333333,
      "val_loss": 0.14947093712284387,
      "val_accuracy": 98.11
    },
    {
      "epoch": 49,
      "train_loss": 0.025558867242687022,
      "train_accuracy": 99.465,
      "val_loss": 0.16652224677594046,
      "val_accuracy": 97.94
    },
    {
      "epoch": 50,
      "train_loss": 0.022020880422763744,
      "train_accuracy": 99.48,
      "val_loss": 0.15221218057759825,
      "val_accuracy": 97.63
    },
    {
      "epoch": 51,
      "train_loss": 0.01979864757791334,
      "train_accuracy": 99.52,
      "val_loss": 0.1382781684232296,
      "val_accuracy": 97.91
    },
    {
      "epoch": 52,
      "train_loss": 0.02236519985542146,
      "train_accuracy": 99.52333333333333,
      "val_loss": 0.14364201972675894,
      "val_accuracy": 98.05
    },
    {
      "epoch": 53,
      "train_loss": 0.02486682083915944,
      "train_accuracy": 99.47833333333334,
      "val_loss": 0.11564463797662607,
      "val_accuracy": 98.16
    },
    {
      "epoch": 54,
      "train_loss": 0.02105240758850307,
      "train_accuracy": 99.56333333333333,
      "val_loss": 0.14479013080254402,
      "val_accuracy": 97.78
    },
    {
      "epoch": 55,
      "train_loss": 0.020054977378179437,
      "train_accuracy": 99.50833333333334,
      "val_loss": 0.1465513777797871,
      "val_accuracy": 97.75
    },
    {
      "epoch": 56,
      "train_loss": 0.01752344254945205,
      "train_accuracy": 99.58166666666666,
      "val_loss": 0.1388231410617081,
      "val_accuracy": 98.22
    },
    {
      "epoch": 57,
      "train_loss": 0.022013536023705718,
      "train_accuracy": 99.52833333333334,
      "val_loss": 0.12951613565002779,
      "val_accuracy": 98.01
    },
    {
      "epoch": 58,
      "train_loss": 0.023225137851335054,
      "train_accuracy": 99.47166666666666,
      "val_loss": 0.14493643144807877,
      "val_accuracy": 97.89
    },
    {
      "epoch": 59,
      "train_loss": 0.02196570266361077,
      "train_accuracy": 99.54,
      "val_loss": 0.14179302054312967,
      "val_accuracy": 97.93
    },
    {
      "epoch": 60,
      "train_loss": 0.020520340791139183,
      "train_accuracy": 99.56,
      "val_loss": 0.15192175745105393,
      "val_accuracy": 98.15
    },
    {
      "epoch": 61,
      "train_loss": 0.02426066961332721,
      "train_accuracy": 99.53,
      "val_loss": 0.13284442289292575,
      "val_accuracy": 97.98
    },
    {
      "epoch": 62,
      "train_loss": 0.018653979241998873,
      "train_accuracy": 99.57333333333334,
      "val_loss": 0.1947750507611466,
      "val_accuracy": 97.62
    },
    {
      "epoch": 63,
      "train_loss": 0.02095710191462444,
      "train_accuracy": 99.525,
      "val_loss": 0.1739140946228295,
      "val_accuracy": 97.92
    },
    {
      "epoch": 64,
      "train_loss": 0.026500953332676498,
      "train_accuracy": 99.46833333333333,
      "val_loss": 0.13581292490779126,
      "val_accuracy": 97.84
    },
    {
      "epoch": 65,
      "train_loss": 0.022037702699038646,
      "train_accuracy": 99.535,
      "val_loss": 0.14548776251251255,
      "val_accuracy": 98.11
    },
    {
      "epoch": 66,
      "train_loss": 0.019413284933585805,
      "train_accuracy": 99.545,
      "val_loss": 0.17180114185418807,
      "val_accuracy": 97.6
    },
    {
      "epoch": 67,
      "train_loss": 0.020972842140020178,
      "train_accuracy": 99.54333333333334,
      "val_loss": 0.17858711704051855,
      "val_accuracy": 97.88
    },
    {
      "epoch": 68,
      "train_loss": 0.022451899050664876,
      "train_accuracy": 99.52,
      "val_loss": 0.1928413131301961,
      "val_accuracy": 97.25
    },
    {
      "epoch": 69,
      "train_loss": 0.020958504507915966,
      "train_accuracy": 99.55,
      "val_loss": 0.1500358481174497,
      "val_accuracy": 97.62
    },
    {
      "epoch": 70,
      "train_loss": 0.016165492739482826,
      "train_accuracy": 99.62833333333333,
      "val_loss": 0.15399753299866228,
      "val_accuracy": 98.19
    },
    {
      "epoch": 71,
      "train_loss": 0.019738391270142513,
      "train_accuracy": 99.58833333333334,
      "val_loss": 0.14774998590369748,
      "val_accuracy": 97.91
    },
    {
      "epoch": 72,
      "train_loss": 0.02212674287318917,
      "train_accuracy": 99.48166666666667,
      "val_loss": 0.124584844514544,
      "val_accuracy": 97.98
    },
    {
      "epoch": 73,
      "train_loss": 0.021443159619837365,
      "train_accuracy": 99.53833333333333,
      "val_loss": 0.12961946257175558,
      "val_accuracy": 97.72
    },
    {
      "epoch": 74,
      "train_loss": 0.015674460453715473,
      "train_accuracy": 99.665,
      "val_loss": 0.142365757038844,
      "val_accuracy": 97.93
    },
    {
      "epoch": 75,
      "train_loss": 0.01964881111279928,
      "train_accuracy": 99.56666666666666,
      "val_loss": 0.26059597993448147,
      "val_accuracy": 97.33
    },
    {
      "epoch": 76,
      "train_loss": 0.017240169651463688,
      "train_accuracy": 99.615,
      "val_loss": 0.19204780967785665,
      "val_accuracy": 97.82
    },
    {
      "epoch": 77,
      "train_loss": 0.02277985638093317,
      "train_accuracy": 99.495,
      "val_loss": 0.18001576187656426,
      "val_accuracy": 97.87
    },
    {
      "epoch": 78,
      "train_loss": 0.01979289164137863,
      "train_accuracy": 99.58,
      "val_loss": 0.1325602042013155,
      "val_accuracy": 97.97
    },
    {
      "epoch": 79,
      "train_loss": 0.01828349685109925,
      "train_accuracy": 99.61833333333334,
      "val_loss": 0.18600966996916463,
      "val_accuracy": 97.64
    },
    {
      "epoch": 80,
      "train_loss": 0.020615191814858075,
      "train_accuracy": 99.55666666666667,
      "val_loss": 0.12544391120143436,
      "val_accuracy": 97.93
    }
  ]
}