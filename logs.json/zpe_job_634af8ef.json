{
  "job_id": "zpe_job_634af8ef",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.0011,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.91,
      0.91,
      0.91,
      0.91,
      0.91,
      0.91
    ],
    "strengthParams": [
      0.09,
      0.09,
      0.09,
      0.09,
      0.09,
      0.09
    ],
    "noiseParams": [
      0.011,
      0.011,
      0.011,
      0.011,
      0.011,
      0.011
    ],
    "couplingParams": [
      0.12,
      0.12,
      0.12,
      0.12,
      0.12,
      0.08
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "baseConfigId": "zpe_job_d243fa84",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-20T05:52:43.326Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.2956",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.3537",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.2483",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.3481",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.1532",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.2690",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.1873",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.3934",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.7259",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.3228",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.3118",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.1550",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.0643",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.3500",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.0599",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.0643",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.4700",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.2394",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1433",
    "Epoch 1 completed - Train Acc: 91.78% - Val Acc: 94.75%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.0423",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0310",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.0582",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.0811",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.1199",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.1202",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.2554",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.2624",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.2069",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.5176",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0318",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.1233",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0637",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0668",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.0075",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0247",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.2359",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.1906",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0702",
    "Epoch 2 completed - Train Acc: 96.28% - Val Acc: 97.01%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0208",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0443",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0318",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0273",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.1307",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0042",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.3803",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.0915",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.1744",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.2063",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.0035",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0650",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1124",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.2432",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0056",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.2332",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0985",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.0557",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.0650",
    "Epoch 3 completed - Train Acc: 97.24% - Val Acc: 97.36%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.1214",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.0512",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0309",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0183",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0282",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0156",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.1225",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0599",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.1420",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0063",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.1592",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0159",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0037",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0091",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0110",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.1456",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0191",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0463",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0256",
    "Epoch 4 completed - Train Acc: 97.63% - Val Acc: 97.10%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0797",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0953",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0274",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.0523",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0129",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0970",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.0150",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0843",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0223",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0211",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0319",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0246",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0638",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0999",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.0217",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0026",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.1755",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0523",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0366",
    "Epoch 5 completed - Train Acc: 97.89% - Val Acc: 97.42%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.0196",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0265",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0005",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.4737",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0468",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0051",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.1191",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.5228",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0069",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.2813",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0053",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0121",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.2021",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0164",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0029",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0581",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0845",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0378",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0024"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.27127917027225096,
      "train_accuracy": 91.77833333333334,
      "val_loss": 0.18218892267250572,
      "val_accuracy": 94.75
    },
    {
      "epoch": 2,
      "train_loss": 0.13340520587712526,
      "train_accuracy": 96.285,
      "val_loss": 0.10707505933044793,
      "val_accuracy": 97.01
    },
    {
      "epoch": 3,
      "train_loss": 0.09989299940218528,
      "train_accuracy": 97.23833333333333,
      "val_loss": 0.09708409866598174,
      "val_accuracy": 97.36
    },
    {
      "epoch": 4,
      "train_loss": 0.08555798678022498,
      "train_accuracy": 97.63166666666666,
      "val_loss": 0.10664145335004242,
      "val_accuracy": 97.1
    },
    {
      "epoch": 5,
      "train_loss": 0.07468666832507977,
      "train_accuracy": 97.885,
      "val_loss": 0.09700609491926025,
      "val_accuracy": 97.42
    }
  ]
}