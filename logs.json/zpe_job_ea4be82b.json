{
  "job_id": "zpe_job_ea4be82b",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.001,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T21:38:36.208Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3168",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.6239",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.1661",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.1625",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.4829",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.4961",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.1461",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.1080",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.3002",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2338",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.4300",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.1778",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.2124",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.3067",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.0438",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.0966",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.2967",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.0783",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1944",
    "Epoch 1 completed - Train Acc: 91.84% - Val Acc: 94.98%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.1922",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0715",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.3980",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.1724",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.2531",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.1208",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0520",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.0757",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.0503",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0240",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0077",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.0173",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.2585",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.1955",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.1610",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0903",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.1379",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.2109",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0730",
    "Epoch 2 completed - Train Acc: 96.33% - Val Acc: 96.52%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0836",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.2990",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0089",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.2835",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0285",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0033",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.3467",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.0611",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.0038",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0227",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.6371",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.1939",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.0033",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0100",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0863",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.1777",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0219",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.0188",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.2595",
    "Epoch 3 completed - Train Acc: 97.23% - Val Acc: 97.54%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0415",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.1165",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.2043",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.1257",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0092",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0850",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.1146",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0152",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0634",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0041",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.0464",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0427",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0063",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.1150",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0219",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.0161",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0841",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.1663",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0159",
    "Epoch 4 completed - Train Acc: 97.66% - Val Acc: 97.52%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0027",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0392",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0482",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.2073",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0681",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.2389",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.0128",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0417",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0721",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.1026",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.3546",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0107",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0500",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.3116",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.2652",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0786",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0077",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.1348",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0124",
    "Epoch 5 completed - Train Acc: 97.97% - Val Acc: 97.54%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.1189",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0215",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0499",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.0197",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0065",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0087",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0037",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0010",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0067",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0614",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.1533",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0186",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.1009",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0785",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0396",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.2645",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0061",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.2431",
    "Epoch 6 completed - Train Acc: 98.23% - Val Acc: 97.19%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0062",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0111",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0152",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0046",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0249",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0115",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0244",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0444",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0044",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.0197",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.2196",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0059",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0037",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0366",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.1823",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0076",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0476",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0531",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 7 completed - Train Acc: 98.35% - Val Acc: 97.06%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0289",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0204",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0059",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.0224",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.3694",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0222",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0312",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0070",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0123",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0662",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0232",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0424",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.1340",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.0706",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0161",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0085",
    "Epoch 8 completed - Train Acc: 98.56% - Val Acc: 97.63%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0287",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0054",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0056",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0025",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0088",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0906",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0158",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0180",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0016",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0369",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0839",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.0698",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0077",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.1684",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0044",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.2163",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.1613",
    "Epoch 9 completed - Train Acc: 98.55% - Val Acc: 97.68%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0099",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0015",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0110",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.2278",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0657",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0152",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.4437",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0067",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0213",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0066",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.1079",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0641",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0561",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0012",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0020",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.1573",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0031",
    "Epoch 10 completed - Train Acc: 98.63% - Val Acc: 97.83%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0017",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.0391",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0049",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0179",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0039",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0199",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0231",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0121",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0031",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.0674",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0538",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0170",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0231",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0132",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.1382",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0039",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.0474",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0113",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0028",
    "Epoch 11 completed - Train Acc: 98.68% - Val Acc: 97.67%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0633",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0084",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0016",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0106",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0349",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0099",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.2522",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.0072",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.0574",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0503",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0119",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.0023",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0537",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0027",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0303",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0585",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0058",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0025",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.0078",
    "Epoch 12 completed - Train Acc: 98.83% - Val Acc: 97.07%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0430",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0013",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.0312",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.0339",
    "Epoch 13/30 - Batch 400/1875 - Loss: 0.0106",
    "Epoch 13/30 - Batch 500/1875 - Loss: 0.0066",
    "Epoch 13/30 - Batch 600/1875 - Loss: 0.0427",
    "Epoch 13/30 - Batch 700/1875 - Loss: 0.0043",
    "Epoch 13/30 - Batch 800/1875 - Loss: 0.0052",
    "Epoch 13/30 - Batch 900/1875 - Loss: 0.0032",
    "Epoch 13/30 - Batch 1000/1875 - Loss: 0.0120",
    "Epoch 13/30 - Batch 1100/1875 - Loss: 0.0071",
    "Epoch 13/30 - Batch 1200/1875 - Loss: 0.0102",
    "Epoch 13/30 - Batch 1300/1875 - Loss: 0.0210",
    "Epoch 13/30 - Batch 1400/1875 - Loss: 0.0159",
    "Epoch 13/30 - Batch 1500/1875 - Loss: 0.0762",
    "Epoch 13/30 - Batch 1600/1875 - Loss: 0.0608",
    "Epoch 13/30 - Batch 1700/1875 - Loss: 0.1375",
    "Epoch 13/30 - Batch 1800/1875 - Loss: 0.0069",
    "Epoch 13 completed - Train Acc: 98.83% - Val Acc: 97.71%",
    "Epoch 14/30 - Batch 0/1875 - Loss: 0.0307",
    "Epoch 14/30 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 14/30 - Batch 200/1875 - Loss: 0.0035",
    "Epoch 14/30 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 14/30 - Batch 400/1875 - Loss: 0.0130",
    "Epoch 14/30 - Batch 500/1875 - Loss: 0.0285",
    "Epoch 14/30 - Batch 600/1875 - Loss: 0.0057",
    "Epoch 14/30 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 14/30 - Batch 800/1875 - Loss: 0.0183",
    "Epoch 14/30 - Batch 900/1875 - Loss: 0.0075",
    "Epoch 14/30 - Batch 1000/1875 - Loss: 0.1654",
    "Epoch 14/30 - Batch 1100/1875 - Loss: 0.0338",
    "Epoch 14/30 - Batch 1200/1875 - Loss: 0.0021",
    "Epoch 14/30 - Batch 1300/1875 - Loss: 0.0058",
    "Epoch 14/30 - Batch 1400/1875 - Loss: 0.0063",
    "Epoch 14/30 - Batch 1500/1875 - Loss: 0.1177",
    "Epoch 14/30 - Batch 1600/1875 - Loss: 0.0042",
    "Epoch 14/30 - Batch 1700/1875 - Loss: 0.0031",
    "Epoch 14/30 - Batch 1800/1875 - Loss: 0.0127",
    "Epoch 14 completed - Train Acc: 98.89% - Val Acc: 97.70%",
    "Epoch 15/30 - Batch 0/1875 - Loss: 0.0430",
    "Epoch 15/30 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 15/30 - Batch 200/1875 - Loss: 0.0085",
    "Epoch 15/30 - Batch 300/1875 - Loss: 0.0206",
    "Epoch 15/30 - Batch 400/1875 - Loss: 0.0392",
    "Epoch 15/30 - Batch 500/1875 - Loss: 0.0010",
    "Epoch 15/30 - Batch 600/1875 - Loss: 0.0194",
    "Epoch 15/30 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 15/30 - Batch 800/1875 - Loss: 0.0469",
    "Epoch 15/30 - Batch 900/1875 - Loss: 0.0040",
    "Epoch 15/30 - Batch 1000/1875 - Loss: 0.1038",
    "Epoch 15/30 - Batch 1100/1875 - Loss: 0.0027",
    "Epoch 15/30 - Batch 1200/1875 - Loss: 0.0101",
    "Epoch 15/30 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 15/30 - Batch 1400/1875 - Loss: 0.0009",
    "Epoch 15/30 - Batch 1500/1875 - Loss: 0.0935",
    "Epoch 15/30 - Batch 1600/1875 - Loss: 0.0079",
    "Epoch 15/30 - Batch 1700/1875 - Loss: 0.1256",
    "Epoch 15/30 - Batch 1800/1875 - Loss: 0.0929",
    "Epoch 15 completed - Train Acc: 98.89% - Val Acc: 97.98%",
    "Epoch 16/30 - Batch 0/1875 - Loss: 0.0300",
    "Epoch 16/30 - Batch 100/1875 - Loss: 0.1288",
    "Epoch 16/30 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 16/30 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 16/30 - Batch 400/1875 - Loss: 0.0013",
    "Epoch 16/30 - Batch 500/1875 - Loss: 0.0471",
    "Epoch 16/30 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 16/30 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 16/30 - Batch 800/1875 - Loss: 0.0194",
    "Epoch 16/30 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 16/30 - Batch 1000/1875 - Loss: 0.0335",
    "Epoch 16/30 - Batch 1100/1875 - Loss: 0.0532",
    "Epoch 16/30 - Batch 1200/1875 - Loss: 0.0015",
    "Epoch 16/30 - Batch 1300/1875 - Loss: 0.0025",
    "Epoch 16/30 - Batch 1400/1875 - Loss: 0.0146",
    "Epoch 16/30 - Batch 1500/1875 - Loss: 0.0236",
    "Epoch 16/30 - Batch 1600/1875 - Loss: 0.0130",
    "Epoch 16/30 - Batch 1700/1875 - Loss: 0.0626",
    "Epoch 16/30 - Batch 1800/1875 - Loss: 0.1465",
    "Epoch 16 completed - Train Acc: 98.94% - Val Acc: 96.67%",
    "Epoch 17/30 - Batch 0/1875 - Loss: 0.1229",
    "Epoch 17/30 - Batch 100/1875 - Loss: 0.0049",
    "Epoch 17/30 - Batch 200/1875 - Loss: 0.0039",
    "Epoch 17/30 - Batch 300/1875 - Loss: 0.0044",
    "Epoch 17/30 - Batch 400/1875 - Loss: 0.0634",
    "Epoch 17/30 - Batch 500/1875 - Loss: 0.0463",
    "Epoch 17/30 - Batch 600/1875 - Loss: 0.0019",
    "Epoch 17/30 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 17/30 - Batch 800/1875 - Loss: 0.0786",
    "Epoch 17/30 - Batch 900/1875 - Loss: 0.0225",
    "Epoch 17/30 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 17/30 - Batch 1100/1875 - Loss: 0.0084",
    "Epoch 17/30 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 17/30 - Batch 1300/1875 - Loss: 0.0178",
    "Epoch 17/30 - Batch 1400/1875 - Loss: 0.0010",
    "Epoch 17/30 - Batch 1500/1875 - Loss: 0.0780",
    "Epoch 17/30 - Batch 1600/1875 - Loss: 0.0009",
    "Epoch 17/30 - Batch 1700/1875 - Loss: 0.0723",
    "Epoch 17/30 - Batch 1800/1875 - Loss: 0.0017",
    "Epoch 17 completed - Train Acc: 99.00% - Val Acc: 97.71%",
    "Epoch 18/30 - Batch 0/1875 - Loss: 0.0093",
    "Epoch 18/30 - Batch 100/1875 - Loss: 0.0025",
    "Epoch 18/30 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 18/30 - Batch 300/1875 - Loss: 0.0064",
    "Epoch 18/30 - Batch 400/1875 - Loss: 0.0022",
    "Epoch 18/30 - Batch 500/1875 - Loss: 0.0059",
    "Epoch 18/30 - Batch 600/1875 - Loss: 0.1431",
    "Epoch 18/30 - Batch 700/1875 - Loss: 0.0179",
    "Epoch 18/30 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 18/30 - Batch 900/1875 - Loss: 0.0984",
    "Epoch 18/30 - Batch 1000/1875 - Loss: 0.0093",
    "Epoch 18/30 - Batch 1100/1875 - Loss: 0.0045",
    "Epoch 18/30 - Batch 1200/1875 - Loss: 0.0541",
    "Epoch 18/30 - Batch 1300/1875 - Loss: 0.0843",
    "Epoch 18/30 - Batch 1400/1875 - Loss: 0.0174",
    "Epoch 18/30 - Batch 1500/1875 - Loss: 0.0071",
    "Epoch 18/30 - Batch 1600/1875 - Loss: 0.0709",
    "Epoch 18/30 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 18/30 - Batch 1800/1875 - Loss: 0.0040",
    "Epoch 18 completed - Train Acc: 98.99% - Val Acc: 97.62%",
    "Epoch 19/30 - Batch 0/1875 - Loss: 0.0019",
    "Epoch 19/30 - Batch 100/1875 - Loss: 0.0032",
    "Epoch 19/30 - Batch 200/1875 - Loss: 0.0074",
    "Epoch 19/30 - Batch 300/1875 - Loss: 0.0069",
    "Epoch 19/30 - Batch 400/1875 - Loss: 0.0019",
    "Epoch 19/30 - Batch 500/1875 - Loss: 0.0356",
    "Epoch 19/30 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 19/30 - Batch 700/1875 - Loss: 0.0009",
    "Epoch 19/30 - Batch 800/1875 - Loss: 0.0018",
    "Epoch 19/30 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 19/30 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 19/30 - Batch 1100/1875 - Loss: 0.0364",
    "Epoch 19/30 - Batch 1200/1875 - Loss: 0.0023",
    "Epoch 19/30 - Batch 1300/1875 - Loss: 0.0269",
    "Epoch 19/30 - Batch 1400/1875 - Loss: 0.0032",
    "Epoch 19/30 - Batch 1500/1875 - Loss: 0.0260",
    "Epoch 19/30 - Batch 1600/1875 - Loss: 0.0028",
    "Epoch 19/30 - Batch 1700/1875 - Loss: 0.0252",
    "Epoch 19/30 - Batch 1800/1875 - Loss: 0.0062",
    "Epoch 19 completed - Train Acc: 98.98% - Val Acc: 97.89%",
    "Epoch 20/30 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 20/30 - Batch 100/1875 - Loss: 0.0059",
    "Epoch 20/30 - Batch 200/1875 - Loss: 0.0031",
    "Epoch 20/30 - Batch 300/1875 - Loss: 0.0499",
    "Epoch 20/30 - Batch 400/1875 - Loss: 0.0131",
    "Epoch 20/30 - Batch 500/1875 - Loss: 0.0041",
    "Epoch 20/30 - Batch 600/1875 - Loss: 0.0757",
    "Epoch 20/30 - Batch 700/1875 - Loss: 0.1078",
    "Epoch 20/30 - Batch 800/1875 - Loss: 0.0349",
    "Epoch 20/30 - Batch 900/1875 - Loss: 0.0099",
    "Epoch 20/30 - Batch 1000/1875 - Loss: 0.0336",
    "Epoch 20/30 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 20/30 - Batch 1200/1875 - Loss: 0.0072",
    "Epoch 20/30 - Batch 1300/1875 - Loss: 0.0575",
    "Epoch 20/30 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 20/30 - Batch 1500/1875 - Loss: 0.0215",
    "Epoch 20/30 - Batch 1600/1875 - Loss: 0.0066",
    "Epoch 20/30 - Batch 1700/1875 - Loss: 0.0378",
    "Epoch 20/30 - Batch 1800/1875 - Loss: 0.1917",
    "Epoch 20 completed - Train Acc: 99.00% - Val Acc: 97.89%",
    "Epoch 21/30 - Batch 0/1875 - Loss: 0.0051",
    "Epoch 21/30 - Batch 100/1875 - Loss: 0.0046",
    "Epoch 21/30 - Batch 200/1875 - Loss: 0.0340",
    "Epoch 21/30 - Batch 300/1875 - Loss: 0.0907",
    "Epoch 21/30 - Batch 400/1875 - Loss: 0.0069",
    "Epoch 21/30 - Batch 500/1875 - Loss: 0.0041",
    "Epoch 21/30 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 21/30 - Batch 700/1875 - Loss: 0.1560",
    "Epoch 21/30 - Batch 800/1875 - Loss: 0.0252",
    "Epoch 21/30 - Batch 900/1875 - Loss: 0.0313",
    "Epoch 21/30 - Batch 1000/1875 - Loss: 0.0055",
    "Epoch 21/30 - Batch 1100/1875 - Loss: 0.0229",
    "Epoch 21/30 - Batch 1200/1875 - Loss: 0.0194",
    "Epoch 21/30 - Batch 1300/1875 - Loss: 0.1642",
    "Epoch 21/30 - Batch 1400/1875 - Loss: 0.0985",
    "Epoch 21/30 - Batch 1500/1875 - Loss: 0.0009",
    "Epoch 21/30 - Batch 1600/1875 - Loss: 0.0140",
    "Epoch 21/30 - Batch 1700/1875 - Loss: 0.0205",
    "Epoch 21/30 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 21 completed - Train Acc: 99.04% - Val Acc: 97.52%",
    "Epoch 22/30 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 22/30 - Batch 100/1875 - Loss: 0.0250",
    "Epoch 22/30 - Batch 200/1875 - Loss: 0.0329",
    "Epoch 22/30 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 22/30 - Batch 400/1875 - Loss: 0.0031",
    "Epoch 22/30 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 22/30 - Batch 600/1875 - Loss: 0.1187",
    "Epoch 22/30 - Batch 700/1875 - Loss: 0.2329",
    "Epoch 22/30 - Batch 800/1875 - Loss: 0.0285",
    "Epoch 22/30 - Batch 900/1875 - Loss: 0.0017",
    "Epoch 22/30 - Batch 1000/1875 - Loss: 0.0085",
    "Epoch 22/30 - Batch 1100/1875 - Loss: 0.0010",
    "Epoch 22/30 - Batch 1200/1875 - Loss: 0.0096",
    "Epoch 22/30 - Batch 1300/1875 - Loss: 0.0111",
    "Epoch 22/30 - Batch 1400/1875 - Loss: 0.0029",
    "Epoch 22/30 - Batch 1500/1875 - Loss: 0.0186",
    "Epoch 22/30 - Batch 1600/1875 - Loss: 0.0065",
    "Epoch 22/30 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 22/30 - Batch 1800/1875 - Loss: 0.0025",
    "Epoch 22 completed - Train Acc: 99.08% - Val Acc: 97.76%",
    "Epoch 23/30 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 23/30 - Batch 100/1875 - Loss: 0.0323",
    "Epoch 23/30 - Batch 200/1875 - Loss: 0.0384",
    "Epoch 23/30 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 23/30 - Batch 400/1875 - Loss: 0.0201",
    "Epoch 23/30 - Batch 500/1875 - Loss: 0.0121",
    "Epoch 23/30 - Batch 600/1875 - Loss: 0.0052",
    "Epoch 23/30 - Batch 700/1875 - Loss: 0.0071",
    "Epoch 23/30 - Batch 800/1875 - Loss: 0.0013",
    "Epoch 23/30 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 23/30 - Batch 1000/1875 - Loss: 0.0291",
    "Epoch 23/30 - Batch 1100/1875 - Loss: 0.0198",
    "Epoch 23/30 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 23/30 - Batch 1300/1875 - Loss: 0.0131",
    "Epoch 23/30 - Batch 1400/1875 - Loss: 0.0079",
    "Epoch 23/30 - Batch 1500/1875 - Loss: 0.0060",
    "Epoch 23/30 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 23/30 - Batch 1700/1875 - Loss: 0.0262",
    "Epoch 23/30 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 23 completed - Train Acc: 99.06% - Val Acc: 97.88%",
    "Epoch 24/30 - Batch 0/1875 - Loss: 0.0732",
    "Epoch 24/30 - Batch 100/1875 - Loss: 0.0040",
    "Epoch 24/30 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 24/30 - Batch 300/1875 - Loss: 0.0055",
    "Epoch 24/30 - Batch 400/1875 - Loss: 0.0282",
    "Epoch 24/30 - Batch 500/1875 - Loss: 0.0123",
    "Epoch 24/30 - Batch 600/1875 - Loss: 0.0010",
    "Epoch 24/30 - Batch 700/1875 - Loss: 0.2442",
    "Epoch 24/30 - Batch 800/1875 - Loss: 0.1372",
    "Epoch 24/30 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 24/30 - Batch 1000/1875 - Loss: 0.0441",
    "Epoch 24/30 - Batch 1100/1875 - Loss: 0.0082",
    "Epoch 24/30 - Batch 1200/1875 - Loss: 0.0033",
    "Epoch 24/30 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 24/30 - Batch 1400/1875 - Loss: 0.0092",
    "Epoch 24/30 - Batch 1500/1875 - Loss: 0.0027",
    "Epoch 24/30 - Batch 1600/1875 - Loss: 0.0529",
    "Epoch 24/30 - Batch 1700/1875 - Loss: 0.0205",
    "Epoch 24/30 - Batch 1800/1875 - Loss: 0.0447",
    "Epoch 24 completed - Train Acc: 99.12% - Val Acc: 98.01%",
    "Epoch 25/30 - Batch 0/1875 - Loss: 0.0290",
    "Epoch 25/30 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 25/30 - Batch 200/1875 - Loss: 0.0032",
    "Epoch 25/30 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 25/30 - Batch 400/1875 - Loss: 0.0327",
    "Epoch 25/30 - Batch 500/1875 - Loss: 0.0009",
    "Epoch 25/30 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 25/30 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 25/30 - Batch 800/1875 - Loss: 0.0049",
    "Epoch 25/30 - Batch 900/1875 - Loss: 0.0019",
    "Epoch 25/30 - Batch 1000/1875 - Loss: 0.0012",
    "Epoch 25/30 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 25/30 - Batch 1200/1875 - Loss: 0.0048",
    "Epoch 25/30 - Batch 1300/1875 - Loss: 0.0067",
    "Epoch 25/30 - Batch 1400/1875 - Loss: 0.0032",
    "Epoch 25/30 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 25/30 - Batch 1600/1875 - Loss: 0.0665",
    "Epoch 25/30 - Batch 1700/1875 - Loss: 0.0032",
    "Epoch 25/30 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 25 completed - Train Acc: 99.07% - Val Acc: 97.86%",
    "Epoch 26/30 - Batch 0/1875 - Loss: 0.0344",
    "Epoch 26/30 - Batch 100/1875 - Loss: 0.0013",
    "Epoch 26/30 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 26/30 - Batch 300/1875 - Loss: 0.0063",
    "Epoch 26/30 - Batch 400/1875 - Loss: 0.0542",
    "Epoch 26/30 - Batch 500/1875 - Loss: 0.0608",
    "Epoch 26/30 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 26/30 - Batch 700/1875 - Loss: 0.0119",
    "Epoch 26/30 - Batch 800/1875 - Loss: 0.1259",
    "Epoch 26/30 - Batch 900/1875 - Loss: 0.0676",
    "Epoch 26/30 - Batch 1000/1875 - Loss: 0.0061",
    "Epoch 26/30 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 26/30 - Batch 1200/1875 - Loss: 0.0522",
    "Epoch 26/30 - Batch 1300/1875 - Loss: 0.0024",
    "Epoch 26/30 - Batch 1400/1875 - Loss: 0.0880",
    "Epoch 26/30 - Batch 1500/1875 - Loss: 0.0529",
    "Epoch 26/30 - Batch 1600/1875 - Loss: 0.0222",
    "Epoch 26/30 - Batch 1700/1875 - Loss: 0.0272",
    "Epoch 26/30 - Batch 1800/1875 - Loss: 0.0594",
    "Epoch 26 completed - Train Acc: 99.13% - Val Acc: 98.11%",
    "Epoch 27/30 - Batch 0/1875 - Loss: 0.0012",
    "Epoch 27/30 - Batch 100/1875 - Loss: 0.0007",
    "Epoch 27/30 - Batch 200/1875 - Loss: 0.0136",
    "Epoch 27/30 - Batch 300/1875 - Loss: 0.0011",
    "Epoch 27/30 - Batch 400/1875 - Loss: 0.0916",
    "Epoch 27/30 - Batch 500/1875 - Loss: 0.0027",
    "Epoch 27/30 - Batch 600/1875 - Loss: 0.1094",
    "Epoch 27/30 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 27/30 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 27/30 - Batch 900/1875 - Loss: 0.1641",
    "Epoch 27/30 - Batch 1000/1875 - Loss: 0.0094",
    "Epoch 27/30 - Batch 1100/1875 - Loss: 0.0117",
    "Epoch 27/30 - Batch 1200/1875 - Loss: 0.0758",
    "Epoch 27/30 - Batch 1300/1875 - Loss: 0.0095",
    "Epoch 27/30 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 27/30 - Batch 1500/1875 - Loss: 0.0146",
    "Epoch 27/30 - Batch 1600/1875 - Loss: 0.0108",
    "Epoch 27/30 - Batch 1700/1875 - Loss: 1.0674",
    "Epoch 27/30 - Batch 1800/1875 - Loss: 0.0035",
    "Epoch 27 completed - Train Acc: 99.20% - Val Acc: 97.88%",
    "Epoch 28/30 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 28/30 - Batch 100/1875 - Loss: 0.0130",
    "Epoch 28/30 - Batch 200/1875 - Loss: 0.0058",
    "Epoch 28/30 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 28/30 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 28/30 - Batch 500/1875 - Loss: 0.0021",
    "Epoch 28/30 - Batch 600/1875 - Loss: 0.0129",
    "Epoch 28/30 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 28/30 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 28/30 - Batch 900/1875 - Loss: 0.0358",
    "Epoch 28/30 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 28/30 - Batch 1100/1875 - Loss: 0.0040",
    "Epoch 28/30 - Batch 1200/1875 - Loss: 0.0619",
    "Epoch 28/30 - Batch 1300/1875 - Loss: 0.2877",
    "Epoch 28/30 - Batch 1400/1875 - Loss: 0.0012",
    "Epoch 28/30 - Batch 1500/1875 - Loss: 0.0449",
    "Epoch 28/30 - Batch 1600/1875 - Loss: 0.0020",
    "Epoch 28/30 - Batch 1700/1875 - Loss: 0.0065",
    "Epoch 28/30 - Batch 1800/1875 - Loss: 0.0287",
    "Epoch 28 completed - Train Acc: 99.18% - Val Acc: 97.36%",
    "Epoch 29/30 - Batch 0/1875 - Loss: 0.0093",
    "Epoch 29/30 - Batch 100/1875 - Loss: 0.0048",
    "Epoch 29/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 29/30 - Batch 300/1875 - Loss: 0.0092",
    "Epoch 29/30 - Batch 400/1875 - Loss: 0.0013",
    "Epoch 29/30 - Batch 500/1875 - Loss: 0.0280",
    "Epoch 29/30 - Batch 600/1875 - Loss: 0.1909",
    "Epoch 29/30 - Batch 700/1875 - Loss: 0.0026",
    "Epoch 29/30 - Batch 800/1875 - Loss: 0.0273",
    "Epoch 29/30 - Batch 900/1875 - Loss: 0.0039",
    "Epoch 29/30 - Batch 1000/1875 - Loss: 0.0037",
    "Epoch 29/30 - Batch 1100/1875 - Loss: 0.1516",
    "Epoch 29/30 - Batch 1200/1875 - Loss: 0.0254",
    "Epoch 29/30 - Batch 1300/1875 - Loss: 0.0017",
    "Epoch 29/30 - Batch 1400/1875 - Loss: 0.0026",
    "Epoch 29/30 - Batch 1500/1875 - Loss: 0.0968",
    "Epoch 29/30 - Batch 1600/1875 - Loss: 0.0140",
    "Epoch 29/30 - Batch 1700/1875 - Loss: 0.0189",
    "Epoch 29/30 - Batch 1800/1875 - Loss: 0.0067",
    "Epoch 29 completed - Train Acc: 99.16% - Val Acc: 97.84%",
    "Epoch 30/30 - Batch 0/1875 - Loss: 0.0124",
    "Epoch 30/30 - Batch 100/1875 - Loss: 0.0345",
    "Epoch 30/30 - Batch 200/1875 - Loss: 0.0016",
    "Epoch 30/30 - Batch 300/1875 - Loss: 0.0026",
    "Epoch 30/30 - Batch 400/1875 - Loss: 0.0282",
    "Epoch 30/30 - Batch 500/1875 - Loss: 0.0041",
    "Epoch 30/30 - Batch 600/1875 - Loss: 0.0033",
    "Epoch 30/30 - Batch 700/1875 - Loss: 0.0045",
    "Epoch 30/30 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 30/30 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 30/30 - Batch 1000/1875 - Loss: 0.0050",
    "Epoch 30/30 - Batch 1100/1875 - Loss: 0.0244",
    "Epoch 30/30 - Batch 1200/1875 - Loss: 0.0027",
    "Epoch 30/30 - Batch 1300/1875 - Loss: 0.0077",
    "Epoch 30/30 - Batch 1400/1875 - Loss: 0.1347",
    "Epoch 30/30 - Batch 1500/1875 - Loss: 0.0027",
    "Epoch 30/30 - Batch 1600/1875 - Loss: 0.0045",
    "Epoch 30/30 - Batch 1700/1875 - Loss: 0.0025",
    "Epoch 30/30 - Batch 1800/1875 - Loss: 0.1156",
    "Epoch 30 completed - Train Acc: 99.14% - Val Acc: 96.99%",
    "Training completed. Model saved to models/ZPE-QuantumWeaver-V1.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.26852562489459914,
      "train_accuracy": 91.845,
      "val_loss": 0.16959765967957605,
      "val_accuracy": 94.98
    },
    {
      "epoch": 2,
      "train_loss": 0.13030631714450816,
      "train_accuracy": 96.33333333333333,
      "val_loss": 0.12287756985642191,
      "val_accuracy": 96.52
    },
    {
      "epoch": 3,
      "train_loss": 0.09912131855357438,
      "train_accuracy": 97.235,
      "val_loss": 0.09417910832261217,
      "val_accuracy": 97.54
    },
    {
      "epoch": 4,
      "train_loss": 0.08264217877667397,
      "train_accuracy": 97.66166666666666,
      "val_loss": 0.09387162247233423,
      "val_accuracy": 97.52
    },
    {
      "epoch": 5,
      "train_loss": 0.07180053292285496,
      "train_accuracy": 97.975,
      "val_loss": 0.08750033143157594,
      "val_accuracy": 97.54
    },
    {
      "epoch": 6,
      "train_loss": 0.06393651076733756,
      "train_accuracy": 98.23333333333333,
      "val_loss": 0.10314821337523405,
      "val_accuracy": 97.19
    },
    {
      "epoch": 7,
      "train_loss": 0.05780833568213663,
      "train_accuracy": 98.34833333333333,
      "val_loss": 0.1076679104489435,
      "val_accuracy": 97.06
    },
    {
      "epoch": 8,
      "train_loss": 0.050973950677384465,
      "train_accuracy": 98.56,
      "val_loss": 0.09832066390105774,
      "val_accuracy": 97.63
    },
    {
      "epoch": 9,
      "train_loss": 0.05028952389863552,
      "train_accuracy": 98.54666666666667,
      "val_loss": 0.08822405829863138,
      "val_accuracy": 97.68
    },
    {
      "epoch": 10,
      "train_loss": 0.04539131717305281,
      "train_accuracy": 98.62666666666667,
      "val_loss": 0.08690069501268988,
      "val_accuracy": 97.83
    },
    {
      "epoch": 11,
      "train_loss": 0.04326443715106774,
      "train_accuracy": 98.68166666666667,
      "val_loss": 0.0947505572636704,
      "val_accuracy": 97.67
    },
    {
      "epoch": 12,
      "train_loss": 0.04012365323995473,
      "train_accuracy": 98.83,
      "val_loss": 0.119809709292546,
      "val_accuracy": 97.07
    },
    {
      "epoch": 13,
      "train_loss": 0.04079301407810611,
      "train_accuracy": 98.835,
      "val_loss": 0.09341689516818856,
      "val_accuracy": 97.71
    },
    {
      "epoch": 14,
      "train_loss": 0.037466947220675256,
      "train_accuracy": 98.89166666666667,
      "val_loss": 0.09179537096502019,
      "val_accuracy": 97.7
    },
    {
      "epoch": 15,
      "train_loss": 0.03915812305840858,
      "train_accuracy": 98.88833333333334,
      "val_loss": 0.07797288406494389,
      "val_accuracy": 97.98
    },
    {
      "epoch": 16,
      "train_loss": 0.035034740599268116,
      "train_accuracy": 98.94166666666666,
      "val_loss": 0.157860658168431,
      "val_accuracy": 96.67
    },
    {
      "epoch": 17,
      "train_loss": 0.03280579808160934,
      "train_accuracy": 98.995,
      "val_loss": 0.08951131753622231,
      "val_accuracy": 97.71
    },
    {
      "epoch": 18,
      "train_loss": 0.03396247207173049,
      "train_accuracy": 98.99333333333334,
      "val_loss": 0.09944887298266744,
      "val_accuracy": 97.62
    },
    {
      "epoch": 19,
      "train_loss": 0.03395238816619579,
      "train_accuracy": 98.985,
      "val_loss": 0.09087860559490557,
      "val_accuracy": 97.89
    },
    {
      "epoch": 20,
      "train_loss": 0.03157767990275364,
      "train_accuracy": 99.005,
      "val_loss": 0.09126927662643507,
      "val_accuracy": 97.89
    },
    {
      "epoch": 21,
      "train_loss": 0.03173002921305112,
      "train_accuracy": 99.03666666666666,
      "val_loss": 0.10059520212086155,
      "val_accuracy": 97.52
    },
    {
      "epoch": 22,
      "train_loss": 0.029982268615134915,
      "train_accuracy": 99.08,
      "val_loss": 0.09623603236451772,
      "val_accuracy": 97.76
    },
    {
      "epoch": 23,
      "train_loss": 0.032263530018753955,
      "train_accuracy": 99.05833333333334,
      "val_loss": 0.08723569914649162,
      "val_accuracy": 97.88
    },
    {
      "epoch": 24,
      "train_loss": 0.02876350415608419,
      "train_accuracy": 99.11666666666666,
      "val_loss": 0.08811229791878757,
      "val_accuracy": 98.01
    },
    {
      "epoch": 25,
      "train_loss": 0.029643877815678326,
      "train_accuracy": 99.07333333333334,
      "val_loss": 0.08538463044707036,
      "val_accuracy": 97.86
    },
    {
      "epoch": 26,
      "train_loss": 0.029292924221813156,
      "train_accuracy": 99.12666666666667,
      "val_loss": 0.08988233711824113,
      "val_accuracy": 98.11
    },
    {
      "epoch": 27,
      "train_loss": 0.027601731368014103,
      "train_accuracy": 99.19666666666667,
      "val_loss": 0.08675638301408649,
      "val_accuracy": 97.88
    },
    {
      "epoch": 28,
      "train_loss": 0.0280731927981183,
      "train_accuracy": 99.18333333333334,
      "val_loss": 0.10373649001032025,
      "val_accuracy": 97.36
    },
    {
      "epoch": 29,
      "train_loss": 0.028421685743807393,
      "train_accuracy": 99.15833333333333,
      "val_loss": 0.08064975090236637,
      "val_accuracy": 97.84
    },
    {
      "epoch": 30,
      "train_loss": 0.02883123434346635,
      "train_accuracy": 99.145,
      "val_loss": 0.12653408059484067,
      "val_accuracy": 96.99
    }
  ]
}