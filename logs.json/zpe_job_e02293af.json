{
  "job_id": "zpe_job_e02293af",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.0011,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.91,
      0.91,
      0.91,
      0.91,
      0.91,
      0.91
    ],
    "strengthParams": [
      0.09,
      0.09,
      0.09,
      0.09,
      0.09,
      0.09
    ],
    "noiseParams": [
      0.011,
      0.011,
      0.011,
      0.011,
      0.011,
      0.011
    ],
    "couplingParams": [
      0.12,
      0.12,
      0.12,
      0.12,
      0.12,
      0.08
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "baseConfigId": "zpe_job_d243fa84",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-20T05:52:43.226Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3012",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.2634",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.4614",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.1005",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.2629",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.2125",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.2606",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.0111",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.1434",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2326",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.1465",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.0348",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.0629",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.1941",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.1673",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.3188",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.1050",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.1291",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.0245",
    "Epoch 1 completed - Train Acc: 91.73% - Val Acc: 96.00%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.1876",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0757",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.3726",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.1504",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.1040",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.1942",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0569",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.2272",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.0817",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0700",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0491",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.0424",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0076",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0596",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.1034",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.4652",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.0299",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.3352",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0173",
    "Epoch 2 completed - Train Acc: 96.34% - Val Acc: 96.79%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.3022",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0576",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0192",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0969",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0023",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0715",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.0041",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.1158",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.1367",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0554",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0427",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1242",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0379",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0494",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.0570",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0501",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.1874",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.1660",
    "Epoch 3 completed - Train Acc: 97.11% - Val Acc: 96.41%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0610",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.0124",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0016",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.1282",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0309",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0109",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0245",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0258",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0801",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.0866",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0424",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0647",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0800",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0444",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.0152",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0242",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0798",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.1003",
    "Epoch 4 completed - Train Acc: 97.58% - Val Acc: 97.63%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0161",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0283",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.2206",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.1101",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.1851",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0432",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.0298",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0080",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.1665",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.1093",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0215",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0236",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0664",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0070",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.3972",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0104",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0019",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0025",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.0311",
    "Epoch 5 completed - Train Acc: 97.96% - Val Acc: 97.73%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.0917",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0880",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0186",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.1843",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0938",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.1406",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0056",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0840",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0397",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0142",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.1079",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.2155",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0075",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0838",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.2152",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0029",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0150",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0697"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.27204415417015554,
      "train_accuracy": 91.735,
      "val_loss": 0.1499515945753328,
      "val_accuracy": 96.0
    },
    {
      "epoch": 2,
      "train_loss": 0.13037022701843332,
      "train_accuracy": 96.33833333333334,
      "val_loss": 0.1220509956080166,
      "val_accuracy": 96.79
    },
    {
      "epoch": 3,
      "train_loss": 0.10410529996543191,
      "train_accuracy": 97.11,
      "val_loss": 0.13103260554967613,
      "val_accuracy": 96.41
    },
    {
      "epoch": 4,
      "train_loss": 0.08741540064829557,
      "train_accuracy": 97.57666666666667,
      "val_loss": 0.09003163739076865,
      "val_accuracy": 97.63
    },
    {
      "epoch": 5,
      "train_loss": 0.07251712293808427,
      "train_accuracy": 97.96166666666667,
      "val_loss": 0.08807990878311033,
      "val_accuracy": 97.73
    }
  ]
}