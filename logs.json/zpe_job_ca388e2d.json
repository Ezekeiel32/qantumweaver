{
  "job_id": "zpe_job_ca388e2d",
  "status": "stopped",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.0011,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.07
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "baseConfigId": "zpe_job_d243fa84",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-20T05:46:29.385Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3141",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.3772",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.8599",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.4992",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.0680",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.3235",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.2703",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.1499",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.2689",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2899",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.1304",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.0312",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.4948",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.0257",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.1354",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.3026",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.2623",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.1694",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1727",
    "Epoch 1 completed - Train Acc: 91.96% - Val Acc: 95.87%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.0805",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.1919",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.3118",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.2252",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.1138",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.2081",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.0981",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.1495",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.1863",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0847",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.4534",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.1608",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0695",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0418",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.0448",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.1096",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.1470",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.2752",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.1784",
    "Epoch 2 completed - Train Acc: 96.50% - Val Acc: 96.47%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.2538",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0475",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0493",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0994",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0368",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.0226",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.1145",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.0476",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.4587",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0743",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.2883",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0780",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1129",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.2579",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0058",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.0690",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0162",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.1553",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.1047",
    "Epoch 3 completed - Train Acc: 97.28% - Val Acc: 97.48%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0442",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.0138",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0097",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0109",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0807",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0569",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0195",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0400",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.2177",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0066",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.0837",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0426",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.1902",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.1463",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0733",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.0355",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0034",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0380",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0359",
    "Epoch 4 completed - Train Acc: 97.61% - Val Acc: 97.09%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0299",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0272",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.2178",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.0063",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.1714",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.1260",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.1426",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.1040",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0452",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0060",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.2142",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0044",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0173",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0148",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.1523",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0224",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0765",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.3277",
    "Epoch 5 completed - Train Acc: 97.93% - Val Acc: 97.21%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.1279",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.2217",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0161",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.1702",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.1096",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0481",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0037",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0522",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.0832",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.1648",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0384",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0587",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0473",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0743",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.0798",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0066",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0368",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0491",
    "Epoch 6 completed - Train Acc: 98.18% - Val Acc: 97.55%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0064",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.1356",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0901",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.1529",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0012",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0209",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0191",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0246",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.0459",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.3762",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0082",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.2726",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0022",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0563",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0189",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0016",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0137",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.3027",
    "Epoch 7 completed - Train Acc: 98.30% - Val Acc: 97.90%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0133",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0348",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0046",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.0074",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.1684",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.1119",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0829",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0015",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0023",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0053",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0305",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.2300",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0151",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0126",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.1479",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0081",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0061",
    "Epoch 8 completed - Train Acc: 98.43% - Val Acc: 96.91%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0104",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0030",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0017",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0723",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.2881",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.2633",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0132",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0150",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0112",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.0582",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0554",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.1194",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.2835",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.1541",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0211",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.1700",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0697",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.1497",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0100",
    "Epoch 9 completed - Train Acc: 98.48% - Val Acc: 97.73%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0156",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0496",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0273",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0131",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.0015",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0352",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.1458",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0176",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0888",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0063",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0067",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.0030",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.1689",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0039",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0797",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0649",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0818",
    "Epoch 10 completed - Train Acc: 98.61% - Val Acc: 97.66%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.1097",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.1311",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0102",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0546",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0653",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0051",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0317",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0121",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.1537",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0629",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0039",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0160",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0008",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.3511",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0323",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0375",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0457",
    "Epoch 11 completed - Train Acc: 98.62% - Val Acc: 98.01%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0279",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0124",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0138",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.2839",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0550",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0006",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.1585",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.0031",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.0177",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.1969",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0243",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.2380",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0187",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0133",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0029",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0655",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0594",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0665",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.0078",
    "Epoch 12 completed - Train Acc: 98.69% - Val Acc: 97.72%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.0122",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0207"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.27056808288544415,
      "train_accuracy": 91.95666666666666,
      "val_loss": 0.14284866030875462,
      "val_accuracy": 95.87
    },
    {
      "epoch": 2,
      "train_loss": 0.12920535203410932,
      "train_accuracy": 96.505,
      "val_loss": 0.12404743793501204,
      "val_accuracy": 96.47
    },
    {
      "epoch": 3,
      "train_loss": 0.1000285049533161,
      "train_accuracy": 97.28,
      "val_loss": 0.09542381821554333,
      "val_accuracy": 97.48
    },
    {
      "epoch": 4,
      "train_loss": 0.08527499270066619,
      "train_accuracy": 97.60666666666667,
      "val_loss": 0.1088563601293723,
      "val_accuracy": 97.09
    },
    {
      "epoch": 5,
      "train_loss": 0.07452267596732903,
      "train_accuracy": 97.92666666666666,
      "val_loss": 0.10371259475986402,
      "val_accuracy": 97.21
    },
    {
      "epoch": 6,
      "train_loss": 0.0637244778771419,
      "train_accuracy": 98.18,
      "val_loss": 0.09651964809892,
      "val_accuracy": 97.55
    },
    {
      "epoch": 7,
      "train_loss": 0.05977557281353899,
      "train_accuracy": 98.30333333333333,
      "val_loss": 0.09309203413899066,
      "val_accuracy": 97.9
    },
    {
      "epoch": 8,
      "train_loss": 0.05324491256780069,
      "train_accuracy": 98.42666666666666,
      "val_loss": 0.1240958883981265,
      "val_accuracy": 96.91
    },
    {
      "epoch": 9,
      "train_loss": 0.05254159109014242,
      "train_accuracy": 98.47833333333334,
      "val_loss": 0.09048737078775301,
      "val_accuracy": 97.73
    },
    {
      "epoch": 10,
      "train_loss": 0.047844356068624375,
      "train_accuracy": 98.61333333333333,
      "val_loss": 0.09927763864929869,
      "val_accuracy": 97.66
    },
    {
      "epoch": 11,
      "train_loss": 0.045987373984240305,
      "train_accuracy": 98.61666666666666,
      "val_loss": 0.08881113615600113,
      "val_accuracy": 98.01
    },
    {
      "epoch": 12,
      "train_loss": 0.04353923573838935,
      "train_accuracy": 98.695,
      "val_loss": 0.09156935978928185,
      "val_accuracy": 97.72
    }
  ]
}