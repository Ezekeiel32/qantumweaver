{
  "job_id": "zpe_job_8f659ae0",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.001,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": false,
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T22:05:01.540Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3080",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.6545",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.4906",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.2519",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.2243",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.3869",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.2424",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.0554",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.3595",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.0581",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.2437",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.2728",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.3028",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.4906",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.1145",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.0434",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.1412",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.0864",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.2550",
    "Epoch 1 completed - Train Acc: 91.88% - Val Acc: 95.80%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.1541",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.2584",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.1631",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.0862",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.0538",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.1566",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.2454",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.1368",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.0653",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.0099",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.0729",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.0706",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.0705",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.0148",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.0281",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0210",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.0252",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.1235",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.0091",
    "Epoch 2 completed - Train Acc: 96.49% - Val Acc: 96.87%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0358",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0954",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.1642",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.0138",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0459",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.1103",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.1591",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.1071",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.0916",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.0851",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.2150",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.2831",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.0211",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0504",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0141",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.0252",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0432",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.1000",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.1707",
    "Epoch 3 completed - Train Acc: 97.25% - Val Acc: 97.24%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.0196",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.1683",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.0257",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0194",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0546",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0237",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0720",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0757",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.1521",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.2202",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.1249",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.0331",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.0126",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0386",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0159",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.1092",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.1083",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0198",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0149",
    "Epoch 4 completed - Train Acc: 97.77% - Val Acc: 97.16%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0733",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0118",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.3442",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0012",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0095",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.0134",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.0141",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.0068",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0499",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0623",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.0381",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0771",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0039",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.0258",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.0375",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0091",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0358",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.1012",
    "Epoch 5 completed - Train Acc: 97.95% - Val Acc: 97.84%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.1920",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0099",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0046",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.0815",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0081",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0080",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0600",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0680",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0066",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.1054",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0083",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0088",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0196",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.0493",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0132",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.1193",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0167",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0585",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0037",
    "Epoch 6 completed - Train Acc: 98.20% - Val Acc: 97.31%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.1857",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0596",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0241",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0166",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0031",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0053",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0504",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.1840",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0393",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.0604",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.1235",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0196",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0016",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0362",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0492",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0009",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0113",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.1105",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.0279",
    "Epoch 7 completed - Train Acc: 98.31% - Val Acc: 97.38%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0028",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0271",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0241",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0057",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.1262",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.1351",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0085",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0150",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0137",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0006",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.1297",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0112",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0838",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0028",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0127",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.0112",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0128",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0766",
    "Epoch 8 completed - Train Acc: 98.48% - Val Acc: 97.54%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.2334",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.1483",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0023",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0376",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0825",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0272",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0838",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0052",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0655",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.0056",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0229",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0709",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.0019",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0020",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0521",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0201",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0953",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0027",
    "Epoch 9 completed - Train Acc: 98.60% - Val Acc: 97.67%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0233",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.1123",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0988",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0218",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.0173",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0062",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0291",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0013",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.0072",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0062",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.0023",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.0070",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0280",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.0023",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.0034",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0207",
    "Epoch 10 completed - Train Acc: 98.74% - Val Acc: 97.74%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0013",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.1190",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0105",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.1226",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.2278",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0386",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0053",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0883",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0173",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.1290",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.0110",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0999",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0112",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.0222",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0528",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.2110",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0450",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0503",
    "Epoch 11 completed - Train Acc: 98.71% - Val Acc: 97.51%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.1014",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0965"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.266449537816147,
      "train_accuracy": 91.87833333333333,
      "val_loss": 0.14828426499788563,
      "val_accuracy": 95.8
    },
    {
      "epoch": 2,
      "train_loss": 0.12393110013262679,
      "train_accuracy": 96.49,
      "val_loss": 0.11676260304405539,
      "val_accuracy": 96.87
    },
    {
      "epoch": 3,
      "train_loss": 0.09767361891192074,
      "train_accuracy": 97.24833333333333,
      "val_loss": 0.10502512054329574,
      "val_accuracy": 97.24
    },
    {
      "epoch": 4,
      "train_loss": 0.08011574484333396,
      "train_accuracy": 97.76666666666667,
      "val_loss": 0.1035550386936775,
      "val_accuracy": 97.16
    },
    {
      "epoch": 5,
      "train_loss": 0.07329976662212673,
      "train_accuracy": 97.94833333333334,
      "val_loss": 0.07802553775451297,
      "val_accuracy": 97.84
    },
    {
      "epoch": 6,
      "train_loss": 0.06340795990980697,
      "train_accuracy": 98.20333333333333,
      "val_loss": 0.0969011226953065,
      "val_accuracy": 97.31
    },
    {
      "epoch": 7,
      "train_loss": 0.05830035932584821,
      "train_accuracy": 98.30666666666667,
      "val_loss": 0.09359842842231171,
      "val_accuracy": 97.38
    },
    {
      "epoch": 8,
      "train_loss": 0.05300448324126191,
      "train_accuracy": 98.47833333333334,
      "val_loss": 0.09906837382753113,
      "val_accuracy": 97.54
    },
    {
      "epoch": 9,
      "train_loss": 0.047544659365389576,
      "train_accuracy": 98.6,
      "val_loss": 0.09187375530618795,
      "val_accuracy": 97.67
    },
    {
      "epoch": 10,
      "train_loss": 0.043543176962186896,
      "train_accuracy": 98.73833333333333,
      "val_loss": 0.09202390476000005,
      "val_accuracy": 97.74
    },
    {
      "epoch": 11,
      "train_loss": 0.04499945876822264,
      "train_accuracy": 98.70666666666666,
      "val_loss": 0.09895375355018113,
      "val_accuracy": 97.51
    }
  ]
}