{
  "job_id": "zpe_job_aee5ef82",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-Colab-Sim_hnn_step8",
    "totalEpochs": 80,
    "batchSize": 32,
    "learningRate": 0.00135,
    "weightDecay": 1e-05,
    "momentumParams": [
      0.965,
      0.855,
      0.785,
      0.725,
      0.415,
      0.755
    ],
    "strengthParams": [
      0.185,
      0.545,
      0.405,
      0.595,
      0.365,
      0.435
    ],
    "noiseParams": [
      0.165,
      0.265,
      0.245,
      0.435,
      0.285,
      0.235
    ],
    "couplingParams": [
      0.78,
      0.83,
      0.8,
      0.77,
      0.74,
      0.63
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.07,
    "quantumMode": true,
    "baseConfigId": "zpe_job_86f76b88",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T20:45:48.032Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/80 - Batch 0/1875 - Loss: 2.3175",
    "Epoch 1/80 - Batch 100/1875 - Loss: 0.3610",
    "Epoch 1/80 - Batch 200/1875 - Loss: 0.3804",
    "Epoch 1/80 - Batch 300/1875 - Loss: 0.2639",
    "Epoch 1/80 - Batch 400/1875 - Loss: 0.0510",
    "Epoch 1/80 - Batch 500/1875 - Loss: 0.4159",
    "Epoch 1/80 - Batch 600/1875 - Loss: 0.2835",
    "Epoch 1/80 - Batch 700/1875 - Loss: 0.7990",
    "Epoch 1/80 - Batch 800/1875 - Loss: 0.2792",
    "Epoch 1/80 - Batch 900/1875 - Loss: 0.0752",
    "Epoch 1/80 - Batch 1000/1875 - Loss: 0.0645",
    "Epoch 1/80 - Batch 1100/1875 - Loss: 0.1048",
    "Epoch 1/80 - Batch 1200/1875 - Loss: 0.2831",
    "Epoch 1/80 - Batch 1300/1875 - Loss: 0.2509",
    "Epoch 1/80 - Batch 1400/1875 - Loss: 0.0370",
    "Epoch 1/80 - Batch 1500/1875 - Loss: 0.3045",
    "Epoch 1/80 - Batch 1600/1875 - Loss: 0.4619",
    "Epoch 1/80 - Batch 1700/1875 - Loss: 0.0305",
    "Epoch 1/80 - Batch 1800/1875 - Loss: 0.2688",
    "Epoch 1 completed - Train Acc: 91.64% - Val Acc: 95.31%",
    "Epoch 2/80 - Batch 0/1875 - Loss: 0.4244",
    "Epoch 2/80 - Batch 100/1875 - Loss: 0.1731",
    "Epoch 2/80 - Batch 200/1875 - Loss: 0.1791",
    "Epoch 2/80 - Batch 300/1875 - Loss: 0.1264",
    "Epoch 2/80 - Batch 400/1875 - Loss: 0.0578",
    "Epoch 2/80 - Batch 500/1875 - Loss: 0.0293",
    "Epoch 2/80 - Batch 600/1875 - Loss: 0.0148",
    "Epoch 2/80 - Batch 700/1875 - Loss: 0.1166",
    "Epoch 2/80 - Batch 800/1875 - Loss: 0.4259",
    "Epoch 2/80 - Batch 900/1875 - Loss: 0.3606",
    "Epoch 2/80 - Batch 1000/1875 - Loss: 0.3300",
    "Epoch 2/80 - Batch 1100/1875 - Loss: 0.0066",
    "Epoch 2/80 - Batch 1200/1875 - Loss: 0.0315",
    "Epoch 2/80 - Batch 1300/1875 - Loss: 0.0976",
    "Epoch 2/80 - Batch 1400/1875 - Loss: 0.1881",
    "Epoch 2/80 - Batch 1500/1875 - Loss: 0.2297",
    "Epoch 2/80 - Batch 1600/1875 - Loss: 0.0496",
    "Epoch 2/80 - Batch 1700/1875 - Loss: 0.3515",
    "Epoch 2/80 - Batch 1800/1875 - Loss: 0.0105",
    "Epoch 2 completed - Train Acc: 96.25% - Val Acc: 95.97%",
    "Epoch 3/80 - Batch 0/1875 - Loss: 0.1208",
    "Epoch 3/80 - Batch 100/1875 - Loss: 0.5118",
    "Epoch 3/80 - Batch 200/1875 - Loss: 0.0021",
    "Epoch 3/80 - Batch 300/1875 - Loss: 0.1455",
    "Epoch 3/80 - Batch 400/1875 - Loss: 0.0229",
    "Epoch 3/80 - Batch 500/1875 - Loss: 0.0332",
    "Epoch 3/80 - Batch 600/1875 - Loss: 0.4661",
    "Epoch 3/80 - Batch 700/1875 - Loss: 0.0024",
    "Epoch 3/80 - Batch 800/1875 - Loss: 0.0356",
    "Epoch 3/80 - Batch 900/1875 - Loss: 0.0329",
    "Epoch 3/80 - Batch 1000/1875 - Loss: 0.1303",
    "Epoch 3/80 - Batch 1100/1875 - Loss: 0.0064",
    "Epoch 3/80 - Batch 1200/1875 - Loss: 0.0120",
    "Epoch 3/80 - Batch 1300/1875 - Loss: 0.3280",
    "Epoch 3/80 - Batch 1400/1875 - Loss: 0.2328",
    "Epoch 3/80 - Batch 1500/1875 - Loss: 0.0080",
    "Epoch 3/80 - Batch 1600/1875 - Loss: 0.0898",
    "Epoch 3/80 - Batch 1700/1875 - Loss: 0.0186",
    "Epoch 3/80 - Batch 1800/1875 - Loss: 0.0233",
    "Epoch 3 completed - Train Acc: 97.14% - Val Acc: 97.31%",
    "Epoch 4/80 - Batch 0/1875 - Loss: 0.0834",
    "Epoch 4/80 - Batch 100/1875 - Loss: 0.0390",
    "Epoch 4/80 - Batch 200/1875 - Loss: 0.1205",
    "Epoch 4/80 - Batch 300/1875 - Loss: 0.0024",
    "Epoch 4/80 - Batch 400/1875 - Loss: 0.0210",
    "Epoch 4/80 - Batch 500/1875 - Loss: 0.1321",
    "Epoch 4/80 - Batch 600/1875 - Loss: 0.0522",
    "Epoch 4/80 - Batch 700/1875 - Loss: 0.0013",
    "Epoch 4/80 - Batch 800/1875 - Loss: 0.0041",
    "Epoch 4/80 - Batch 900/1875 - Loss: 0.1498",
    "Epoch 4/80 - Batch 1000/1875 - Loss: 0.1832",
    "Epoch 4/80 - Batch 1100/1875 - Loss: 0.0383",
    "Epoch 4/80 - Batch 1200/1875 - Loss: 0.3045",
    "Epoch 4/80 - Batch 1300/1875 - Loss: 0.0294",
    "Epoch 4/80 - Batch 1400/1875 - Loss: 0.0625",
    "Epoch 4/80 - Batch 1500/1875 - Loss: 0.1875",
    "Epoch 4/80 - Batch 1600/1875 - Loss: 0.1881",
    "Epoch 4/80 - Batch 1700/1875 - Loss: 0.1259",
    "Epoch 4/80 - Batch 1800/1875 - Loss: 0.1924",
    "Epoch 4 completed - Train Acc: 97.52% - Val Acc: 97.39%",
    "Epoch 5/80 - Batch 0/1875 - Loss: 0.0031",
    "Epoch 5/80 - Batch 100/1875 - Loss: 0.0984",
    "Epoch 5/80 - Batch 200/1875 - Loss: 0.1010",
    "Epoch 5/80 - Batch 300/1875 - Loss: 0.1077",
    "Epoch 5/80 - Batch 400/1875 - Loss: 0.1521",
    "Epoch 5/80 - Batch 500/1875 - Loss: 0.0253",
    "Epoch 5/80 - Batch 600/1875 - Loss: 0.0461",
    "Epoch 5/80 - Batch 700/1875 - Loss: 0.1281",
    "Epoch 5/80 - Batch 800/1875 - Loss: 0.0032",
    "Epoch 5/80 - Batch 900/1875 - Loss: 0.0039",
    "Epoch 5/80 - Batch 1000/1875 - Loss: 0.0719",
    "Epoch 5/80 - Batch 1100/1875 - Loss: 0.0226",
    "Epoch 5/80 - Batch 1200/1875 - Loss: 0.0128",
    "Epoch 5/80 - Batch 1300/1875 - Loss: 0.0369",
    "Epoch 5/80 - Batch 1400/1875 - Loss: 0.0449",
    "Epoch 5/80 - Batch 1500/1875 - Loss: 0.1093",
    "Epoch 5/80 - Batch 1600/1875 - Loss: 0.0094",
    "Epoch 5/80 - Batch 1700/1875 - Loss: 0.0037",
    "Epoch 5/80 - Batch 1800/1875 - Loss: 0.2202",
    "Epoch 5 completed - Train Acc: 97.88% - Val Acc: 97.03%",
    "Epoch 6/80 - Batch 0/1875 - Loss: 0.0076",
    "Epoch 6/80 - Batch 100/1875 - Loss: 0.0701",
    "Epoch 6/80 - Batch 200/1875 - Loss: 0.0991",
    "Epoch 6/80 - Batch 300/1875 - Loss: 0.0037",
    "Epoch 6/80 - Batch 400/1875 - Loss: 0.1276",
    "Epoch 6/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 6/80 - Batch 600/1875 - Loss: 0.0141",
    "Epoch 6/80 - Batch 700/1875 - Loss: 0.0126",
    "Epoch 6/80 - Batch 800/1875 - Loss: 0.1492",
    "Epoch 6/80 - Batch 900/1875 - Loss: 0.0021",
    "Epoch 6/80 - Batch 1000/1875 - Loss: 0.0049",
    "Epoch 6/80 - Batch 1100/1875 - Loss: 0.0876",
    "Epoch 6/80 - Batch 1200/1875 - Loss: 0.0340",
    "Epoch 6/80 - Batch 1300/1875 - Loss: 0.0090",
    "Epoch 6/80 - Batch 1400/1875 - Loss: 0.0066",
    "Epoch 6/80 - Batch 1500/1875 - Loss: 0.1556",
    "Epoch 6/80 - Batch 1600/1875 - Loss: 0.0011",
    "Epoch 6/80 - Batch 1700/1875 - Loss: 0.0610",
    "Epoch 6/80 - Batch 1800/1875 - Loss: 0.0057",
    "Epoch 6 completed - Train Acc: 98.16% - Val Acc: 97.66%",
    "Epoch 7/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 7/80 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 7/80 - Batch 200/1875 - Loss: 0.2079",
    "Epoch 7/80 - Batch 300/1875 - Loss: 0.1937",
    "Epoch 7/80 - Batch 400/1875 - Loss: 0.1514",
    "Epoch 7/80 - Batch 500/1875 - Loss: 0.0129",
    "Epoch 7/80 - Batch 600/1875 - Loss: 0.2897",
    "Epoch 7/80 - Batch 700/1875 - Loss: 0.3077",
    "Epoch 7/80 - Batch 800/1875 - Loss: 0.0571",
    "Epoch 7/80 - Batch 900/1875 - Loss: 0.0017",
    "Epoch 7/80 - Batch 1000/1875 - Loss: 0.0066",
    "Epoch 7/80 - Batch 1100/1875 - Loss: 0.0029",
    "Epoch 7/80 - Batch 1200/1875 - Loss: 0.0190",
    "Epoch 7/80 - Batch 1300/1875 - Loss: 0.0707",
    "Epoch 7/80 - Batch 1400/1875 - Loss: 0.0685",
    "Epoch 7/80 - Batch 1500/1875 - Loss: 0.0025",
    "Epoch 7/80 - Batch 1600/1875 - Loss: 0.0185",
    "Epoch 7/80 - Batch 1700/1875 - Loss: 0.0289",
    "Epoch 7/80 - Batch 1800/1875 - Loss: 0.1139",
    "Epoch 7 completed - Train Acc: 98.36% - Val Acc: 97.78%",
    "Epoch 8/80 - Batch 0/1875 - Loss: 0.0210",
    "Epoch 8/80 - Batch 100/1875 - Loss: 0.0551",
    "Epoch 8/80 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 8/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 8/80 - Batch 400/1875 - Loss: 0.1195",
    "Epoch 8/80 - Batch 500/1875 - Loss: 0.0400",
    "Epoch 8/80 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 8/80 - Batch 700/1875 - Loss: 0.0547",
    "Epoch 8/80 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 8/80 - Batch 900/1875 - Loss: 0.0123",
    "Epoch 8/80 - Batch 1000/1875 - Loss: 0.0499",
    "Epoch 8/80 - Batch 1100/1875 - Loss: 0.0021",
    "Epoch 8/80 - Batch 1200/1875 - Loss: 0.0938",
    "Epoch 8/80 - Batch 1300/1875 - Loss: 0.0021",
    "Epoch 8/80 - Batch 1400/1875 - Loss: 0.1788",
    "Epoch 8/80 - Batch 1500/1875 - Loss: 0.0039",
    "Epoch 8/80 - Batch 1600/1875 - Loss: 0.0099",
    "Epoch 8/80 - Batch 1700/1875 - Loss: 0.0097",
    "Epoch 8/80 - Batch 1800/1875 - Loss: 0.0092",
    "Epoch 8 completed - Train Acc: 98.53% - Val Acc: 97.83%",
    "Epoch 9/80 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 9/80 - Batch 100/1875 - Loss: 0.1300",
    "Epoch 9/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 9/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 9/80 - Batch 400/1875 - Loss: 0.0373",
    "Epoch 9/80 - Batch 500/1875 - Loss: 0.0284",
    "Epoch 9/80 - Batch 600/1875 - Loss: 0.0473",
    "Epoch 9/80 - Batch 700/1875 - Loss: 0.0084",
    "Epoch 9/80 - Batch 800/1875 - Loss: 0.0112",
    "Epoch 9/80 - Batch 900/1875 - Loss: 0.0008",
    "Epoch 9/80 - Batch 1000/1875 - Loss: 0.0235",
    "Epoch 9/80 - Batch 1100/1875 - Loss: 0.0464",
    "Epoch 9/80 - Batch 1200/1875 - Loss: 0.0483",
    "Epoch 9/80 - Batch 1300/1875 - Loss: 0.0003",
    "Epoch 9/80 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 9/80 - Batch 1500/1875 - Loss: 0.0092",
    "Epoch 9/80 - Batch 1600/1875 - Loss: 0.0039",
    "Epoch 9/80 - Batch 1700/1875 - Loss: 0.0018",
    "Epoch 9/80 - Batch 1800/1875 - Loss: 0.6772",
    "Epoch 9 completed - Train Acc: 98.63% - Val Acc: 97.97%",
    "Epoch 10/80 - Batch 0/1875 - Loss: 0.0186",
    "Epoch 10/80 - Batch 100/1875 - Loss: 0.1015",
    "Epoch 10/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 10/80 - Batch 300/1875 - Loss: 0.0509",
    "Epoch 10/80 - Batch 400/1875 - Loss: 0.0967",
    "Epoch 10/80 - Batch 500/1875 - Loss: 0.1748",
    "Epoch 10/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 10/80 - Batch 700/1875 - Loss: 0.0795",
    "Epoch 10/80 - Batch 800/1875 - Loss: 0.0786",
    "Epoch 10/80 - Batch 900/1875 - Loss: 0.0905",
    "Epoch 10/80 - Batch 1000/1875 - Loss: 0.0474",
    "Epoch 10/80 - Batch 1100/1875 - Loss: 0.0028",
    "Epoch 10/80 - Batch 1200/1875 - Loss: 0.0175",
    "Epoch 10/80 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 10/80 - Batch 1400/1875 - Loss: 0.0016",
    "Epoch 10/80 - Batch 1500/1875 - Loss: 0.0369",
    "Epoch 10/80 - Batch 1600/1875 - Loss: 0.0622",
    "Epoch 10/80 - Batch 1700/1875 - Loss: 0.1072",
    "Epoch 10/80 - Batch 1800/1875 - Loss: 0.0065",
    "Epoch 10 completed - Train Acc: 98.71% - Val Acc: 97.67%",
    "Epoch 11/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 11/80 - Batch 100/1875 - Loss: 0.0091",
    "Epoch 11/80 - Batch 200/1875 - Loss: 0.0078",
    "Epoch 11/80 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 11/80 - Batch 400/1875 - Loss: 0.0345",
    "Epoch 11/80 - Batch 500/1875 - Loss: 0.0043",
    "Epoch 11/80 - Batch 600/1875 - Loss: 0.2538",
    "Epoch 11/80 - Batch 700/1875 - Loss: 0.0021",
    "Epoch 11/80 - Batch 800/1875 - Loss: 0.0246",
    "Epoch 11/80 - Batch 900/1875 - Loss: 0.0665",
    "Epoch 11/80 - Batch 1000/1875 - Loss: 0.0297",
    "Epoch 11/80 - Batch 1100/1875 - Loss: 0.0836",
    "Epoch 11/80 - Batch 1200/1875 - Loss: 0.0064",
    "Epoch 11/80 - Batch 1300/1875 - Loss: 0.0043",
    "Epoch 11/80 - Batch 1400/1875 - Loss: 0.0078",
    "Epoch 11/80 - Batch 1500/1875 - Loss: 0.0950",
    "Epoch 11/80 - Batch 1600/1875 - Loss: 0.0359",
    "Epoch 11/80 - Batch 1700/1875 - Loss: 0.1691",
    "Epoch 11/80 - Batch 1800/1875 - Loss: 0.5324",
    "Epoch 11 completed - Train Acc: 98.71% - Val Acc: 97.68%",
    "Epoch 12/80 - Batch 0/1875 - Loss: 0.0023",
    "Epoch 12/80 - Batch 100/1875 - Loss: 0.0344",
    "Epoch 12/80 - Batch 200/1875 - Loss: 0.0027",
    "Epoch 12/80 - Batch 300/1875 - Loss: 0.1060",
    "Epoch 12/80 - Batch 400/1875 - Loss: 0.0016",
    "Epoch 12/80 - Batch 500/1875 - Loss: 0.0405",
    "Epoch 12/80 - Batch 600/1875 - Loss: 0.0394",
    "Epoch 12/80 - Batch 700/1875 - Loss: 0.0111",
    "Epoch 12/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 12/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 12/80 - Batch 1000/1875 - Loss: 0.0188",
    "Epoch 12/80 - Batch 1100/1875 - Loss: 0.0857",
    "Epoch 12/80 - Batch 1200/1875 - Loss: 0.1482",
    "Epoch 12/80 - Batch 1300/1875 - Loss: 0.1374",
    "Epoch 12/80 - Batch 1400/1875 - Loss: 0.0027",
    "Epoch 12/80 - Batch 1500/1875 - Loss: 0.2417",
    "Epoch 12/80 - Batch 1600/1875 - Loss: 0.0063",
    "Epoch 12/80 - Batch 1700/1875 - Loss: 0.0139",
    "Epoch 12/80 - Batch 1800/1875 - Loss: 0.1645",
    "Epoch 12 completed - Train Acc: 98.92% - Val Acc: 97.24%",
    "Epoch 13/80 - Batch 0/1875 - Loss: 0.0303",
    "Epoch 13/80 - Batch 100/1875 - Loss: 0.0137",
    "Epoch 13/80 - Batch 200/1875 - Loss: 0.0034",
    "Epoch 13/80 - Batch 300/1875 - Loss: 0.0568",
    "Epoch 13/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 13/80 - Batch 500/1875 - Loss: 0.1127",
    "Epoch 13/80 - Batch 600/1875 - Loss: 0.0151",
    "Epoch 13/80 - Batch 700/1875 - Loss: 0.1825",
    "Epoch 13/80 - Batch 800/1875 - Loss: 0.0324",
    "Epoch 13/80 - Batch 900/1875 - Loss: 0.2932",
    "Epoch 13/80 - Batch 1000/1875 - Loss: 0.0193",
    "Epoch 13/80 - Batch 1100/1875 - Loss: 0.3827",
    "Epoch 13/80 - Batch 1200/1875 - Loss: 0.0024",
    "Epoch 13/80 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 13/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 13/80 - Batch 1500/1875 - Loss: 0.0159",
    "Epoch 13/80 - Batch 1600/1875 - Loss: 0.0627",
    "Epoch 13/80 - Batch 1700/1875 - Loss: 0.0041",
    "Epoch 13/80 - Batch 1800/1875 - Loss: 0.0024",
    "Epoch 13 completed - Train Acc: 98.94% - Val Acc: 97.44%",
    "Epoch 14/80 - Batch 0/1875 - Loss: 0.0019",
    "Epoch 14/80 - Batch 100/1875 - Loss: 0.2739",
    "Epoch 14/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 14/80 - Batch 300/1875 - Loss: 0.0118",
    "Epoch 14/80 - Batch 400/1875 - Loss: 0.1435",
    "Epoch 14/80 - Batch 500/1875 - Loss: 0.0248",
    "Epoch 14/80 - Batch 600/1875 - Loss: 0.0415",
    "Epoch 14/80 - Batch 700/1875 - Loss: 0.0025",
    "Epoch 14/80 - Batch 800/1875 - Loss: 0.0421",
    "Epoch 14/80 - Batch 900/1875 - Loss: 0.0040",
    "Epoch 14/80 - Batch 1000/1875 - Loss: 0.0094",
    "Epoch 14/80 - Batch 1100/1875 - Loss: 0.0015",
    "Epoch 14/80 - Batch 1200/1875 - Loss: 0.0090",
    "Epoch 14/80 - Batch 1300/1875 - Loss: 0.0512",
    "Epoch 14/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 14/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 14/80 - Batch 1600/1875 - Loss: 0.0416",
    "Epoch 14/80 - Batch 1700/1875 - Loss: 0.0192",
    "Epoch 14/80 - Batch 1800/1875 - Loss: 0.1662",
    "Epoch 14 completed - Train Acc: 98.94% - Val Acc: 97.51%",
    "Epoch 15/80 - Batch 0/1875 - Loss: 0.0016",
    "Epoch 15/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 15/80 - Batch 200/1875 - Loss: 0.1511",
    "Epoch 15/80 - Batch 300/1875 - Loss: 0.0260",
    "Epoch 15/80 - Batch 400/1875 - Loss: 0.0291",
    "Epoch 15/80 - Batch 500/1875 - Loss: 0.0430",
    "Epoch 15/80 - Batch 600/1875 - Loss: 0.0012",
    "Epoch 15/80 - Batch 700/1875 - Loss: 0.0249",
    "Epoch 15/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 15/80 - Batch 900/1875 - Loss: 0.0558",
    "Epoch 15/80 - Batch 1000/1875 - Loss: 0.0687",
    "Epoch 15/80 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 15/80 - Batch 1200/1875 - Loss: 0.0057",
    "Epoch 15/80 - Batch 1300/1875 - Loss: 0.1092",
    "Epoch 15/80 - Batch 1400/1875 - Loss: 0.0014",
    "Epoch 15/80 - Batch 1500/1875 - Loss: 0.1246",
    "Epoch 15/80 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 15/80 - Batch 1700/1875 - Loss: 0.0197",
    "Epoch 15/80 - Batch 1800/1875 - Loss: 0.0051",
    "Epoch 15 completed - Train Acc: 99.05% - Val Acc: 97.78%",
    "Epoch 16/80 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 16/80 - Batch 100/1875 - Loss: 0.0019",
    "Epoch 16/80 - Batch 200/1875 - Loss: 0.0529",
    "Epoch 16/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 16/80 - Batch 400/1875 - Loss: 0.0735",
    "Epoch 16/80 - Batch 500/1875 - Loss: 0.0774",
    "Epoch 16/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 16/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 16/80 - Batch 800/1875 - Loss: 0.0018",
    "Epoch 16/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 16/80 - Batch 1000/1875 - Loss: 0.0159",
    "Epoch 16/80 - Batch 1100/1875 - Loss: 0.0014",
    "Epoch 16/80 - Batch 1200/1875 - Loss: 0.0253",
    "Epoch 16/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 16/80 - Batch 1400/1875 - Loss: 0.0775",
    "Epoch 16/80 - Batch 1500/1875 - Loss: 0.0012",
    "Epoch 16/80 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 16/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 16/80 - Batch 1800/1875 - Loss: 0.0464",
    "Epoch 16 completed - Train Acc: 99.09% - Val Acc: 97.65%",
    "Epoch 17/80 - Batch 0/1875 - Loss: 0.1932",
    "Epoch 17/80 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 17/80 - Batch 200/1875 - Loss: 0.0176",
    "Epoch 17/80 - Batch 300/1875 - Loss: 0.1691",
    "Epoch 17/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 17/80 - Batch 500/1875 - Loss: 0.2600",
    "Epoch 17/80 - Batch 600/1875 - Loss: 0.0044",
    "Epoch 17/80 - Batch 700/1875 - Loss: 0.0165",
    "Epoch 17/80 - Batch 800/1875 - Loss: 0.0020",
    "Epoch 17/80 - Batch 900/1875 - Loss: 0.1599",
    "Epoch 17/80 - Batch 1000/1875 - Loss: 0.0369",
    "Epoch 17/80 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 17/80 - Batch 1200/1875 - Loss: 0.1011",
    "Epoch 17/80 - Batch 1300/1875 - Loss: 0.1016",
    "Epoch 17/80 - Batch 1400/1875 - Loss: 0.0484",
    "Epoch 17/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 17/80 - Batch 1600/1875 - Loss: 0.1876",
    "Epoch 17/80 - Batch 1700/1875 - Loss: 0.1574",
    "Epoch 17/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 17 completed - Train Acc: 99.13% - Val Acc: 97.69%",
    "Epoch 18/80 - Batch 0/1875 - Loss: 0.4050",
    "Epoch 18/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 200/1875 - Loss: 0.0025",
    "Epoch 18/80 - Batch 300/1875 - Loss: 0.0358",
    "Epoch 18/80 - Batch 400/1875 - Loss: 0.0408",
    "Epoch 18/80 - Batch 500/1875 - Loss: 0.0004",
    "Epoch 18/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 700/1875 - Loss: 0.1928",
    "Epoch 18/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 18/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 18/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 1100/1875 - Loss: 0.0019",
    "Epoch 18/80 - Batch 1200/1875 - Loss: 0.0164",
    "Epoch 18/80 - Batch 1300/1875 - Loss: 0.1041",
    "Epoch 18/80 - Batch 1400/1875 - Loss: 0.0362",
    "Epoch 18/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 18/80 - Batch 1600/1875 - Loss: 0.0399",
    "Epoch 18/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 18/80 - Batch 1800/1875 - Loss: 0.0500",
    "Epoch 18 completed - Train Acc: 99.02% - Val Acc: 97.75%",
    "Epoch 19/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 19/80 - Batch 100/1875 - Loss: 0.0221",
    "Epoch 19/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 19/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 19/80 - Batch 400/1875 - Loss: 0.0008",
    "Epoch 19/80 - Batch 500/1875 - Loss: 0.0021",
    "Epoch 19/80 - Batch 600/1875 - Loss: 0.2444",
    "Epoch 19/80 - Batch 700/1875 - Loss: 0.0373",
    "Epoch 19/80 - Batch 800/1875 - Loss: 0.0197",
    "Epoch 19/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 19/80 - Batch 1000/1875 - Loss: 0.0503",
    "Epoch 19/80 - Batch 1100/1875 - Loss: 0.0682",
    "Epoch 19/80 - Batch 1200/1875 - Loss: 0.0036",
    "Epoch 19/80 - Batch 1300/1875 - Loss: 0.0145",
    "Epoch 19/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 19/80 - Batch 1500/1875 - Loss: 0.0154",
    "Epoch 19/80 - Batch 1600/1875 - Loss: 0.0007",
    "Epoch 19/80 - Batch 1700/1875 - Loss: 0.1851",
    "Epoch 19/80 - Batch 1800/1875 - Loss: 0.0437",
    "Epoch 19 completed - Train Acc: 99.21% - Val Acc: 97.92%",
    "Epoch 20/80 - Batch 0/1875 - Loss: 0.0473",
    "Epoch 20/80 - Batch 100/1875 - Loss: 0.0655",
    "Epoch 20/80 - Batch 200/1875 - Loss: 0.0096",
    "Epoch 20/80 - Batch 300/1875 - Loss: 0.0243",
    "Epoch 20/80 - Batch 400/1875 - Loss: 0.0605",
    "Epoch 20/80 - Batch 500/1875 - Loss: 0.0103",
    "Epoch 20/80 - Batch 600/1875 - Loss: 0.0060",
    "Epoch 20/80 - Batch 700/1875 - Loss: 0.0075",
    "Epoch 20/80 - Batch 800/1875 - Loss: 0.0069",
    "Epoch 20/80 - Batch 900/1875 - Loss: 0.0006",
    "Epoch 20/80 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 20/80 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 20/80 - Batch 1200/1875 - Loss: 0.0239",
    "Epoch 20/80 - Batch 1300/1875 - Loss: 0.0103",
    "Epoch 20/80 - Batch 1400/1875 - Loss: 0.0517",
    "Epoch 20/80 - Batch 1500/1875 - Loss: 0.0030",
    "Epoch 20/80 - Batch 1600/1875 - Loss: 0.0310",
    "Epoch 20/80 - Batch 1700/1875 - Loss: 0.0023",
    "Epoch 20/80 - Batch 1800/1875 - Loss: 0.1478",
    "Epoch 20 completed - Train Acc: 99.17% - Val Acc: 97.74%",
    "Epoch 21/80 - Batch 0/1875 - Loss: 0.0043",
    "Epoch 21/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 200/1875 - Loss: 0.0529",
    "Epoch 21/80 - Batch 300/1875 - Loss: 0.0400",
    "Epoch 21/80 - Batch 400/1875 - Loss: 0.0735",
    "Epoch 21/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 600/1875 - Loss: 0.0020",
    "Epoch 21/80 - Batch 700/1875 - Loss: 0.0069",
    "Epoch 21/80 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 21/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 21/80 - Batch 1000/1875 - Loss: 0.0012",
    "Epoch 21/80 - Batch 1100/1875 - Loss: 0.0070",
    "Epoch 21/80 - Batch 1200/1875 - Loss: 0.0261",
    "Epoch 21/80 - Batch 1300/1875 - Loss: 0.0040",
    "Epoch 21/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 21/80 - Batch 1500/1875 - Loss: 0.0010",
    "Epoch 21/80 - Batch 1600/1875 - Loss: 0.0840",
    "Epoch 21/80 - Batch 1700/1875 - Loss: 0.0204",
    "Epoch 21/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 21 completed - Train Acc: 99.25% - Val Acc: 97.84%",
    "Epoch 22/80 - Batch 0/1875 - Loss: 0.0059",
    "Epoch 22/80 - Batch 100/1875 - Loss: 0.0994",
    "Epoch 22/80 - Batch 200/1875 - Loss: 0.0641",
    "Epoch 22/80 - Batch 300/1875 - Loss: 0.0014",
    "Epoch 22/80 - Batch 400/1875 - Loss: 0.0102",
    "Epoch 22/80 - Batch 500/1875 - Loss: 0.0015",
    "Epoch 22/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 22/80 - Batch 700/1875 - Loss: 0.0010",
    "Epoch 22/80 - Batch 800/1875 - Loss: 0.0023",
    "Epoch 22/80 - Batch 900/1875 - Loss: 0.0182",
    "Epoch 22/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 22/80 - Batch 1100/1875 - Loss: 0.4827",
    "Epoch 22/80 - Batch 1200/1875 - Loss: 0.0828",
    "Epoch 22/80 - Batch 1300/1875 - Loss: 0.1246",
    "Epoch 22/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 22/80 - Batch 1500/1875 - Loss: 0.0298",
    "Epoch 22/80 - Batch 1600/1875 - Loss: 0.0041",
    "Epoch 22/80 - Batch 1700/1875 - Loss: 0.0379",
    "Epoch 22/80 - Batch 1800/1875 - Loss: 0.0007",
    "Epoch 22 completed - Train Acc: 99.33% - Val Acc: 97.84%",
    "Epoch 23/80 - Batch 0/1875 - Loss: 0.0058",
    "Epoch 23/80 - Batch 100/1875 - Loss: 0.0124",
    "Epoch 23/80 - Batch 200/1875 - Loss: 0.0460",
    "Epoch 23/80 - Batch 300/1875 - Loss: 0.0040",
    "Epoch 23/80 - Batch 400/1875 - Loss: 0.0299",
    "Epoch 23/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 23/80 - Batch 600/1875 - Loss: 0.0031",
    "Epoch 23/80 - Batch 700/1875 - Loss: 0.0182",
    "Epoch 23/80 - Batch 800/1875 - Loss: 0.0069",
    "Epoch 23/80 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 23/80 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 23/80 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 23/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 23/80 - Batch 1300/1875 - Loss: 0.0009",
    "Epoch 23/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 23/80 - Batch 1500/1875 - Loss: 0.0361",
    "Epoch 23/80 - Batch 1600/1875 - Loss: 0.0106",
    "Epoch 23/80 - Batch 1700/1875 - Loss: 0.0016",
    "Epoch 23/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 23 completed - Train Acc: 99.15% - Val Acc: 98.01%",
    "Epoch 24/80 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 24/80 - Batch 100/1875 - Loss: 0.0655",
    "Epoch 24/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 24/80 - Batch 300/1875 - Loss: 0.1074",
    "Epoch 24/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 500/1875 - Loss: 0.0121",
    "Epoch 24/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 24/80 - Batch 700/1875 - Loss: 0.0130",
    "Epoch 24/80 - Batch 800/1875 - Loss: 0.0011",
    "Epoch 24/80 - Batch 900/1875 - Loss: 0.0018",
    "Epoch 24/80 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 24/80 - Batch 1100/1875 - Loss: 0.0045",
    "Epoch 24/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 24/80 - Batch 1400/1875 - Loss: 0.0019",
    "Epoch 24/80 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 24/80 - Batch 1600/1875 - Loss: 0.0935",
    "Epoch 24/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 24/80 - Batch 1800/1875 - Loss: 0.0037",
    "Epoch 24 completed - Train Acc: 99.33% - Val Acc: 97.97%",
    "Epoch 25/80 - Batch 0/1875 - Loss: 0.0211",
    "Epoch 25/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 25/80 - Batch 200/1875 - Loss: 0.0066",
    "Epoch 25/80 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 25/80 - Batch 400/1875 - Loss: 0.0607",
    "Epoch 25/80 - Batch 500/1875 - Loss: 0.0016",
    "Epoch 25/80 - Batch 600/1875 - Loss: 0.0669",
    "Epoch 25/80 - Batch 700/1875 - Loss: 0.0026",
    "Epoch 25/80 - Batch 800/1875 - Loss: 0.0223",
    "Epoch 25/80 - Batch 900/1875 - Loss: 0.0933",
    "Epoch 25/80 - Batch 1000/1875 - Loss: 0.2459",
    "Epoch 25/80 - Batch 1100/1875 - Loss: 0.0220",
    "Epoch 25/80 - Batch 1200/1875 - Loss: 0.0104",
    "Epoch 25/80 - Batch 1300/1875 - Loss: 0.0038",
    "Epoch 25/80 - Batch 1400/1875 - Loss: 0.2668",
    "Epoch 25/80 - Batch 1500/1875 - Loss: 0.0671",
    "Epoch 25/80 - Batch 1600/1875 - Loss: 0.0733",
    "Epoch 25/80 - Batch 1700/1875 - Loss: 0.0110",
    "Epoch 25/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 25 completed - Train Acc: 99.31% - Val Acc: 97.79%",
    "Epoch 26/80 - Batch 0/1875 - Loss: 0.0057",
    "Epoch 26/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 26/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 26/80 - Batch 300/1875 - Loss: 0.0033",
    "Epoch 26/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 26/80 - Batch 500/1875 - Loss: 0.0029",
    "Epoch 26/80 - Batch 600/1875 - Loss: 0.0048",
    "Epoch 26/80 - Batch 700/1875 - Loss: 0.0064",
    "Epoch 26/80 - Batch 800/1875 - Loss: 0.0241",
    "Epoch 26/80 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 26/80 - Batch 1000/1875 - Loss: 0.0053",
    "Epoch 26/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 26/80 - Batch 1200/1875 - Loss: 0.0020",
    "Epoch 26/80 - Batch 1300/1875 - Loss: 0.0153",
    "Epoch 26/80 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 26/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 26/80 - Batch 1600/1875 - Loss: 0.1229",
    "Epoch 26/80 - Batch 1700/1875 - Loss: 0.1377",
    "Epoch 26/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 26 completed - Train Acc: 99.30% - Val Acc: 97.10%",
    "Epoch 27/80 - Batch 0/1875 - Loss: 0.0030",
    "Epoch 27/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 27/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 27/80 - Batch 300/1875 - Loss: 0.0427",
    "Epoch 27/80 - Batch 400/1875 - Loss: 0.0007",
    "Epoch 27/80 - Batch 500/1875 - Loss: 0.0168",
    "Epoch 27/80 - Batch 600/1875 - Loss: 0.1718",
    "Epoch 27/80 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 27/80 - Batch 800/1875 - Loss: 0.0029",
    "Epoch 27/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 27/80 - Batch 1000/1875 - Loss: 0.0094",
    "Epoch 27/80 - Batch 1100/1875 - Loss: 0.0017",
    "Epoch 27/80 - Batch 1200/1875 - Loss: 0.0265",
    "Epoch 27/80 - Batch 1300/1875 - Loss: 0.0030",
    "Epoch 27/80 - Batch 1400/1875 - Loss: 0.0090",
    "Epoch 27/80 - Batch 1500/1875 - Loss: 0.0530",
    "Epoch 27/80 - Batch 1600/1875 - Loss: 0.0412",
    "Epoch 27/80 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 27/80 - Batch 1800/1875 - Loss: 0.0694",
    "Epoch 27 completed - Train Acc: 99.27% - Val Acc: 97.83%",
    "Epoch 28/80 - Batch 0/1875 - Loss: 0.0016",
    "Epoch 28/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 28/80 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 28/80 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 28/80 - Batch 400/1875 - Loss: 0.0009",
    "Epoch 28/80 - Batch 500/1875 - Loss: 0.1065",
    "Epoch 28/80 - Batch 600/1875 - Loss: 0.0027",
    "Epoch 28/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 28/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 28/80 - Batch 900/1875 - Loss: 0.0031",
    "Epoch 28/80 - Batch 1000/1875 - Loss: 0.0066",
    "Epoch 28/80 - Batch 1100/1875 - Loss: 0.0029",
    "Epoch 28/80 - Batch 1200/1875 - Loss: 0.0278",
    "Epoch 28/80 - Batch 1300/1875 - Loss: 0.1249",
    "Epoch 28/80 - Batch 1400/1875 - Loss: 0.0072",
    "Epoch 28/80 - Batch 1500/1875 - Loss: 0.0026",
    "Epoch 28/80 - Batch 1600/1875 - Loss: 0.0472",
    "Epoch 28/80 - Batch 1700/1875 - Loss: 0.0109",
    "Epoch 28/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 28 completed - Train Acc: 99.35% - Val Acc: 97.76%",
    "Epoch 29/80 - Batch 0/1875 - Loss: 0.0008",
    "Epoch 29/80 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 29/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 300/1875 - Loss: 0.0119",
    "Epoch 29/80 - Batch 400/1875 - Loss: 0.0009",
    "Epoch 29/80 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 29/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 29/80 - Batch 800/1875 - Loss: 0.0007",
    "Epoch 29/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 29/80 - Batch 1000/1875 - Loss: 0.0015",
    "Epoch 29/80 - Batch 1100/1875 - Loss: 0.0381",
    "Epoch 29/80 - Batch 1200/1875 - Loss: 0.0158",
    "Epoch 29/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 29/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 1500/1875 - Loss: 0.0312",
    "Epoch 29/80 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 29/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 29/80 - Batch 1800/1875 - Loss: 0.0173",
    "Epoch 29 completed - Train Acc: 99.33% - Val Acc: 97.55%",
    "Epoch 30/80 - Batch 0/1875 - Loss: 0.0028",
    "Epoch 30/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 30/80 - Batch 200/1875 - Loss: 0.0143",
    "Epoch 30/80 - Batch 300/1875 - Loss: 0.0028",
    "Epoch 30/80 - Batch 400/1875 - Loss: 0.0033",
    "Epoch 30/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 30/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 30/80 - Batch 700/1875 - Loss: 0.1800",
    "Epoch 30/80 - Batch 800/1875 - Loss: 0.0132",
    "Epoch 30/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 30/80 - Batch 1000/1875 - Loss: 0.0029",
    "Epoch 30/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 30/80 - Batch 1200/1875 - Loss: 0.6697",
    "Epoch 30/80 - Batch 1300/1875 - Loss: 0.0015",
    "Epoch 30/80 - Batch 1400/1875 - Loss: 0.0085",
    "Epoch 30/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 30/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 30/80 - Batch 1700/1875 - Loss: 0.0091",
    "Epoch 30/80 - Batch 1800/1875 - Loss: 0.0082",
    "Epoch 30 completed - Train Acc: 99.31% - Val Acc: 97.94%",
    "Epoch 31/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 31/80 - Batch 100/1875 - Loss: 0.0116",
    "Epoch 31/80 - Batch 200/1875 - Loss: 0.0264",
    "Epoch 31/80 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 31/80 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 31/80 - Batch 500/1875 - Loss: 0.0195",
    "Epoch 31/80 - Batch 600/1875 - Loss: 0.0111",
    "Epoch 31/80 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 31/80 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 31/80 - Batch 900/1875 - Loss: 0.0184",
    "Epoch 31/80 - Batch 1000/1875 - Loss: 0.0024",
    "Epoch 31/80 - Batch 1100/1875 - Loss: 0.1152",
    "Epoch 31/80 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 31/80 - Batch 1300/1875 - Loss: 0.0079",
    "Epoch 31/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 31/80 - Batch 1500/1875 - Loss: 0.0070",
    "Epoch 31/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 31/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 31/80 - Batch 1800/1875 - Loss: 0.0079",
    "Epoch 31 completed - Train Acc: 99.34% - Val Acc: 97.78%",
    "Epoch 32/80 - Batch 0/1875 - Loss: 0.0196",
    "Epoch 32/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 200/1875 - Loss: 0.0020",
    "Epoch 32/80 - Batch 300/1875 - Loss: 0.0216",
    "Epoch 32/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 32/80 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 32/80 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 32/80 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 32/80 - Batch 800/1875 - Loss: 0.0007",
    "Epoch 32/80 - Batch 900/1875 - Loss: 0.0825",
    "Epoch 32/80 - Batch 1000/1875 - Loss: 0.0206",
    "Epoch 32/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 32/80 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 32/80 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 32/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 32/80 - Batch 1500/1875 - Loss: 0.0003",
    "Epoch 32/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 32/80 - Batch 1700/1875 - Loss: 0.2269",
    "Epoch 32/80 - Batch 1800/1875 - Loss: 0.0366",
    "Epoch 32 completed - Train Acc: 99.42% - Val Acc: 97.96%",
    "Epoch 33/80 - Batch 0/1875 - Loss: 0.0350",
    "Epoch 33/80 - Batch 100/1875 - Loss: 0.0074",
    "Epoch 33/80 - Batch 200/1875 - Loss: 0.0059",
    "Epoch 33/80 - Batch 300/1875 - Loss: 0.0201",
    "Epoch 33/80 - Batch 400/1875 - Loss: 0.0055",
    "Epoch 33/80 - Batch 500/1875 - Loss: 0.0596",
    "Epoch 33/80 - Batch 600/1875 - Loss: 0.0493",
    "Epoch 33/80 - Batch 700/1875 - Loss: 0.0255",
    "Epoch 33/80 - Batch 800/1875 - Loss: 0.0223",
    "Epoch 33/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 33/80 - Batch 1000/1875 - Loss: 0.0205",
    "Epoch 33/80 - Batch 1100/1875 - Loss: 0.0545",
    "Epoch 33/80 - Batch 1200/1875 - Loss: 0.0021",
    "Epoch 33/80 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 33/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 33/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 33/80 - Batch 1600/1875 - Loss: 0.0091",
    "Epoch 33/80 - Batch 1700/1875 - Loss: 0.0036",
    "Epoch 33/80 - Batch 1800/1875 - Loss: 0.0019",
    "Epoch 33 completed - Train Acc: 99.39% - Val Acc: 97.77%",
    "Epoch 34/80 - Batch 0/1875 - Loss: 0.0351",
    "Epoch 34/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 34/80 - Batch 200/1875 - Loss: 0.0012",
    "Epoch 34/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 34/80 - Batch 500/1875 - Loss: 0.2704",
    "Epoch 34/80 - Batch 600/1875 - Loss: 0.9842",
    "Epoch 34/80 - Batch 700/1875 - Loss: 0.0076",
    "Epoch 34/80 - Batch 800/1875 - Loss: 0.0185",
    "Epoch 34/80 - Batch 900/1875 - Loss: 0.0148",
    "Epoch 34/80 - Batch 1000/1875 - Loss: 0.0316",
    "Epoch 34/80 - Batch 1100/1875 - Loss: 0.0022",
    "Epoch 34/80 - Batch 1200/1875 - Loss: 0.0530",
    "Epoch 34/80 - Batch 1300/1875 - Loss: 0.0100",
    "Epoch 34/80 - Batch 1400/1875 - Loss: 0.0513",
    "Epoch 34/80 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 34/80 - Batch 1600/1875 - Loss: 0.0011",
    "Epoch 34/80 - Batch 1700/1875 - Loss: 0.0015",
    "Epoch 34/80 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 34 completed - Train Acc: 99.33% - Val Acc: 98.02%",
    "Epoch 35/80 - Batch 0/1875 - Loss: 0.0190",
    "Epoch 35/80 - Batch 100/1875 - Loss: 0.0058",
    "Epoch 35/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 35/80 - Batch 300/1875 - Loss: 0.0256",
    "Epoch 35/80 - Batch 400/1875 - Loss: 0.0067",
    "Epoch 35/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 35/80 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 35/80 - Batch 700/1875 - Loss: 0.2939",
    "Epoch 35/80 - Batch 800/1875 - Loss: 0.0696",
    "Epoch 35/80 - Batch 900/1875 - Loss: 0.1070",
    "Epoch 35/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 35/80 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 35/80 - Batch 1200/1875 - Loss: 0.0644",
    "Epoch 35/80 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 35/80 - Batch 1400/1875 - Loss: 0.0015",
    "Epoch 35/80 - Batch 1500/1875 - Loss: 0.0064",
    "Epoch 35/80 - Batch 1600/1875 - Loss: 0.0039",
    "Epoch 35/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 35/80 - Batch 1800/1875 - Loss: 0.0537",
    "Epoch 35 completed - Train Acc: 99.36% - Val Acc: 97.72%",
    "Epoch 36/80 - Batch 0/1875 - Loss: 0.0019",
    "Epoch 36/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 36/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 36/80 - Batch 400/1875 - Loss: 0.0361",
    "Epoch 36/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 36/80 - Batch 700/1875 - Loss: 0.3330",
    "Epoch 36/80 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 36/80 - Batch 900/1875 - Loss: 0.0072",
    "Epoch 36/80 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 36/80 - Batch 1100/1875 - Loss: 0.0035",
    "Epoch 36/80 - Batch 1200/1875 - Loss: 0.0010",
    "Epoch 36/80 - Batch 1300/1875 - Loss: 0.0100",
    "Epoch 36/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 36/80 - Batch 1500/1875 - Loss: 0.0040",
    "Epoch 36/80 - Batch 1600/1875 - Loss: 0.0064",
    "Epoch 36/80 - Batch 1700/1875 - Loss: 0.0052",
    "Epoch 36/80 - Batch 1800/1875 - Loss: 0.0031",
    "Epoch 36 completed - Train Acc: 99.45% - Val Acc: 97.79%",
    "Epoch 37/80 - Batch 0/1875 - Loss: 0.0013",
    "Epoch 37/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 37/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 37/80 - Batch 300/1875 - Loss: 0.0086",
    "Epoch 37/80 - Batch 400/1875 - Loss: 0.0035",
    "Epoch 37/80 - Batch 500/1875 - Loss: 0.0014",
    "Epoch 37/80 - Batch 600/1875 - Loss: 0.0497",
    "Epoch 37/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 37/80 - Batch 800/1875 - Loss: 0.0035",
    "Epoch 37/80 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 37/80 - Batch 1000/1875 - Loss: 0.0026",
    "Epoch 37/80 - Batch 1100/1875 - Loss: 0.0008",
    "Epoch 37/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 37/80 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 37/80 - Batch 1400/1875 - Loss: 0.0014",
    "Epoch 37/80 - Batch 1500/1875 - Loss: 0.0183",
    "Epoch 37/80 - Batch 1600/1875 - Loss: 0.0203",
    "Epoch 37/80 - Batch 1700/1875 - Loss: 0.0010",
    "Epoch 37/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 37 completed - Train Acc: 99.37% - Val Acc: 98.00%",
    "Epoch 38/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 38/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 38/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 38/80 - Batch 300/1875 - Loss: 0.0009",
    "Epoch 38/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 38/80 - Batch 500/1875 - Loss: 0.0045",
    "Epoch 38/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 38/80 - Batch 700/1875 - Loss: 0.0058",
    "Epoch 38/80 - Batch 800/1875 - Loss: 0.0125",
    "Epoch 38/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 38/80 - Batch 1000/1875 - Loss: 0.0069",
    "Epoch 38/80 - Batch 1100/1875 - Loss: 0.0018",
    "Epoch 38/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 38/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 38/80 - Batch 1400/1875 - Loss: 0.2462",
    "Epoch 38/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 38/80 - Batch 1600/1875 - Loss: 0.0368",
    "Epoch 38/80 - Batch 1700/1875 - Loss: 0.0055",
    "Epoch 38/80 - Batch 1800/1875 - Loss: 0.0046",
    "Epoch 38 completed - Train Acc: 99.50% - Val Acc: 97.85%",
    "Epoch 39/80 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 39/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 200/1875 - Loss: 0.0998",
    "Epoch 39/80 - Batch 300/1875 - Loss: 0.1535",
    "Epoch 39/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 39/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 39/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 39/80 - Batch 700/1875 - Loss: 0.0020",
    "Epoch 39/80 - Batch 800/1875 - Loss: 0.0119",
    "Epoch 39/80 - Batch 900/1875 - Loss: 0.0886",
    "Epoch 39/80 - Batch 1000/1875 - Loss: 0.0013",
    "Epoch 39/80 - Batch 1100/1875 - Loss: 0.0844",
    "Epoch 39/80 - Batch 1200/1875 - Loss: 0.0004",
    "Epoch 39/80 - Batch 1300/1875 - Loss: 0.0171",
    "Epoch 39/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 39/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 39/80 - Batch 1700/1875 - Loss: 0.0394",
    "Epoch 39/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 39 completed - Train Acc: 99.41% - Val Acc: 97.99%",
    "Epoch 40/80 - Batch 0/1875 - Loss: 0.0047",
    "Epoch 40/80 - Batch 100/1875 - Loss: 0.0861",
    "Epoch 40/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 40/80 - Batch 300/1875 - Loss: 0.0713",
    "Epoch 40/80 - Batch 400/1875 - Loss: 0.1430",
    "Epoch 40/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 40/80 - Batch 600/1875 - Loss: 0.0070",
    "Epoch 40/80 - Batch 700/1875 - Loss: 0.3487",
    "Epoch 40/80 - Batch 800/1875 - Loss: 0.0139",
    "Epoch 40/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 40/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 40/80 - Batch 1100/1875 - Loss: 0.3545",
    "Epoch 40/80 - Batch 1200/1875 - Loss: 0.0045",
    "Epoch 40/80 - Batch 1300/1875 - Loss: 0.0430",
    "Epoch 40/80 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 40/80 - Batch 1500/1875 - Loss: 0.0432",
    "Epoch 40/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 40/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 40/80 - Batch 1800/1875 - Loss: 0.1350",
    "Epoch 40 completed - Train Acc: 99.40% - Val Acc: 97.85%",
    "Epoch 41/80 - Batch 0/1875 - Loss: 0.0005",
    "Epoch 41/80 - Batch 100/1875 - Loss: 0.2193",
    "Epoch 41/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 41/80 - Batch 300/1875 - Loss: 0.0031",
    "Epoch 41/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 41/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 41/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 41/80 - Batch 700/1875 - Loss: 0.0035",
    "Epoch 41/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 41/80 - Batch 900/1875 - Loss: 0.0034",
    "Epoch 41/80 - Batch 1000/1875 - Loss: 0.0275",
    "Epoch 41/80 - Batch 1100/1875 - Loss: 0.0040",
    "Epoch 41/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 41/80 - Batch 1300/1875 - Loss: 0.0240",
    "Epoch 41/80 - Batch 1400/1875 - Loss: 0.0066",
    "Epoch 41/80 - Batch 1500/1875 - Loss: 0.0019",
    "Epoch 41/80 - Batch 1600/1875 - Loss: 0.0399",
    "Epoch 41/80 - Batch 1700/1875 - Loss: 0.0017",
    "Epoch 41/80 - Batch 1800/1875 - Loss: 0.2892",
    "Epoch 41 completed - Train Acc: 99.41% - Val Acc: 97.75%",
    "Epoch 42/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 42/80 - Batch 200/1875 - Loss: 0.0034",
    "Epoch 42/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 400/1875 - Loss: 0.0426",
    "Epoch 42/80 - Batch 500/1875 - Loss: 0.0077",
    "Epoch 42/80 - Batch 600/1875 - Loss: 0.0501",
    "Epoch 42/80 - Batch 700/1875 - Loss: 0.0029",
    "Epoch 42/80 - Batch 800/1875 - Loss: 0.1196",
    "Epoch 42/80 - Batch 900/1875 - Loss: 0.0091",
    "Epoch 42/80 - Batch 1000/1875 - Loss: 0.0021",
    "Epoch 42/80 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 42/80 - Batch 1200/1875 - Loss: 0.1890",
    "Epoch 42/80 - Batch 1300/1875 - Loss: 0.0206",
    "Epoch 42/80 - Batch 1400/1875 - Loss: 0.0135",
    "Epoch 42/80 - Batch 1500/1875 - Loss: 0.0143",
    "Epoch 42/80 - Batch 1600/1875 - Loss: 0.0689",
    "Epoch 42/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 42/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 42 completed - Train Acc: 99.44% - Val Acc: 98.17%",
    "Epoch 43/80 - Batch 0/1875 - Loss: 0.0006",
    "Epoch 43/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 43/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 43/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 43/80 - Batch 400/1875 - Loss: 0.0591",
    "Epoch 43/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 43/80 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 43/80 - Batch 700/1875 - Loss: 0.0012",
    "Epoch 43/80 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 43/80 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 43/80 - Batch 1000/1875 - Loss: 0.0121",
    "Epoch 43/80 - Batch 1100/1875 - Loss: 0.0168",
    "Epoch 43/80 - Batch 1200/1875 - Loss: 0.0354",
    "Epoch 43/80 - Batch 1300/1875 - Loss: 0.1521",
    "Epoch 43/80 - Batch 1400/1875 - Loss: 0.0028",
    "Epoch 43/80 - Batch 1500/1875 - Loss: 0.0034",
    "Epoch 43/80 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 43/80 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 43/80 - Batch 1800/1875 - Loss: 0.0015",
    "Epoch 43 completed - Train Acc: 99.61% - Val Acc: 97.82%",
    "Epoch 44/80 - Batch 0/1875 - Loss: 0.0138",
    "Epoch 44/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 44/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 44/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 44/80 - Batch 800/1875 - Loss: 0.0947",
    "Epoch 44/80 - Batch 900/1875 - Loss: 0.0903",
    "Epoch 44/80 - Batch 1000/1875 - Loss: 0.0685",
    "Epoch 44/80 - Batch 1100/1875 - Loss: 0.0018",
    "Epoch 44/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 1300/1875 - Loss: 0.0400",
    "Epoch 44/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 44/80 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 44/80 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 44/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 44/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 44 completed - Train Acc: 99.45% - Val Acc: 98.25%",
    "Epoch 45/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 45/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 45/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 45/80 - Batch 300/1875 - Loss: 0.0162",
    "Epoch 45/80 - Batch 400/1875 - Loss: 0.1134",
    "Epoch 45/80 - Batch 500/1875 - Loss: 0.3217",
    "Epoch 45/80 - Batch 600/1875 - Loss: 0.0020",
    "Epoch 45/80 - Batch 700/1875 - Loss: 0.3677",
    "Epoch 45/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 45/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 1000/1875 - Loss: 0.0145",
    "Epoch 45/80 - Batch 1100/1875 - Loss: 0.0328",
    "Epoch 45/80 - Batch 1200/1875 - Loss: 0.0377",
    "Epoch 45/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 45/80 - Batch 1400/1875 - Loss: 0.0031",
    "Epoch 45/80 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 45/80 - Batch 1600/1875 - Loss: 0.0990",
    "Epoch 45/80 - Batch 1700/1875 - Loss: 0.0890",
    "Epoch 45/80 - Batch 1800/1875 - Loss: 0.0104",
    "Epoch 45 completed - Train Acc: 99.51% - Val Acc: 98.41%",
    "Epoch 46/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 100/1875 - Loss: 0.2464",
    "Epoch 46/80 - Batch 200/1875 - Loss: 0.0056",
    "Epoch 46/80 - Batch 300/1875 - Loss: 0.7301",
    "Epoch 46/80 - Batch 400/1875 - Loss: 0.0132",
    "Epoch 46/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 700/1875 - Loss: 0.0333",
    "Epoch 46/80 - Batch 800/1875 - Loss: 0.0138",
    "Epoch 46/80 - Batch 900/1875 - Loss: 0.0056",
    "Epoch 46/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 46/80 - Batch 1100/1875 - Loss: 0.0500",
    "Epoch 46/80 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 46/80 - Batch 1300/1875 - Loss: 0.0088",
    "Epoch 46/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 46/80 - Batch 1500/1875 - Loss: 0.0322",
    "Epoch 46/80 - Batch 1600/1875 - Loss: 0.0010",
    "Epoch 46/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 46/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 46 completed - Train Acc: 99.50% - Val Acc: 98.09%",
    "Epoch 47/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 47/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 47/80 - Batch 500/1875 - Loss: 0.1807",
    "Epoch 47/80 - Batch 600/1875 - Loss: 0.0113",
    "Epoch 47/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 47/80 - Batch 800/1875 - Loss: 0.2887",
    "Epoch 47/80 - Batch 900/1875 - Loss: 0.1509",
    "Epoch 47/80 - Batch 1000/1875 - Loss: 0.0194",
    "Epoch 47/80 - Batch 1100/1875 - Loss: 0.0009",
    "Epoch 47/80 - Batch 1200/1875 - Loss: 0.0008",
    "Epoch 47/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 47/80 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 47/80 - Batch 1500/1875 - Loss: 0.0502",
    "Epoch 47/80 - Batch 1600/1875 - Loss: 0.1031",
    "Epoch 47/80 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 47/80 - Batch 1800/1875 - Loss: 0.0024",
    "Epoch 47 completed - Train Acc: 99.47% - Val Acc: 97.72%",
    "Epoch 48/80 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 48/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 200/1875 - Loss: 0.7759",
    "Epoch 48/80 - Batch 300/1875 - Loss: 0.0019",
    "Epoch 48/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 500/1875 - Loss: 0.0434",
    "Epoch 48/80 - Batch 600/1875 - Loss: 0.0012",
    "Epoch 48/80 - Batch 700/1875 - Loss: 0.0029",
    "Epoch 48/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 48/80 - Batch 900/1875 - Loss: 0.0376",
    "Epoch 48/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 48/80 - Batch 1100/1875 - Loss: 0.0007",
    "Epoch 48/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 48/80 - Batch 1300/1875 - Loss: 0.1024",
    "Epoch 48/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 48/80 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 48/80 - Batch 1600/1875 - Loss: 0.0029",
    "Epoch 48/80 - Batch 1700/1875 - Loss: 0.0017",
    "Epoch 48/80 - Batch 1800/1875 - Loss: 0.0029",
    "Epoch 48 completed - Train Acc: 99.48% - Val Acc: 97.98%",
    "Epoch 49/80 - Batch 0/1875 - Loss: 0.0266",
    "Epoch 49/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 49/80 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 49/80 - Batch 300/1875 - Loss: 0.0287",
    "Epoch 49/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 49/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 49/80 - Batch 600/1875 - Loss: 0.0689",
    "Epoch 49/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 49/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 49/80 - Batch 900/1875 - Loss: 0.0157",
    "Epoch 49/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 49/80 - Batch 1100/1875 - Loss: 0.0281",
    "Epoch 49/80 - Batch 1200/1875 - Loss: 0.0015",
    "Epoch 49/80 - Batch 1300/1875 - Loss: 0.0041",
    "Epoch 49/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 49/80 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 49/80 - Batch 1600/1875 - Loss: 0.0108",
    "Epoch 49/80 - Batch 1700/1875 - Loss: 0.0239",
    "Epoch 49/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 49 completed - Train Acc: 99.50% - Val Acc: 98.03%",
    "Epoch 50/80 - Batch 0/1875 - Loss: 0.0032",
    "Epoch 50/80 - Batch 100/1875 - Loss: 0.0052",
    "Epoch 50/80 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 50/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 50/80 - Batch 400/1875 - Loss: 0.0078",
    "Epoch 50/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 50/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 50/80 - Batch 800/1875 - Loss: 0.0212",
    "Epoch 50/80 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 50/80 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 50/80 - Batch 1100/1875 - Loss: 0.0231",
    "Epoch 50/80 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 50/80 - Batch 1300/1875 - Loss: 0.0073",
    "Epoch 50/80 - Batch 1400/1875 - Loss: 0.0207",
    "Epoch 50/80 - Batch 1500/1875 - Loss: 0.0584",
    "Epoch 50/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 50/80 - Batch 1700/1875 - Loss: 0.0088",
    "Epoch 50/80 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 50 completed - Train Acc: 99.47% - Val Acc: 97.79%",
    "Epoch 51/80 - Batch 0/1875 - Loss: 0.0091",
    "Epoch 51/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 51/80 - Batch 200/1875 - Loss: 0.0057",
    "Epoch 51/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 51/80 - Batch 500/1875 - Loss: 0.0035",
    "Epoch 51/80 - Batch 600/1875 - Loss: 0.0221",
    "Epoch 51/80 - Batch 700/1875 - Loss: 0.1048",
    "Epoch 51/80 - Batch 800/1875 - Loss: 0.0114",
    "Epoch 51/80 - Batch 900/1875 - Loss: 0.0182",
    "Epoch 51/80 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 51/80 - Batch 1100/1875 - Loss: 0.0876",
    "Epoch 51/80 - Batch 1200/1875 - Loss: 0.0049",
    "Epoch 51/80 - Batch 1300/1875 - Loss: 0.0041",
    "Epoch 51/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 51/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 51/80 - Batch 1600/1875 - Loss: 0.0182",
    "Epoch 51/80 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 51/80 - Batch 1800/1875 - Loss: 0.0011",
    "Epoch 51 completed - Train Acc: 99.60% - Val Acc: 97.70%",
    "Epoch 52/80 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 52/80 - Batch 200/1875 - Loss: 0.0065",
    "Epoch 52/80 - Batch 300/1875 - Loss: 0.1740",
    "Epoch 52/80 - Batch 400/1875 - Loss: 0.0625",
    "Epoch 52/80 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 52/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 52/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 800/1875 - Loss: 0.0010",
    "Epoch 52/80 - Batch 900/1875 - Loss: 0.1244",
    "Epoch 52/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 52/80 - Batch 1100/1875 - Loss: 0.0022",
    "Epoch 52/80 - Batch 1200/1875 - Loss: 0.0845",
    "Epoch 52/80 - Batch 1300/1875 - Loss: 0.3081",
    "Epoch 52/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 52/80 - Batch 1500/1875 - Loss: 0.0141",
    "Epoch 52/80 - Batch 1600/1875 - Loss: 0.0029",
    "Epoch 52/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 52/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 52 completed - Train Acc: 99.43% - Val Acc: 98.12%",
    "Epoch 53/80 - Batch 0/1875 - Loss: 0.0501",
    "Epoch 53/80 - Batch 100/1875 - Loss: 0.0012",
    "Epoch 53/80 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 53/80 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 53/80 - Batch 400/1875 - Loss: 0.0381",
    "Epoch 53/80 - Batch 500/1875 - Loss: 0.0379",
    "Epoch 53/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 53/80 - Batch 800/1875 - Loss: 0.0722",
    "Epoch 53/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 1000/1875 - Loss: 0.0431",
    "Epoch 53/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 1200/1875 - Loss: 0.0029",
    "Epoch 53/80 - Batch 1300/1875 - Loss: 0.0084",
    "Epoch 53/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 53/80 - Batch 1500/1875 - Loss: 0.0025",
    "Epoch 53/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 53/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 53/80 - Batch 1800/1875 - Loss: 0.0169",
    "Epoch 53 completed - Train Acc: 99.53% - Val Acc: 97.70%",
    "Epoch 54/80 - Batch 0/1875 - Loss: 0.0075",
    "Epoch 54/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 200/1875 - Loss: 0.0019",
    "Epoch 54/80 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 54/80 - Batch 400/1875 - Loss: 0.1012",
    "Epoch 54/80 - Batch 500/1875 - Loss: 0.0439",
    "Epoch 54/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 54/80 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 54/80 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 54/80 - Batch 900/1875 - Loss: 0.0011",
    "Epoch 54/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 54/80 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 54/80 - Batch 1200/1875 - Loss: 0.0017",
    "Epoch 54/80 - Batch 1300/1875 - Loss: 0.0501",
    "Epoch 54/80 - Batch 1400/1875 - Loss: 0.0022",
    "Epoch 54/80 - Batch 1500/1875 - Loss: 0.0043",
    "Epoch 54/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 54/80 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 54/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 54 completed - Train Acc: 99.55% - Val Acc: 98.25%",
    "Epoch 55/80 - Batch 0/1875 - Loss: 0.0018",
    "Epoch 55/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 200/1875 - Loss: 0.0357",
    "Epoch 55/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 55/80 - Batch 500/1875 - Loss: 0.0016",
    "Epoch 55/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 700/1875 - Loss: 0.3878",
    "Epoch 55/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 55/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 1100/1875 - Loss: 0.0025",
    "Epoch 55/80 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 55/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 55/80 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 55/80 - Batch 1500/1875 - Loss: 0.0037",
    "Epoch 55/80 - Batch 1600/1875 - Loss: 0.0444",
    "Epoch 55/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 55/80 - Batch 1800/1875 - Loss: 0.1364",
    "Epoch 55 completed - Train Acc: 99.44% - Val Acc: 98.00%",
    "Epoch 56/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 56/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 56/80 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 56/80 - Batch 300/1875 - Loss: 0.1120",
    "Epoch 56/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 600/1875 - Loss: 0.0041",
    "Epoch 56/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 56/80 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 56/80 - Batch 900/1875 - Loss: 0.0124",
    "Epoch 56/80 - Batch 1000/1875 - Loss: 0.0026",
    "Epoch 56/80 - Batch 1100/1875 - Loss: 0.0431",
    "Epoch 56/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 56/80 - Batch 1400/1875 - Loss: 0.0019",
    "Epoch 56/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 56/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 56/80 - Batch 1700/1875 - Loss: 0.0044",
    "Epoch 56/80 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 56 completed - Train Acc: 99.52% - Val Acc: 98.20%",
    "Epoch 57/80 - Batch 0/1875 - Loss: 0.0021",
    "Epoch 57/80 - Batch 100/1875 - Loss: 0.0172",
    "Epoch 57/80 - Batch 200/1875 - Loss: 0.0610",
    "Epoch 57/80 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 57/80 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 57/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 57/80 - Batch 600/1875 - Loss: 0.0383",
    "Epoch 57/80 - Batch 700/1875 - Loss: 0.0290",
    "Epoch 57/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 57/80 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 57/80 - Batch 1000/1875 - Loss: 0.0284",
    "Epoch 57/80 - Batch 1100/1875 - Loss: 0.0009",
    "Epoch 57/80 - Batch 1200/1875 - Loss: 0.0020",
    "Epoch 57/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 57/80 - Batch 1400/1875 - Loss: 0.1862",
    "Epoch 57/80 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 57/80 - Batch 1600/1875 - Loss: 0.0383",
    "Epoch 57/80 - Batch 1700/1875 - Loss: 0.1364",
    "Epoch 57/80 - Batch 1800/1875 - Loss: 0.2575",
    "Epoch 57 completed - Train Acc: 99.50% - Val Acc: 97.58%",
    "Epoch 58/80 - Batch 0/1875 - Loss: 0.0105",
    "Epoch 58/80 - Batch 100/1875 - Loss: 0.0021",
    "Epoch 58/80 - Batch 200/1875 - Loss: 0.2674",
    "Epoch 58/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 58/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 58/80 - Batch 500/1875 - Loss: 0.0718",
    "Epoch 58/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 58/80 - Batch 700/1875 - Loss: 0.0069",
    "Epoch 58/80 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 58/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 58/80 - Batch 1000/1875 - Loss: 0.0025",
    "Epoch 58/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 58/80 - Batch 1200/1875 - Loss: 0.0031",
    "Epoch 58/80 - Batch 1300/1875 - Loss: 0.0800",
    "Epoch 58/80 - Batch 1400/1875 - Loss: 0.0180",
    "Epoch 58/80 - Batch 1500/1875 - Loss: 0.0023",
    "Epoch 58/80 - Batch 1600/1875 - Loss: 0.0010",
    "Epoch 58/80 - Batch 1700/1875 - Loss: 0.0064",
    "Epoch 58/80 - Batch 1800/1875 - Loss: 0.0035",
    "Epoch 58 completed - Train Acc: 99.48% - Val Acc: 97.90%",
    "Epoch 59/80 - Batch 0/1875 - Loss: 0.0378",
    "Epoch 59/80 - Batch 100/1875 - Loss: 0.0171",
    "Epoch 59/80 - Batch 200/1875 - Loss: 0.0010",
    "Epoch 59/80 - Batch 300/1875 - Loss: 0.0006",
    "Epoch 59/80 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 59/80 - Batch 500/1875 - Loss: 0.0015",
    "Epoch 59/80 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 59/80 - Batch 700/1875 - Loss: 0.0015",
    "Epoch 59/80 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 59/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 59/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 59/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 1400/1875 - Loss: 0.0392",
    "Epoch 59/80 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 59/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 59/80 - Batch 1700/1875 - Loss: 0.0049",
    "Epoch 59/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 59 completed - Train Acc: 99.56% - Val Acc: 98.12%",
    "Epoch 60/80 - Batch 0/1875 - Loss: 0.0070",
    "Epoch 60/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 60/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 300/1875 - Loss: 0.0016",
    "Epoch 60/80 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 60/80 - Batch 500/1875 - Loss: 0.0006",
    "Epoch 60/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 800/1875 - Loss: 0.0282",
    "Epoch 60/80 - Batch 900/1875 - Loss: 0.0023",
    "Epoch 60/80 - Batch 1000/1875 - Loss: 0.0022",
    "Epoch 60/80 - Batch 1100/1875 - Loss: 0.1222",
    "Epoch 60/80 - Batch 1200/1875 - Loss: 0.0031",
    "Epoch 60/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 60/80 - Batch 1400/1875 - Loss: 0.0347",
    "Epoch 60/80 - Batch 1500/1875 - Loss: 0.0028",
    "Epoch 60/80 - Batch 1600/1875 - Loss: 0.1420",
    "Epoch 60/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 60/80 - Batch 1800/1875 - Loss: 0.1746",
    "Epoch 60 completed - Train Acc: 99.50% - Val Acc: 98.09%",
    "Epoch 61/80 - Batch 0/1875 - Loss: 0.1628",
    "Epoch 61/80 - Batch 100/1875 - Loss: 0.0929",
    "Epoch 61/80 - Batch 200/1875 - Loss: 0.0064",
    "Epoch 61/80 - Batch 300/1875 - Loss: 0.0166",
    "Epoch 61/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 61/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 61/80 - Batch 700/1875 - Loss: 0.0026",
    "Epoch 61/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 61/80 - Batch 1000/1875 - Loss: 0.1869",
    "Epoch 61/80 - Batch 1100/1875 - Loss: 0.0451",
    "Epoch 61/80 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 61/80 - Batch 1300/1875 - Loss: 0.0021",
    "Epoch 61/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 1500/1875 - Loss: 0.0020",
    "Epoch 61/80 - Batch 1600/1875 - Loss: 0.0051",
    "Epoch 61/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 61/80 - Batch 1800/1875 - Loss: 0.0304",
    "Epoch 61 completed - Train Acc: 99.55% - Val Acc: 97.90%",
    "Epoch 62/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 100/1875 - Loss: 0.3235",
    "Epoch 62/80 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 62/80 - Batch 300/1875 - Loss: 0.0348",
    "Epoch 62/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 62/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 600/1875 - Loss: 0.0028",
    "Epoch 62/80 - Batch 700/1875 - Loss: 0.0038",
    "Epoch 62/80 - Batch 800/1875 - Loss: 0.1077",
    "Epoch 62/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 62/80 - Batch 1000/1875 - Loss: 0.1317",
    "Epoch 62/80 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 62/80 - Batch 1200/1875 - Loss: 0.1239",
    "Epoch 62/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 62/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 62/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 62/80 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 62/80 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 62/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 62 completed - Train Acc: 99.57% - Val Acc: 97.63%",
    "Epoch 63/80 - Batch 0/1875 - Loss: 0.0090",
    "Epoch 63/80 - Batch 100/1875 - Loss: 0.1571",
    "Epoch 63/80 - Batch 200/1875 - Loss: 0.0040",
    "Epoch 63/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 63/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 63/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 63/80 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 63/80 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 63/80 - Batch 900/1875 - Loss: 0.0136",
    "Epoch 63/80 - Batch 1000/1875 - Loss: 0.1050",
    "Epoch 63/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 63/80 - Batch 1400/1875 - Loss: 0.5113",
    "Epoch 63/80 - Batch 1500/1875 - Loss: 0.0031",
    "Epoch 63/80 - Batch 1600/1875 - Loss: 0.0050",
    "Epoch 63/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 63/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 63 completed - Train Acc: 99.42% - Val Acc: 98.22%",
    "Epoch 64/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 64/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 200/1875 - Loss: 0.1812",
    "Epoch 64/80 - Batch 300/1875 - Loss: 0.0337",
    "Epoch 64/80 - Batch 400/1875 - Loss: 0.0051",
    "Epoch 64/80 - Batch 500/1875 - Loss: 0.2155",
    "Epoch 64/80 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 64/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 800/1875 - Loss: 0.0010",
    "Epoch 64/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 64/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 1300/1875 - Loss: 0.0008",
    "Epoch 64/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 64/80 - Batch 1500/1875 - Loss: 0.0730",
    "Epoch 64/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 64/80 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 64/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 64 completed - Train Acc: 99.56% - Val Acc: 97.74%",
    "Epoch 65/80 - Batch 0/1875 - Loss: 0.0024",
    "Epoch 65/80 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 65/80 - Batch 200/1875 - Loss: 0.0008",
    "Epoch 65/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 400/1875 - Loss: 0.0207",
    "Epoch 65/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 65/80 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 65/80 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 65/80 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 65/80 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 65/80 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 65/80 - Batch 1100/1875 - Loss: 0.0336",
    "Epoch 65/80 - Batch 1200/1875 - Loss: 0.0013",
    "Epoch 65/80 - Batch 1300/1875 - Loss: 0.0028",
    "Epoch 65/80 - Batch 1400/1875 - Loss: 0.0005",
    "Epoch 65/80 - Batch 1500/1875 - Loss: 0.0126",
    "Epoch 65/80 - Batch 1600/1875 - Loss: 0.0171",
    "Epoch 65/80 - Batch 1700/1875 - Loss: 0.3718",
    "Epoch 65/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 65 completed - Train Acc: 99.58% - Val Acc: 97.81%",
    "Epoch 66/80 - Batch 0/1875 - Loss: 0.0069",
    "Epoch 66/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 66/80 - Batch 200/1875 - Loss: 0.0138",
    "Epoch 66/80 - Batch 300/1875 - Loss: 0.0095",
    "Epoch 66/80 - Batch 400/1875 - Loss: 0.0055",
    "Epoch 66/80 - Batch 500/1875 - Loss: 0.0018",
    "Epoch 66/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 700/1875 - Loss: 0.0068",
    "Epoch 66/80 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 66/80 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 66/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 66/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 66/80 - Batch 1200/1875 - Loss: 0.0874",
    "Epoch 66/80 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 66/80 - Batch 1400/1875 - Loss: 0.1870",
    "Epoch 66/80 - Batch 1500/1875 - Loss: 0.0019",
    "Epoch 66/80 - Batch 1600/1875 - Loss: 0.0012",
    "Epoch 66/80 - Batch 1700/1875 - Loss: 0.1834",
    "Epoch 66/80 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 66 completed - Train Acc: 99.54% - Val Acc: 98.12%",
    "Epoch 67/80 - Batch 0/1875 - Loss: 0.0042",
    "Epoch 67/80 - Batch 100/1875 - Loss: 0.0032",
    "Epoch 67/80 - Batch 200/1875 - Loss: 0.0019",
    "Epoch 67/80 - Batch 300/1875 - Loss: 0.0023",
    "Epoch 67/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 700/1875 - Loss: 0.0080",
    "Epoch 67/80 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 67/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 67/80 - Batch 1100/1875 - Loss: 0.0071",
    "Epoch 67/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 1300/1875 - Loss: 0.0004",
    "Epoch 67/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 67/80 - Batch 1600/1875 - Loss: 0.0183",
    "Epoch 67/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 67/80 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 67 completed - Train Acc: 99.59% - Val Acc: 97.85%",
    "Epoch 68/80 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 68/80 - Batch 100/1875 - Loss: 0.0024",
    "Epoch 68/80 - Batch 200/1875 - Loss: 0.0029",
    "Epoch 68/80 - Batch 300/1875 - Loss: 0.0566",
    "Epoch 68/80 - Batch 400/1875 - Loss: 0.0083",
    "Epoch 68/80 - Batch 500/1875 - Loss: 0.0013",
    "Epoch 68/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 68/80 - Batch 700/1875 - Loss: 0.0003",
    "Epoch 68/80 - Batch 800/1875 - Loss: 0.0069",
    "Epoch 68/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 68/80 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 68/80 - Batch 1100/1875 - Loss: 0.0007",
    "Epoch 68/80 - Batch 1200/1875 - Loss: 0.0331",
    "Epoch 68/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 68/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 68/80 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 68/80 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 68/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 68/80 - Batch 1800/1875 - Loss: 0.0517",
    "Epoch 68 completed - Train Acc: 99.53% - Val Acc: 97.56%",
    "Epoch 69/80 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 69/80 - Batch 100/1875 - Loss: 0.1381",
    "Epoch 69/80 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 300/1875 - Loss: 0.1668",
    "Epoch 69/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 69/80 - Batch 500/1875 - Loss: 0.0014",
    "Epoch 69/80 - Batch 600/1875 - Loss: 0.1863",
    "Epoch 69/80 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 69/80 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 69/80 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 69/80 - Batch 1000/1875 - Loss: 0.0044",
    "Epoch 69/80 - Batch 1100/1875 - Loss: 0.0193",
    "Epoch 69/80 - Batch 1200/1875 - Loss: 0.1134",
    "Epoch 69/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 69/80 - Batch 1500/1875 - Loss: 0.0016",
    "Epoch 69/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 69/80 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 69/80 - Batch 1800/1875 - Loss: 0.0037",
    "Epoch 69 completed - Train Acc: 99.52% - Val Acc: 97.57%",
    "Epoch 70/80 - Batch 0/1875 - Loss: 0.2191",
    "Epoch 70/80 - Batch 100/1875 - Loss: 0.0004",
    "Epoch 70/80 - Batch 200/1875 - Loss: 0.0020",
    "Epoch 70/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 70/80 - Batch 400/1875 - Loss: 0.0013",
    "Epoch 70/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 70/80 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 70/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 70/80 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 70/80 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 70/80 - Batch 1000/1875 - Loss: 0.0057",
    "Epoch 70/80 - Batch 1100/1875 - Loss: 0.0239",
    "Epoch 70/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 70/80 - Batch 1300/1875 - Loss: 0.1085",
    "Epoch 70/80 - Batch 1400/1875 - Loss: 0.0073",
    "Epoch 70/80 - Batch 1500/1875 - Loss: 0.3631",
    "Epoch 70/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 70/80 - Batch 1700/1875 - Loss: 0.1153",
    "Epoch 70/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 70 completed - Train Acc: 99.60% - Val Acc: 97.21%",
    "Epoch 71/80 - Batch 0/1875 - Loss: 0.0156",
    "Epoch 71/80 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 71/80 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 71/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 71/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 71/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 71/80 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 71/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 71/80 - Batch 1000/1875 - Loss: 0.0086",
    "Epoch 71/80 - Batch 1100/1875 - Loss: 0.0073",
    "Epoch 71/80 - Batch 1200/1875 - Loss: 0.0612",
    "Epoch 71/80 - Batch 1300/1875 - Loss: 0.0006",
    "Epoch 71/80 - Batch 1400/1875 - Loss: 0.0223",
    "Epoch 71/80 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 71/80 - Batch 1600/1875 - Loss: 0.0085",
    "Epoch 71/80 - Batch 1700/1875 - Loss: 0.0618",
    "Epoch 71/80 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 71 completed - Train Acc: 99.50% - Val Acc: 98.09%",
    "Epoch 72/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 600/1875 - Loss: 0.0415",
    "Epoch 72/80 - Batch 700/1875 - Loss: 0.0013",
    "Epoch 72/80 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 72/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 72/80 - Batch 1100/1875 - Loss: 0.0030",
    "Epoch 72/80 - Batch 1200/1875 - Loss: 0.0005",
    "Epoch 72/80 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 72/80 - Batch 1400/1875 - Loss: 0.0470",
    "Epoch 72/80 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 72/80 - Batch 1600/1875 - Loss: 0.0085",
    "Epoch 72/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 72/80 - Batch 1800/1875 - Loss: 0.0006",
    "Epoch 72 completed - Train Acc: 99.57% - Val Acc: 97.61%",
    "Epoch 73/80 - Batch 0/1875 - Loss: 0.0323",
    "Epoch 73/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 400/1875 - Loss: 0.0138",
    "Epoch 73/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 73/80 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 73/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 1000/1875 - Loss: 0.0130",
    "Epoch 73/80 - Batch 1100/1875 - Loss: 0.0013",
    "Epoch 73/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 73/80 - Batch 1300/1875 - Loss: 0.0014",
    "Epoch 73/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 73/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 73/80 - Batch 1600/1875 - Loss: 0.0019",
    "Epoch 73/80 - Batch 1700/1875 - Loss: 0.2566",
    "Epoch 73/80 - Batch 1800/1875 - Loss: 0.0099",
    "Epoch 73 completed - Train Acc: 99.49% - Val Acc: 98.01%",
    "Epoch 74/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 74/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 200/1875 - Loss: 0.0007",
    "Epoch 74/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 500/1875 - Loss: 0.0141",
    "Epoch 74/80 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 74/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 800/1875 - Loss: 0.0081",
    "Epoch 74/80 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 1000/1875 - Loss: 0.3509",
    "Epoch 74/80 - Batch 1100/1875 - Loss: 0.0013",
    "Epoch 74/80 - Batch 1200/1875 - Loss: 0.1091",
    "Epoch 74/80 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 74/80 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 74/80 - Batch 1500/1875 - Loss: 0.0064",
    "Epoch 74/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 74/80 - Batch 1700/1875 - Loss: 0.0085",
    "Epoch 74/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 74 completed - Train Acc: 99.63% - Val Acc: 97.97%",
    "Epoch 75/80 - Batch 0/1875 - Loss: 0.0012",
    "Epoch 75/80 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 75/80 - Batch 200/1875 - Loss: 0.0068",
    "Epoch 75/80 - Batch 300/1875 - Loss: 0.0165",
    "Epoch 75/80 - Batch 400/1875 - Loss: 0.0032",
    "Epoch 75/80 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 75/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 800/1875 - Loss: 0.0027",
    "Epoch 75/80 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 75/80 - Batch 1000/1875 - Loss: 0.0031",
    "Epoch 75/80 - Batch 1100/1875 - Loss: 0.1875",
    "Epoch 75/80 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 75/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 75/80 - Batch 1400/1875 - Loss: 0.0251",
    "Epoch 75/80 - Batch 1500/1875 - Loss: 0.1324",
    "Epoch 75/80 - Batch 1600/1875 - Loss: 0.0039",
    "Epoch 75/80 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 75/80 - Batch 1800/1875 - Loss: 0.1033",
    "Epoch 75 completed - Train Acc: 99.52% - Val Acc: 98.10%",
    "Epoch 76/80 - Batch 0/1875 - Loss: 0.0367",
    "Epoch 76/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 76/80 - Batch 300/1875 - Loss: 0.0866",
    "Epoch 76/80 - Batch 400/1875 - Loss: 0.0011",
    "Epoch 76/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 700/1875 - Loss: 0.0017",
    "Epoch 76/80 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 900/1875 - Loss: 0.0013",
    "Epoch 76/80 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 76/80 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 76/80 - Batch 1300/1875 - Loss: 0.2155",
    "Epoch 76/80 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 76/80 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 76/80 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 76/80 - Batch 1700/1875 - Loss: 0.3000",
    "Epoch 76/80 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 76 completed - Train Acc: 99.64% - Val Acc: 97.84%",
    "Epoch 77/80 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 77/80 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 300/1875 - Loss: 0.0478",
    "Epoch 77/80 - Batch 400/1875 - Loss: 0.0025",
    "Epoch 77/80 - Batch 500/1875 - Loss: 0.2296",
    "Epoch 77/80 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 77/80 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 77/80 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 77/80 - Batch 900/1875 - Loss: 0.6105",
    "Epoch 77/80 - Batch 1000/1875 - Loss: 0.0027",
    "Epoch 77/80 - Batch 1100/1875 - Loss: 0.0003",
    "Epoch 77/80 - Batch 1200/1875 - Loss: 0.1248",
    "Epoch 77/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1500/1875 - Loss: 0.0068",
    "Epoch 77/80 - Batch 1600/1875 - Loss: 0.0539",
    "Epoch 77/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 77/80 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 77 completed - Train Acc: 99.47% - Val Acc: 97.80%",
    "Epoch 78/80 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 78/80 - Batch 100/1875 - Loss: 0.0197",
    "Epoch 78/80 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 78/80 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 400/1875 - Loss: 0.8666",
    "Epoch 78/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 600/1875 - Loss: 0.0145",
    "Epoch 78/80 - Batch 700/1875 - Loss: 0.1549",
    "Epoch 78/80 - Batch 800/1875 - Loss: 0.0052",
    "Epoch 78/80 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 78/80 - Batch 1000/1875 - Loss: 0.0043",
    "Epoch 78/80 - Batch 1100/1875 - Loss: 0.2297",
    "Epoch 78/80 - Batch 1200/1875 - Loss: 0.0004",
    "Epoch 78/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1400/1875 - Loss: 0.0018",
    "Epoch 78/80 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 78/80 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 78/80 - Batch 1800/1875 - Loss: 0.0010",
    "Epoch 78 completed - Train Acc: 99.47% - Val Acc: 98.04%",
    "Epoch 79/80 - Batch 0/1875 - Loss: 0.0008",
    "Epoch 79/80 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 79/80 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 79/80 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 400/1875 - Loss: 0.0824",
    "Epoch 79/80 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 79/80 - Batch 700/1875 - Loss: 0.0133",
    "Epoch 79/80 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 79/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 1000/1875 - Loss: 0.0168",
    "Epoch 79/80 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 79/80 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1500/1875 - Loss: 0.0012",
    "Epoch 79/80 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 79/80 - Batch 1700/1875 - Loss: 0.0435",
    "Epoch 79/80 - Batch 1800/1875 - Loss: 0.0129",
    "Epoch 79 completed - Train Acc: 99.59% - Val Acc: 98.04%",
    "Epoch 80/80 - Batch 0/1875 - Loss: 0.0643",
    "Epoch 80/80 - Batch 100/1875 - Loss: 0.0002",
    "Epoch 80/80 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 80/80 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 500/1875 - Loss: 0.0034",
    "Epoch 80/80 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 80/80 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 80/80 - Batch 1000/1875 - Loss: 0.0011",
    "Epoch 80/80 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 1200/1875 - Loss: 0.0010",
    "Epoch 80/80 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 80/80 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 80/80 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 80/80 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 80/80 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 80/80 - Batch 1800/1875 - Loss: 0.0025",
    "Epoch 80 completed - Train Acc: 99.58% - Val Acc: 98.23%",
    "Training completed. Model saved to models/ZPE-Colab-Sim_hnn_step8.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.2844203330697492,
      "train_accuracy": 91.635,
      "val_loss": 0.18888002013179142,
      "val_accuracy": 95.31
    },
    {
      "epoch": 2,
      "train_loss": 0.13967585807203625,
      "train_accuracy": 96.245,
      "val_loss": 0.1470590927747834,
      "val_accuracy": 95.97
    },
    {
      "epoch": 3,
      "train_loss": 0.10797806095442114,
      "train_accuracy": 97.13666666666667,
      "val_loss": 0.10328982141224889,
      "val_accuracy": 97.31
    },
    {
      "epoch": 4,
      "train_loss": 0.09068160723787733,
      "train_accuracy": 97.52166666666666,
      "val_loss": 0.09719215338727533,
      "val_accuracy": 97.39
    },
    {
      "epoch": 5,
      "train_loss": 0.0748819048046212,
      "train_accuracy": 97.88333333333334,
      "val_loss": 0.11295904785701236,
      "val_accuracy": 97.03
    },
    {
      "epoch": 6,
      "train_loss": 0.0681356853869666,
      "train_accuracy": 98.15833333333333,
      "val_loss": 0.11086178955412279,
      "val_accuracy": 97.66
    },
    {
      "epoch": 7,
      "train_loss": 0.06245112754818838,
      "train_accuracy": 98.35666666666667,
      "val_loss": 0.09103790534864899,
      "val_accuracy": 97.78
    },
    {
      "epoch": 8,
      "train_loss": 0.05346432847406007,
      "train_accuracy": 98.52833333333334,
      "val_loss": 0.09054173318998854,
      "val_accuracy": 97.83
    },
    {
      "epoch": 9,
      "train_loss": 0.05018459848964109,
      "train_accuracy": 98.63,
      "val_loss": 0.08377636356342587,
      "val_accuracy": 97.97
    },
    {
      "epoch": 10,
      "train_loss": 0.04791840152644957,
      "train_accuracy": 98.71166666666667,
      "val_loss": 0.12892070981525486,
      "val_accuracy": 97.67
    },
    {
      "epoch": 11,
      "train_loss": 0.04667734310365546,
      "train_accuracy": 98.71166666666667,
      "val_loss": 0.10677847098571375,
      "val_accuracy": 97.68
    },
    {
      "epoch": 12,
      "train_loss": 0.03853220396145425,
      "train_accuracy": 98.915,
      "val_loss": 0.14154374664222966,
      "val_accuracy": 97.24
    },
    {
      "epoch": 13,
      "train_loss": 0.04049644621050888,
      "train_accuracy": 98.94166666666666,
      "val_loss": 0.10684542282266388,
      "val_accuracy": 97.44
    },
    {
      "epoch": 14,
      "train_loss": 0.039555059555817056,
      "train_accuracy": 98.935,
      "val_loss": 0.13737953350451004,
      "val_accuracy": 97.51
    },
    {
      "epoch": 15,
      "train_loss": 0.03599878504562069,
      "train_accuracy": 99.045,
      "val_loss": 0.11539103196688932,
      "val_accuracy": 97.78
    },
    {
      "epoch": 16,
      "train_loss": 0.03269859153231919,
      "train_accuracy": 99.09,
      "val_loss": 0.12251121329420214,
      "val_accuracy": 97.65
    },
    {
      "epoch": 17,
      "train_loss": 0.033144963189990204,
      "train_accuracy": 99.13,
      "val_loss": 0.12945275635347953,
      "val_accuracy": 97.69
    },
    {
      "epoch": 18,
      "train_loss": 0.037890513748026634,
      "train_accuracy": 99.015,
      "val_loss": 0.12038843431505294,
      "val_accuracy": 97.75
    },
    {
      "epoch": 19,
      "train_loss": 0.032116614194450395,
      "train_accuracy": 99.20833333333333,
      "val_loss": 0.12608437724916716,
      "val_accuracy": 97.92
    },
    {
      "epoch": 20,
      "train_loss": 0.032853664696697975,
      "train_accuracy": 99.17333333333333,
      "val_loss": 0.11352930159337019,
      "val_accuracy": 97.74
    },
    {
      "epoch": 21,
      "train_loss": 0.03091272959645435,
      "train_accuracy": 99.245,
      "val_loss": 0.11959302354003508,
      "val_accuracy": 97.84
    },
    {
      "epoch": 22,
      "train_loss": 0.026681153462048808,
      "train_accuracy": 99.33333333333333,
      "val_loss": 0.10903870525121205,
      "val_accuracy": 97.84
    },
    {
      "epoch": 23,
      "train_loss": 0.03146512497157113,
      "train_accuracy": 99.14833333333333,
      "val_loss": 0.11911264021139772,
      "val_accuracy": 98.01
    },
    {
      "epoch": 24,
      "train_loss": 0.027279674613640403,
      "train_accuracy": 99.32666666666667,
      "val_loss": 0.11211771812157066,
      "val_accuracy": 97.97
    },
    {
      "epoch": 25,
      "train_loss": 0.030279568018545092,
      "train_accuracy": 99.30833333333334,
      "val_loss": 0.1379840767424217,
      "val_accuracy": 97.79
    },
    {
      "epoch": 26,
      "train_loss": 0.02976159391018622,
      "train_accuracy": 99.30333333333333,
      "val_loss": 0.21767485164756284,
      "val_accuracy": 97.1
    },
    {
      "epoch": 27,
      "train_loss": 0.030889710582925244,
      "train_accuracy": 99.265,
      "val_loss": 0.12589065215853154,
      "val_accuracy": 97.83
    },
    {
      "epoch": 28,
      "train_loss": 0.025509240239633576,
      "train_accuracy": 99.35166666666667,
      "val_loss": 0.1347508385060002,
      "val_accuracy": 97.76
    },
    {
      "epoch": 29,
      "train_loss": 0.026790958294743077,
      "train_accuracy": 99.32666666666667,
      "val_loss": 0.15851095548678978,
      "val_accuracy": 97.55
    },
    {
      "epoch": 30,
      "train_loss": 0.028651915796861264,
      "train_accuracy": 99.30666666666667,
      "val_loss": 0.1141947330027365,
      "val_accuracy": 97.94
    },
    {
      "epoch": 31,
      "train_loss": 0.026814629295262626,
      "train_accuracy": 99.33666666666667,
      "val_loss": 0.1413701447614557,
      "val_accuracy": 97.78
    },
    {
      "epoch": 32,
      "train_loss": 0.02409671341149437,
      "train_accuracy": 99.415,
      "val_loss": 0.11282232071828041,
      "val_accuracy": 97.96
    },
    {
      "epoch": 33,
      "train_loss": 0.024490047533589965,
      "train_accuracy": 99.39333333333333,
      "val_loss": 0.1600264471173809,
      "val_accuracy": 97.77
    },
    {
      "epoch": 34,
      "train_loss": 0.028407262825723263,
      "train_accuracy": 99.32666666666667,
      "val_loss": 0.09696307159034653,
      "val_accuracy": 98.02
    },
    {
      "epoch": 35,
      "train_loss": 0.02738858884253518,
      "train_accuracy": 99.36333333333333,
      "val_loss": 0.14396108344083228,
      "val_accuracy": 97.72
    },
    {
      "epoch": 36,
      "train_loss": 0.024126296054661392,
      "train_accuracy": 99.455,
      "val_loss": 0.15664287591777037,
      "val_accuracy": 97.79
    },
    {
      "epoch": 37,
      "train_loss": 0.026934334507052567,
      "train_accuracy": 99.36833333333334,
      "val_loss": 0.12963214882758078,
      "val_accuracy": 98.0
    },
    {
      "epoch": 38,
      "train_loss": 0.019191396243084705,
      "train_accuracy": 99.50333333333333,
      "val_loss": 0.1484009865717261,
      "val_accuracy": 97.85
    },
    {
      "epoch": 39,
      "train_loss": 0.02611596639901081,
      "train_accuracy": 99.41166666666666,
      "val_loss": 0.13180829662185112,
      "val_accuracy": 97.99
    },
    {
      "epoch": 40,
      "train_loss": 0.024443725642518226,
      "train_accuracy": 99.40333333333334,
      "val_loss": 0.1380307664066635,
      "val_accuracy": 97.85
    },
    {
      "epoch": 41,
      "train_loss": 0.024395478124719677,
      "train_accuracy": 99.405,
      "val_loss": 0.11985021840038111,
      "val_accuracy": 97.75
    },
    {
      "epoch": 42,
      "train_loss": 0.023306907511716584,
      "train_accuracy": 99.44,
      "val_loss": 0.1027266624758947,
      "val_accuracy": 98.17
    },
    {
      "epoch": 43,
      "train_loss": 0.0181866794641265,
      "train_accuracy": 99.61166666666666,
      "val_loss": 0.14997428367815482,
      "val_accuracy": 97.82
    },
    {
      "epoch": 44,
      "train_loss": 0.023962670364241153,
      "train_accuracy": 99.44833333333334,
      "val_loss": 0.1204058632433782,
      "val_accuracy": 98.25
    },
    {
      "epoch": 45,
      "train_loss": 0.02129175659596236,
      "train_accuracy": 99.51166666666667,
      "val_loss": 0.12487818473287887,
      "val_accuracy": 98.41
    },
    {
      "epoch": 46,
      "train_loss": 0.02148395701079798,
      "train_accuracy": 99.50166666666667,
      "val_loss": 0.14792176337936078,
      "val_accuracy": 98.09
    },
    {
      "epoch": 47,
      "train_loss": 0.02524800717940653,
      "train_accuracy": 99.46833333333333,
      "val_loss": 0.15907938015656878,
      "val_accuracy": 97.72
    },
    {
      "epoch": 48,
      "train_loss": 0.022372618402811555,
      "train_accuracy": 99.48333333333333,
      "val_loss": 0.12739865960585636,
      "val_accuracy": 97.98
    },
    {
      "epoch": 49,
      "train_loss": 0.023966333888422866,
      "train_accuracy": 99.50166666666667,
      "val_loss": 0.13027147422902174,
      "val_accuracy": 98.03
    },
    {
      "epoch": 50,
      "train_loss": 0.024332253050270022,
      "train_accuracy": 99.47166666666666,
      "val_loss": 0.13420959545071648,
      "val_accuracy": 97.79
    },
    {
      "epoch": 51,
      "train_loss": 0.017834667074213283,
      "train_accuracy": 99.60166666666667,
      "val_loss": 0.1620528830221289,
      "val_accuracy": 97.7
    },
    {
      "epoch": 52,
      "train_loss": 0.025193028004604864,
      "train_accuracy": 99.43333333333334,
      "val_loss": 0.1540388431227178,
      "val_accuracy": 98.12
    },
    {
      "epoch": 53,
      "train_loss": 0.02083720219165658,
      "train_accuracy": 99.53,
      "val_loss": 0.14983300107310904,
      "val_accuracy": 97.7
    },
    {
      "epoch": 54,
      "train_loss": 0.020099500110355308,
      "train_accuracy": 99.54833333333333,
      "val_loss": 0.1273204814512512,
      "val_accuracy": 98.25
    },
    {
      "epoch": 55,
      "train_loss": 0.02364328099220552,
      "train_accuracy": 99.43666666666667,
      "val_loss": 0.10946389117163938,
      "val_accuracy": 98.0
    },
    {
      "epoch": 56,
      "train_loss": 0.020164541169494905,
      "train_accuracy": 99.52,
      "val_loss": 0.11598781756808008,
      "val_accuracy": 98.2
    },
    {
      "epoch": 57,
      "train_loss": 0.022881001307317426,
      "train_accuracy": 99.49666666666667,
      "val_loss": 0.15824373750587892,
      "val_accuracy": 97.58
    },
    {
      "epoch": 58,
      "train_loss": 0.024607256495267158,
      "train_accuracy": 99.485,
      "val_loss": 0.1270288373133953,
      "val_accuracy": 97.9
    },
    {
      "epoch": 59,
      "train_loss": 0.017329799351822953,
      "train_accuracy": 99.56166666666667,
      "val_loss": 0.14224113770932625,
      "val_accuracy": 98.12
    },
    {
      "epoch": 60,
      "train_loss": 0.02257587553216018,
      "train_accuracy": 99.49666666666667,
      "val_loss": 0.12228939278530136,
      "val_accuracy": 98.09
    },
    {
      "epoch": 61,
      "train_loss": 0.022753716132900274,
      "train_accuracy": 99.545,
      "val_loss": 0.13433654782831914,
      "val_accuracy": 97.9
    },
    {
      "epoch": 62,
      "train_loss": 0.01820804135343461,
      "train_accuracy": 99.57,
      "val_loss": 0.16799482721335532,
      "val_accuracy": 97.63
    },
    {
      "epoch": 63,
      "train_loss": 0.023217549634112693,
      "train_accuracy": 99.425,
      "val_loss": 0.13253814273160924,
      "val_accuracy": 98.22
    },
    {
      "epoch": 64,
      "train_loss": 0.019606406908252282,
      "train_accuracy": 99.55833333333334,
      "val_loss": 0.12525235193019266,
      "val_accuracy": 97.74
    },
    {
      "epoch": 65,
      "train_loss": 0.018175325819866927,
      "train_accuracy": 99.57833333333333,
      "val_loss": 0.13867168478682165,
      "val_accuracy": 97.81
    },
    {
      "epoch": 66,
      "train_loss": 0.02070174484076626,
      "train_accuracy": 99.54333333333334,
      "val_loss": 0.15404853165171542,
      "val_accuracy": 98.12
    },
    {
      "epoch": 67,
      "train_loss": 0.015830530488991388,
      "train_accuracy": 99.59333333333333,
      "val_loss": 0.16322082215802128,
      "val_accuracy": 97.85
    },
    {
      "epoch": 68,
      "train_loss": 0.022736645281320175,
      "train_accuracy": 99.52833333333334,
      "val_loss": 0.19716292030735671,
      "val_accuracy": 97.56
    },
    {
      "epoch": 69,
      "train_loss": 0.023048481597942518,
      "train_accuracy": 99.51833333333333,
      "val_loss": 0.1617953322918635,
      "val_accuracy": 97.57
    },
    {
      "epoch": 70,
      "train_loss": 0.017644496234647162,
      "train_accuracy": 99.59666666666666,
      "val_loss": 0.2005858107281023,
      "val_accuracy": 97.21
    },
    {
      "epoch": 71,
      "train_loss": 0.022170811697315173,
      "train_accuracy": 99.5,
      "val_loss": 0.15420086450982187,
      "val_accuracy": 98.09
    },
    {
      "epoch": 72,
      "train_loss": 0.018576420539045773,
      "train_accuracy": 99.57333333333334,
      "val_loss": 0.1951191153953662,
      "val_accuracy": 97.61
    },
    {
      "epoch": 73,
      "train_loss": 0.02334481269051062,
      "train_accuracy": 99.49333333333334,
      "val_loss": 0.1439675955127255,
      "val_accuracy": 98.01
    },
    {
      "epoch": 74,
      "train_loss": 0.01819450151813491,
      "train_accuracy": 99.63166666666666,
      "val_loss": 0.16683663007183885,
      "val_accuracy": 97.97
    },
    {
      "epoch": 75,
      "train_loss": 0.021901151569863296,
      "train_accuracy": 99.52,
      "val_loss": 0.11992739001831133,
      "val_accuracy": 98.1
    },
    {
      "epoch": 76,
      "train_loss": 0.016437072344734512,
      "train_accuracy": 99.63833333333334,
      "val_loss": 0.18098790439141638,
      "val_accuracy": 97.84
    },
    {
      "epoch": 77,
      "train_loss": 0.022410311740382598,
      "train_accuracy": 99.46666666666667,
      "val_loss": 0.14678702972555363,
      "val_accuracy": 97.8
    },
    {
      "epoch": 78,
      "train_loss": 0.02315374450660681,
      "train_accuracy": 99.46666666666667,
      "val_loss": 0.1529802096410417,
      "val_accuracy": 98.04
    },
    {
      "epoch": 79,
      "train_loss": 0.017006398219372627,
      "train_accuracy": 99.59166666666667,
      "val_loss": 0.13965399755514088,
      "val_accuracy": 98.04
    },
    {
      "epoch": 80,
      "train_loss": 0.019339394274083703,
      "train_accuracy": 99.58166666666666,
      "val_loss": 0.15327013411503207,
      "val_accuracy": 98.23
    }
  ]
}