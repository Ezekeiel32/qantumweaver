{
  "job_id": "zpe_job_584704de",
  "status": "running",
  "parameters": {
    "modelName": "ZPE-QuantumWeaver-V1",
    "totalEpochs": 30,
    "batchSize": 32,
    "learningRate": 0.001,
    "weightDecay": 0.0001,
    "momentumParams": [
      0.9,
      0.9,
      0.9,
      0.9,
      0.9,
      0.9
    ],
    "strengthParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "noiseParams": [
      0.01,
      0.01,
      0.01,
      0.01,
      0.01,
      0.01
    ],
    "couplingParams": [
      0.1,
      0.1,
      0.1,
      0.1,
      0.1,
      0.1
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.1,
    "quantumMode": true,
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T21:50:28.197Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/30 - Batch 0/1875 - Loss: 2.3051",
    "Epoch 1/30 - Batch 100/1875 - Loss: 0.3616",
    "Epoch 1/30 - Batch 200/1875 - Loss: 0.3098",
    "Epoch 1/30 - Batch 300/1875 - Loss: 0.4563",
    "Epoch 1/30 - Batch 400/1875 - Loss: 0.3669",
    "Epoch 1/30 - Batch 500/1875 - Loss: 0.1933",
    "Epoch 1/30 - Batch 600/1875 - Loss: 0.4320",
    "Epoch 1/30 - Batch 700/1875 - Loss: 0.5594",
    "Epoch 1/30 - Batch 800/1875 - Loss: 0.1571",
    "Epoch 1/30 - Batch 900/1875 - Loss: 0.2774",
    "Epoch 1/30 - Batch 1000/1875 - Loss: 0.0829",
    "Epoch 1/30 - Batch 1100/1875 - Loss: 0.1835",
    "Epoch 1/30 - Batch 1200/1875 - Loss: 0.6322",
    "Epoch 1/30 - Batch 1300/1875 - Loss: 0.2252",
    "Epoch 1/30 - Batch 1400/1875 - Loss: 0.2025",
    "Epoch 1/30 - Batch 1500/1875 - Loss: 0.2784",
    "Epoch 1/30 - Batch 1600/1875 - Loss: 0.2982",
    "Epoch 1/30 - Batch 1700/1875 - Loss: 0.0680",
    "Epoch 1/30 - Batch 1800/1875 - Loss: 0.1419",
    "Epoch 1 completed - Train Acc: 91.86% - Val Acc: 94.82%",
    "Epoch 2/30 - Batch 0/1875 - Loss: 0.0689",
    "Epoch 2/30 - Batch 100/1875 - Loss: 0.0386",
    "Epoch 2/30 - Batch 200/1875 - Loss: 0.0364",
    "Epoch 2/30 - Batch 300/1875 - Loss: 0.3866",
    "Epoch 2/30 - Batch 400/1875 - Loss: 0.4480",
    "Epoch 2/30 - Batch 500/1875 - Loss: 0.0590",
    "Epoch 2/30 - Batch 600/1875 - Loss: 0.3272",
    "Epoch 2/30 - Batch 700/1875 - Loss: 0.0237",
    "Epoch 2/30 - Batch 800/1875 - Loss: 0.3922",
    "Epoch 2/30 - Batch 900/1875 - Loss: 0.2689",
    "Epoch 2/30 - Batch 1000/1875 - Loss: 0.1528",
    "Epoch 2/30 - Batch 1100/1875 - Loss: 0.0273",
    "Epoch 2/30 - Batch 1200/1875 - Loss: 0.1334",
    "Epoch 2/30 - Batch 1300/1875 - Loss: 0.3902",
    "Epoch 2/30 - Batch 1400/1875 - Loss: 0.0436",
    "Epoch 2/30 - Batch 1500/1875 - Loss: 0.0071",
    "Epoch 2/30 - Batch 1600/1875 - Loss: 0.0918",
    "Epoch 2/30 - Batch 1700/1875 - Loss: 0.1362",
    "Epoch 2/30 - Batch 1800/1875 - Loss: 0.1505",
    "Epoch 2 completed - Train Acc: 96.21% - Val Acc: 96.09%",
    "Epoch 3/30 - Batch 0/1875 - Loss: 0.0736",
    "Epoch 3/30 - Batch 100/1875 - Loss: 0.0790",
    "Epoch 3/30 - Batch 200/1875 - Loss: 0.0095",
    "Epoch 3/30 - Batch 300/1875 - Loss: 0.3191",
    "Epoch 3/30 - Batch 400/1875 - Loss: 0.0836",
    "Epoch 3/30 - Batch 500/1875 - Loss: 0.1405",
    "Epoch 3/30 - Batch 600/1875 - Loss: 0.0830",
    "Epoch 3/30 - Batch 700/1875 - Loss: 0.3398",
    "Epoch 3/30 - Batch 800/1875 - Loss: 0.0544",
    "Epoch 3/30 - Batch 900/1875 - Loss: 0.1037",
    "Epoch 3/30 - Batch 1000/1875 - Loss: 0.0188",
    "Epoch 3/30 - Batch 1100/1875 - Loss: 0.0657",
    "Epoch 3/30 - Batch 1200/1875 - Loss: 0.1088",
    "Epoch 3/30 - Batch 1300/1875 - Loss: 0.0555",
    "Epoch 3/30 - Batch 1400/1875 - Loss: 0.0073",
    "Epoch 3/30 - Batch 1500/1875 - Loss: 0.1382",
    "Epoch 3/30 - Batch 1600/1875 - Loss: 0.0276",
    "Epoch 3/30 - Batch 1700/1875 - Loss: 0.0595",
    "Epoch 3/30 - Batch 1800/1875 - Loss: 0.0039",
    "Epoch 3 completed - Train Acc: 97.30% - Val Acc: 95.49%",
    "Epoch 4/30 - Batch 0/1875 - Loss: 0.1513",
    "Epoch 4/30 - Batch 100/1875 - Loss: 0.3612",
    "Epoch 4/30 - Batch 200/1875 - Loss: 0.1426",
    "Epoch 4/30 - Batch 300/1875 - Loss: 0.0287",
    "Epoch 4/30 - Batch 400/1875 - Loss: 0.0378",
    "Epoch 4/30 - Batch 500/1875 - Loss: 0.0530",
    "Epoch 4/30 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 4/30 - Batch 700/1875 - Loss: 0.0615",
    "Epoch 4/30 - Batch 800/1875 - Loss: 0.0055",
    "Epoch 4/30 - Batch 900/1875 - Loss: 0.0036",
    "Epoch 4/30 - Batch 1000/1875 - Loss: 0.0833",
    "Epoch 4/30 - Batch 1100/1875 - Loss: 0.3143",
    "Epoch 4/30 - Batch 1200/1875 - Loss: 0.1282",
    "Epoch 4/30 - Batch 1300/1875 - Loss: 0.0635",
    "Epoch 4/30 - Batch 1400/1875 - Loss: 0.0879",
    "Epoch 4/30 - Batch 1500/1875 - Loss: 0.1388",
    "Epoch 4/30 - Batch 1600/1875 - Loss: 0.0257",
    "Epoch 4/30 - Batch 1700/1875 - Loss: 0.0192",
    "Epoch 4/30 - Batch 1800/1875 - Loss: 0.0628",
    "Epoch 4 completed - Train Acc: 97.62% - Val Acc: 97.55%",
    "Epoch 5/30 - Batch 0/1875 - Loss: 0.0145",
    "Epoch 5/30 - Batch 100/1875 - Loss: 0.0722",
    "Epoch 5/30 - Batch 200/1875 - Loss: 0.0069",
    "Epoch 5/30 - Batch 300/1875 - Loss: 0.0087",
    "Epoch 5/30 - Batch 400/1875 - Loss: 0.0462",
    "Epoch 5/30 - Batch 500/1875 - Loss: 0.0119",
    "Epoch 5/30 - Batch 600/1875 - Loss: 0.2120",
    "Epoch 5/30 - Batch 700/1875 - Loss: 0.1040",
    "Epoch 5/30 - Batch 800/1875 - Loss: 0.2170",
    "Epoch 5/30 - Batch 900/1875 - Loss: 0.0299",
    "Epoch 5/30 - Batch 1000/1875 - Loss: 0.0838",
    "Epoch 5/30 - Batch 1100/1875 - Loss: 0.1055",
    "Epoch 5/30 - Batch 1200/1875 - Loss: 0.0121",
    "Epoch 5/30 - Batch 1300/1875 - Loss: 0.0026",
    "Epoch 5/30 - Batch 1400/1875 - Loss: 0.3997",
    "Epoch 5/30 - Batch 1500/1875 - Loss: 0.1484",
    "Epoch 5/30 - Batch 1600/1875 - Loss: 0.0034",
    "Epoch 5/30 - Batch 1700/1875 - Loss: 0.0375",
    "Epoch 5/30 - Batch 1800/1875 - Loss: 0.1533",
    "Epoch 5 completed - Train Acc: 98.00% - Val Acc: 97.05%",
    "Epoch 6/30 - Batch 0/1875 - Loss: 0.1243",
    "Epoch 6/30 - Batch 100/1875 - Loss: 0.0084",
    "Epoch 6/30 - Batch 200/1875 - Loss: 0.0086",
    "Epoch 6/30 - Batch 300/1875 - Loss: 0.0879",
    "Epoch 6/30 - Batch 400/1875 - Loss: 0.0427",
    "Epoch 6/30 - Batch 500/1875 - Loss: 0.0583",
    "Epoch 6/30 - Batch 600/1875 - Loss: 0.0818",
    "Epoch 6/30 - Batch 700/1875 - Loss: 0.0102",
    "Epoch 6/30 - Batch 800/1875 - Loss: 0.0235",
    "Epoch 6/30 - Batch 900/1875 - Loss: 0.1480",
    "Epoch 6/30 - Batch 1000/1875 - Loss: 0.0213",
    "Epoch 6/30 - Batch 1100/1875 - Loss: 0.0486",
    "Epoch 6/30 - Batch 1200/1875 - Loss: 0.0255",
    "Epoch 6/30 - Batch 1300/1875 - Loss: 0.1833",
    "Epoch 6/30 - Batch 1400/1875 - Loss: 0.0021",
    "Epoch 6/30 - Batch 1500/1875 - Loss: 0.1033",
    "Epoch 6/30 - Batch 1600/1875 - Loss: 0.0870",
    "Epoch 6/30 - Batch 1700/1875 - Loss: 0.0451",
    "Epoch 6/30 - Batch 1800/1875 - Loss: 0.0645",
    "Epoch 6 completed - Train Acc: 98.18% - Val Acc: 97.08%",
    "Epoch 7/30 - Batch 0/1875 - Loss: 0.0719",
    "Epoch 7/30 - Batch 100/1875 - Loss: 0.0108",
    "Epoch 7/30 - Batch 200/1875 - Loss: 0.0612",
    "Epoch 7/30 - Batch 300/1875 - Loss: 0.0187",
    "Epoch 7/30 - Batch 400/1875 - Loss: 0.0012",
    "Epoch 7/30 - Batch 500/1875 - Loss: 0.0511",
    "Epoch 7/30 - Batch 600/1875 - Loss: 0.0067",
    "Epoch 7/30 - Batch 700/1875 - Loss: 0.0036",
    "Epoch 7/30 - Batch 800/1875 - Loss: 0.0453",
    "Epoch 7/30 - Batch 900/1875 - Loss: 0.0708",
    "Epoch 7/30 - Batch 1000/1875 - Loss: 0.0129",
    "Epoch 7/30 - Batch 1100/1875 - Loss: 0.0064",
    "Epoch 7/30 - Batch 1200/1875 - Loss: 0.0695",
    "Epoch 7/30 - Batch 1300/1875 - Loss: 0.0042",
    "Epoch 7/30 - Batch 1400/1875 - Loss: 0.0031",
    "Epoch 7/30 - Batch 1500/1875 - Loss: 0.0086",
    "Epoch 7/30 - Batch 1600/1875 - Loss: 0.0995",
    "Epoch 7/30 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 7/30 - Batch 1800/1875 - Loss: 0.0883",
    "Epoch 7 completed - Train Acc: 98.32% - Val Acc: 97.35%",
    "Epoch 8/30 - Batch 0/1875 - Loss: 0.0086",
    "Epoch 8/30 - Batch 100/1875 - Loss: 0.0016",
    "Epoch 8/30 - Batch 200/1875 - Loss: 0.0005",
    "Epoch 8/30 - Batch 300/1875 - Loss: 0.0318",
    "Epoch 8/30 - Batch 400/1875 - Loss: 0.0559",
    "Epoch 8/30 - Batch 500/1875 - Loss: 0.0322",
    "Epoch 8/30 - Batch 600/1875 - Loss: 0.0160",
    "Epoch 8/30 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 8/30 - Batch 800/1875 - Loss: 0.0035",
    "Epoch 8/30 - Batch 900/1875 - Loss: 0.0029",
    "Epoch 8/30 - Batch 1000/1875 - Loss: 0.0034",
    "Epoch 8/30 - Batch 1100/1875 - Loss: 0.0093",
    "Epoch 8/30 - Batch 1200/1875 - Loss: 0.0086",
    "Epoch 8/30 - Batch 1300/1875 - Loss: 0.0109",
    "Epoch 8/30 - Batch 1400/1875 - Loss: 0.0746",
    "Epoch 8/30 - Batch 1500/1875 - Loss: 0.0033",
    "Epoch 8/30 - Batch 1600/1875 - Loss: 0.0030",
    "Epoch 8/30 - Batch 1700/1875 - Loss: 0.0078",
    "Epoch 8/30 - Batch 1800/1875 - Loss: 0.0098",
    "Epoch 8 completed - Train Acc: 98.47% - Val Acc: 97.64%",
    "Epoch 9/30 - Batch 0/1875 - Loss: 0.0013",
    "Epoch 9/30 - Batch 100/1875 - Loss: 0.0037",
    "Epoch 9/30 - Batch 200/1875 - Loss: 0.0005",
    "Epoch 9/30 - Batch 300/1875 - Loss: 0.0015",
    "Epoch 9/30 - Batch 400/1875 - Loss: 0.0652",
    "Epoch 9/30 - Batch 500/1875 - Loss: 0.0028",
    "Epoch 9/30 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 9/30 - Batch 700/1875 - Loss: 0.0217",
    "Epoch 9/30 - Batch 800/1875 - Loss: 0.0043",
    "Epoch 9/30 - Batch 900/1875 - Loss: 0.2023",
    "Epoch 9/30 - Batch 1000/1875 - Loss: 0.0060",
    "Epoch 9/30 - Batch 1100/1875 - Loss: 0.0363",
    "Epoch 9/30 - Batch 1200/1875 - Loss: 0.0515",
    "Epoch 9/30 - Batch 1300/1875 - Loss: 0.0027",
    "Epoch 9/30 - Batch 1400/1875 - Loss: 0.0045",
    "Epoch 9/30 - Batch 1500/1875 - Loss: 0.0102",
    "Epoch 9/30 - Batch 1600/1875 - Loss: 0.0570",
    "Epoch 9/30 - Batch 1700/1875 - Loss: 0.0193",
    "Epoch 9/30 - Batch 1800/1875 - Loss: 0.0927",
    "Epoch 9 completed - Train Acc: 98.68% - Val Acc: 97.33%",
    "Epoch 10/30 - Batch 0/1875 - Loss: 0.0176",
    "Epoch 10/30 - Batch 100/1875 - Loss: 0.0179",
    "Epoch 10/30 - Batch 200/1875 - Loss: 0.2034",
    "Epoch 10/30 - Batch 300/1875 - Loss: 0.0039",
    "Epoch 10/30 - Batch 400/1875 - Loss: 0.0511",
    "Epoch 10/30 - Batch 500/1875 - Loss: 0.2490",
    "Epoch 10/30 - Batch 600/1875 - Loss: 0.0069",
    "Epoch 10/30 - Batch 700/1875 - Loss: 0.0287",
    "Epoch 10/30 - Batch 800/1875 - Loss: 0.0572",
    "Epoch 10/30 - Batch 900/1875 - Loss: 0.0058",
    "Epoch 10/30 - Batch 1000/1875 - Loss: 0.1789",
    "Epoch 10/30 - Batch 1100/1875 - Loss: 0.0193",
    "Epoch 10/30 - Batch 1200/1875 - Loss: 0.1429",
    "Epoch 10/30 - Batch 1300/1875 - Loss: 0.0020",
    "Epoch 10/30 - Batch 1400/1875 - Loss: 0.1567",
    "Epoch 10/30 - Batch 1500/1875 - Loss: 0.0046",
    "Epoch 10/30 - Batch 1600/1875 - Loss: 0.1500",
    "Epoch 10/30 - Batch 1700/1875 - Loss: 0.5491",
    "Epoch 10/30 - Batch 1800/1875 - Loss: 0.0103",
    "Epoch 10 completed - Train Acc: 98.66% - Val Acc: 97.73%",
    "Epoch 11/30 - Batch 0/1875 - Loss: 0.0079",
    "Epoch 11/30 - Batch 100/1875 - Loss: 0.0066",
    "Epoch 11/30 - Batch 200/1875 - Loss: 0.0294",
    "Epoch 11/30 - Batch 300/1875 - Loss: 0.0906",
    "Epoch 11/30 - Batch 400/1875 - Loss: 0.0066",
    "Epoch 11/30 - Batch 500/1875 - Loss: 0.0507",
    "Epoch 11/30 - Batch 600/1875 - Loss: 0.0663",
    "Epoch 11/30 - Batch 700/1875 - Loss: 0.0156",
    "Epoch 11/30 - Batch 800/1875 - Loss: 0.0363",
    "Epoch 11/30 - Batch 900/1875 - Loss: 0.1354",
    "Epoch 11/30 - Batch 1000/1875 - Loss: 0.1575",
    "Epoch 11/30 - Batch 1100/1875 - Loss: 0.0080",
    "Epoch 11/30 - Batch 1200/1875 - Loss: 0.0015",
    "Epoch 11/30 - Batch 1300/1875 - Loss: 0.0059",
    "Epoch 11/30 - Batch 1400/1875 - Loss: 0.0039",
    "Epoch 11/30 - Batch 1500/1875 - Loss: 0.0069",
    "Epoch 11/30 - Batch 1600/1875 - Loss: 0.1230",
    "Epoch 11/30 - Batch 1700/1875 - Loss: 0.0138",
    "Epoch 11/30 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 11 completed - Train Acc: 98.69% - Val Acc: 97.78%",
    "Epoch 12/30 - Batch 0/1875 - Loss: 0.0007",
    "Epoch 12/30 - Batch 100/1875 - Loss: 0.0121",
    "Epoch 12/30 - Batch 200/1875 - Loss: 0.0071",
    "Epoch 12/30 - Batch 300/1875 - Loss: 0.0200",
    "Epoch 12/30 - Batch 400/1875 - Loss: 0.0508",
    "Epoch 12/30 - Batch 500/1875 - Loss: 0.0026",
    "Epoch 12/30 - Batch 600/1875 - Loss: 0.0722",
    "Epoch 12/30 - Batch 700/1875 - Loss: 0.1612",
    "Epoch 12/30 - Batch 800/1875 - Loss: 0.2365",
    "Epoch 12/30 - Batch 900/1875 - Loss: 0.0027",
    "Epoch 12/30 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 12/30 - Batch 1100/1875 - Loss: 0.0089",
    "Epoch 12/30 - Batch 1200/1875 - Loss: 0.0085",
    "Epoch 12/30 - Batch 1300/1875 - Loss: 0.0047",
    "Epoch 12/30 - Batch 1400/1875 - Loss: 0.0044",
    "Epoch 12/30 - Batch 1500/1875 - Loss: 0.0290",
    "Epoch 12/30 - Batch 1600/1875 - Loss: 0.0261",
    "Epoch 12/30 - Batch 1700/1875 - Loss: 0.0122",
    "Epoch 12/30 - Batch 1800/1875 - Loss: 0.1902",
    "Epoch 12 completed - Train Acc: 98.77% - Val Acc: 97.80%",
    "Epoch 13/30 - Batch 0/1875 - Loss: 0.1668",
    "Epoch 13/30 - Batch 100/1875 - Loss: 0.0577",
    "Epoch 13/30 - Batch 200/1875 - Loss: 0.1411",
    "Epoch 13/30 - Batch 300/1875 - Loss: 0.1500",
    "Epoch 13/30 - Batch 400/1875 - Loss: 0.0633",
    "Epoch 13/30 - Batch 500/1875 - Loss: 0.1149",
    "Epoch 13/30 - Batch 600/1875 - Loss: 0.0185",
    "Epoch 13/30 - Batch 700/1875 - Loss: 0.0311",
    "Epoch 13/30 - Batch 800/1875 - Loss: 0.0099",
    "Epoch 13/30 - Batch 900/1875 - Loss: 0.0520",
    "Epoch 13/30 - Batch 1000/1875 - Loss: 0.1852",
    "Epoch 13/30 - Batch 1100/1875 - Loss: 0.0025",
    "Epoch 13/30 - Batch 1200/1875 - Loss: 0.1450",
    "Epoch 13/30 - Batch 1300/1875 - Loss: 0.0025",
    "Epoch 13/30 - Batch 1400/1875 - Loss: 0.0314",
    "Epoch 13/30 - Batch 1500/1875 - Loss: 0.1094",
    "Epoch 13/30 - Batch 1600/1875 - Loss: 0.0030",
    "Epoch 13/30 - Batch 1700/1875 - Loss: 0.0277",
    "Epoch 13/30 - Batch 1800/1875 - Loss: 0.0006",
    "Epoch 13 completed - Train Acc: 98.82% - Val Acc: 98.02%",
    "Epoch 14/30 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 14/30 - Batch 100/1875 - Loss: 0.0143",
    "Epoch 14/30 - Batch 200/1875 - Loss: 0.0271",
    "Epoch 14/30 - Batch 300/1875 - Loss: 0.0135",
    "Epoch 14/30 - Batch 400/1875 - Loss: 0.0697",
    "Epoch 14/30 - Batch 500/1875 - Loss: 0.0011",
    "Epoch 14/30 - Batch 600/1875 - Loss: 0.2714",
    "Epoch 14/30 - Batch 700/1875 - Loss: 0.0024",
    "Epoch 14/30 - Batch 800/1875 - Loss: 0.0026",
    "Epoch 14/30 - Batch 900/1875 - Loss: 0.0584",
    "Epoch 14/30 - Batch 1000/1875 - Loss: 0.0026",
    "Epoch 14/30 - Batch 1100/1875 - Loss: 0.0021",
    "Epoch 14/30 - Batch 1200/1875 - Loss: 0.0693",
    "Epoch 14/30 - Batch 1300/1875 - Loss: 0.0043",
    "Epoch 14/30 - Batch 1400/1875 - Loss: 0.0238",
    "Epoch 14/30 - Batch 1500/1875 - Loss: 0.0305",
    "Epoch 14/30 - Batch 1600/1875 - Loss: 0.0139",
    "Epoch 14/30 - Batch 1700/1875 - Loss: 0.0015",
    "Epoch 14/30 - Batch 1800/1875 - Loss: 0.0561",
    "Epoch 14 completed - Train Acc: 98.86% - Val Acc: 97.67%",
    "Epoch 15/30 - Batch 0/1875 - Loss: 0.0109",
    "Epoch 15/30 - Batch 100/1875 - Loss: 0.0104",
    "Epoch 15/30 - Batch 200/1875 - Loss: 0.1289",
    "Epoch 15/30 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 15/30 - Batch 400/1875 - Loss: 0.0316",
    "Epoch 15/30 - Batch 500/1875 - Loss: 0.0239",
    "Epoch 15/30 - Batch 600/1875 - Loss: 0.0158",
    "Epoch 15/30 - Batch 700/1875 - Loss: 0.0015",
    "Epoch 15/30 - Batch 800/1875 - Loss: 0.0126",
    "Epoch 15/30 - Batch 900/1875 - Loss: 0.0080",
    "Epoch 15/30 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 15/30 - Batch 1100/1875 - Loss: 0.0036",
    "Epoch 15/30 - Batch 1200/1875 - Loss: 0.0106",
    "Epoch 15/30 - Batch 1300/1875 - Loss: 0.0777",
    "Epoch 15/30 - Batch 1400/1875 - Loss: 0.0233",
    "Epoch 15/30 - Batch 1500/1875 - Loss: 0.0342",
    "Epoch 15/30 - Batch 1600/1875 - Loss: 0.0113",
    "Epoch 15/30 - Batch 1700/1875 - Loss: 0.0113",
    "Epoch 15/30 - Batch 1800/1875 - Loss: 0.0016",
    "Epoch 15 completed - Train Acc: 98.86% - Val Acc: 97.82%",
    "Epoch 16/30 - Batch 0/1875 - Loss: 0.0015",
    "Epoch 16/30 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 16/30 - Batch 200/1875 - Loss: 0.0167",
    "Epoch 16/30 - Batch 300/1875 - Loss: 0.0174",
    "Epoch 16/30 - Batch 400/1875 - Loss: 0.0236",
    "Epoch 16/30 - Batch 500/1875 - Loss: 0.0019",
    "Epoch 16/30 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 16/30 - Batch 700/1875 - Loss: 0.0267",
    "Epoch 16/30 - Batch 800/1875 - Loss: 0.0016",
    "Epoch 16/30 - Batch 900/1875 - Loss: 0.0010",
    "Epoch 16/30 - Batch 1000/1875 - Loss: 0.0030",
    "Epoch 16/30 - Batch 1100/1875 - Loss: 0.0028",
    "Epoch 16/30 - Batch 1200/1875 - Loss: 0.0256",
    "Epoch 16/30 - Batch 1300/1875 - Loss: 0.0020",
    "Epoch 16/30 - Batch 1400/1875 - Loss: 0.0115",
    "Epoch 16/30 - Batch 1500/1875 - Loss: 0.1385",
    "Epoch 16/30 - Batch 1600/1875 - Loss: 0.0007",
    "Epoch 16/30 - Batch 1700/1875 - Loss: 0.0143",
    "Epoch 16/30 - Batch 1800/1875 - Loss: 0.0283",
    "Epoch 16 completed - Train Acc: 98.94% - Val Acc: 97.87%",
    "Epoch 17/30 - Batch 0/1875 - Loss: 0.0299",
    "Epoch 17/30 - Batch 100/1875 - Loss: 0.0036",
    "Epoch 17/30 - Batch 200/1875 - Loss: 0.0306",
    "Epoch 17/30 - Batch 300/1875 - Loss: 0.0219",
    "Epoch 17/30 - Batch 400/1875 - Loss: 0.0009",
    "Epoch 17/30 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 17/30 - Batch 600/1875 - Loss: 0.0070",
    "Epoch 17/30 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 17/30 - Batch 800/1875 - Loss: 0.0045",
    "Epoch 17/30 - Batch 900/1875 - Loss: 0.0099",
    "Epoch 17/30 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 17/30 - Batch 1100/1875 - Loss: 0.0083",
    "Epoch 17/30 - Batch 1200/1875 - Loss: 0.0112",
    "Epoch 17/30 - Batch 1300/1875 - Loss: 0.0176",
    "Epoch 17/30 - Batch 1400/1875 - Loss: 0.0022",
    "Epoch 17/30 - Batch 1500/1875 - Loss: 0.1486",
    "Epoch 17/30 - Batch 1600/1875 - Loss: 0.0121",
    "Epoch 17/30 - Batch 1700/1875 - Loss: 0.0031",
    "Epoch 17/30 - Batch 1800/1875 - Loss: 0.0097",
    "Epoch 17 completed - Train Acc: 99.05% - Val Acc: 97.91%",
    "Epoch 18/30 - Batch 0/1875 - Loss: 0.0089",
    "Epoch 18/30 - Batch 100/1875 - Loss: 0.0054",
    "Epoch 18/30 - Batch 200/1875 - Loss: 0.0030",
    "Epoch 18/30 - Batch 300/1875 - Loss: 0.0405",
    "Epoch 18/30 - Batch 400/1875 - Loss: 0.0270",
    "Epoch 18/30 - Batch 500/1875 - Loss: 0.0169",
    "Epoch 18/30 - Batch 600/1875 - Loss: 0.0139",
    "Epoch 18/30 - Batch 700/1875 - Loss: 0.0577",
    "Epoch 18/30 - Batch 800/1875 - Loss: 0.0029",
    "Epoch 18/30 - Batch 900/1875 - Loss: 0.0361",
    "Epoch 18/30 - Batch 1000/1875 - Loss: 0.0014",
    "Epoch 18/30 - Batch 1100/1875 - Loss: 0.3209",
    "Epoch 18/30 - Batch 1200/1875 - Loss: 0.2510",
    "Epoch 18/30 - Batch 1300/1875 - Loss: 0.0019",
    "Epoch 18/30 - Batch 1400/1875 - Loss: 0.0058",
    "Epoch 18/30 - Batch 1500/1875 - Loss: 0.0028",
    "Epoch 18/30 - Batch 1600/1875 - Loss: 0.0056",
    "Epoch 18/30 - Batch 1700/1875 - Loss: 0.0512",
    "Epoch 18/30 - Batch 1800/1875 - Loss: 0.1352",
    "Epoch 18 completed - Train Acc: 99.05% - Val Acc: 97.98%",
    "Epoch 19/30 - Batch 0/1875 - Loss: 0.0016",
    "Epoch 19/30 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 19/30 - Batch 200/1875 - Loss: 0.0039",
    "Epoch 19/30 - Batch 300/1875 - Loss: 0.0979",
    "Epoch 19/30 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 19/30 - Batch 500/1875 - Loss: 0.0837",
    "Epoch 19/30 - Batch 600/1875 - Loss: 0.0017",
    "Epoch 19/30 - Batch 700/1875 - Loss: 0.0027",
    "Epoch 19/30 - Batch 800/1875 - Loss: 0.0505",
    "Epoch 19/30 - Batch 900/1875 - Loss: 0.0012",
    "Epoch 19/30 - Batch 1000/1875 - Loss: 0.0038",
    "Epoch 19/30 - Batch 1100/1875 - Loss: 0.0020",
    "Epoch 19/30 - Batch 1200/1875 - Loss: 0.0830",
    "Epoch 19/30 - Batch 1300/1875 - Loss: 0.0088",
    "Epoch 19/30 - Batch 1400/1875 - Loss: 0.0322",
    "Epoch 19/30 - Batch 1500/1875 - Loss: 0.0275",
    "Epoch 19/30 - Batch 1600/1875 - Loss: 0.0092",
    "Epoch 19/30 - Batch 1700/1875 - Loss: 0.0095",
    "Epoch 19/30 - Batch 1800/1875 - Loss: 0.0050",
    "Epoch 19 completed - Train Acc: 99.03% - Val Acc: 97.56%",
    "Epoch 20/30 - Batch 0/1875 - Loss: 0.0110",
    "Epoch 20/30 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 20/30 - Batch 200/1875 - Loss: 0.0906",
    "Epoch 20/30 - Batch 300/1875 - Loss: 0.0010",
    "Epoch 20/30 - Batch 400/1875 - Loss: 0.0005",
    "Epoch 20/30 - Batch 500/1875 - Loss: 0.0017",
    "Epoch 20/30 - Batch 600/1875 - Loss: 0.0028",
    "Epoch 20/30 - Batch 700/1875 - Loss: 0.0010",
    "Epoch 20/30 - Batch 800/1875 - Loss: 0.1344",
    "Epoch 20/30 - Batch 900/1875 - Loss: 0.0315",
    "Epoch 20/30 - Batch 1000/1875 - Loss: 0.0039",
    "Epoch 20/30 - Batch 1100/1875 - Loss: 0.0726",
    "Epoch 20/30 - Batch 1200/1875 - Loss: 0.0007",
    "Epoch 20/30 - Batch 1300/1875 - Loss: 0.0133",
    "Epoch 20/30 - Batch 1400/1875 - Loss: 0.0295",
    "Epoch 20/30 - Batch 1500/1875 - Loss: 0.0053",
    "Epoch 20/30 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 20/30 - Batch 1700/1875 - Loss: 0.0009",
    "Epoch 20/30 - Batch 1800/1875 - Loss: 0.0033",
    "Epoch 20 completed - Train Acc: 99.00% - Val Acc: 97.85%",
    "Epoch 21/30 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 21/30 - Batch 100/1875 - Loss: 0.0092",
    "Epoch 21/30 - Batch 200/1875 - Loss: 0.0043",
    "Epoch 21/30 - Batch 300/1875 - Loss: 0.0020",
    "Epoch 21/30 - Batch 400/1875 - Loss: 0.0018",
    "Epoch 21/30 - Batch 500/1875 - Loss: 0.0010",
    "Epoch 21/30 - Batch 600/1875 - Loss: 0.0121",
    "Epoch 21/30 - Batch 700/1875 - Loss: 0.0215",
    "Epoch 21/30 - Batch 800/1875 - Loss: 0.0141",
    "Epoch 21/30 - Batch 900/1875 - Loss: 0.0206",
    "Epoch 21/30 - Batch 1000/1875 - Loss: 0.0008",
    "Epoch 21/30 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 21/30 - Batch 1200/1875 - Loss: 0.0035",
    "Epoch 21/30 - Batch 1300/1875 - Loss: 0.0063",
    "Epoch 21/30 - Batch 1400/1875 - Loss: 0.0021",
    "Epoch 21/30 - Batch 1500/1875 - Loss: 0.0012",
    "Epoch 21/30 - Batch 1600/1875 - Loss: 0.1415",
    "Epoch 21/30 - Batch 1700/1875 - Loss: 0.0082",
    "Epoch 21/30 - Batch 1800/1875 - Loss: 0.0112",
    "Epoch 21 completed - Train Acc: 99.08% - Val Acc: 97.46%",
    "Epoch 22/30 - Batch 0/1875 - Loss: 0.0326",
    "Epoch 22/30 - Batch 100/1875 - Loss: 0.0098",
    "Epoch 22/30 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 22/30 - Batch 300/1875 - Loss: 0.0378",
    "Epoch 22/30 - Batch 400/1875 - Loss: 0.0034",
    "Epoch 22/30 - Batch 500/1875 - Loss: 0.0093",
    "Epoch 22/30 - Batch 600/1875 - Loss: 0.0688",
    "Epoch 22/30 - Batch 700/1875 - Loss: 0.0730",
    "Epoch 22/30 - Batch 800/1875 - Loss: 0.0013",
    "Epoch 22/30 - Batch 900/1875 - Loss: 0.0116",
    "Epoch 22/30 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 22/30 - Batch 1100/1875 - Loss: 0.0206",
    "Epoch 22/30 - Batch 1200/1875 - Loss: 0.0026",
    "Epoch 22/30 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 22/30 - Batch 1400/1875 - Loss: 0.0111",
    "Epoch 22/30 - Batch 1500/1875 - Loss: 0.0463",
    "Epoch 22/30 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 22/30 - Batch 1700/1875 - Loss: 0.0136",
    "Epoch 22/30 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 22 completed - Train Acc: 99.07% - Val Acc: 97.86%",
    "Epoch 23/30 - Batch 0/1875 - Loss: 0.0237",
    "Epoch 23/30 - Batch 100/1875 - Loss: 0.0051",
    "Epoch 23/30 - Batch 200/1875 - Loss: 0.0785",
    "Epoch 23/30 - Batch 300/1875 - Loss: 0.1245",
    "Epoch 23/30 - Batch 400/1875 - Loss: 0.0296",
    "Epoch 23/30 - Batch 500/1875 - Loss: 0.0220",
    "Epoch 23/30 - Batch 600/1875 - Loss: 0.2941",
    "Epoch 23/30 - Batch 700/1875 - Loss: 0.1041",
    "Epoch 23/30 - Batch 800/1875 - Loss: 0.0019",
    "Epoch 23/30 - Batch 900/1875 - Loss: 0.2105",
    "Epoch 23/30 - Batch 1000/1875 - Loss: 0.0946",
    "Epoch 23/30 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 23/30 - Batch 1200/1875 - Loss: 0.0006",
    "Epoch 23/30 - Batch 1300/1875 - Loss: 0.0448",
    "Epoch 23/30 - Batch 1400/1875 - Loss: 0.0923",
    "Epoch 23/30 - Batch 1500/1875 - Loss: 0.1340",
    "Epoch 23/30 - Batch 1600/1875 - Loss: 0.0251",
    "Epoch 23/30 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 23/30 - Batch 1800/1875 - Loss: 0.0886",
    "Epoch 23 completed - Train Acc: 99.03% - Val Acc: 97.57%",
    "Epoch 24/30 - Batch 0/1875 - Loss: 0.0016",
    "Epoch 24/30 - Batch 100/1875 - Loss: 0.0061",
    "Epoch 24/30 - Batch 200/1875 - Loss: 0.0046",
    "Epoch 24/30 - Batch 300/1875 - Loss: 0.0112",
    "Epoch 24/30 - Batch 400/1875 - Loss: 0.0106",
    "Epoch 24/30 - Batch 500/1875 - Loss: 0.0033",
    "Epoch 24/30 - Batch 600/1875 - Loss: 0.0017",
    "Epoch 24/30 - Batch 700/1875 - Loss: 0.0399",
    "Epoch 24/30 - Batch 800/1875 - Loss: 0.1335",
    "Epoch 24/30 - Batch 900/1875 - Loss: 0.0043",
    "Epoch 24/30 - Batch 1000/1875 - Loss: 0.0039",
    "Epoch 24/30 - Batch 1100/1875 - Loss: 0.0018",
    "Epoch 24/30 - Batch 1200/1875 - Loss: 0.0192",
    "Epoch 24/30 - Batch 1300/1875 - Loss: 0.0211",
    "Epoch 24/30 - Batch 1400/1875 - Loss: 0.0005",
    "Epoch 24/30 - Batch 1500/1875 - Loss: 0.0017",
    "Epoch 24/30 - Batch 1600/1875 - Loss: 0.0293",
    "Epoch 24/30 - Batch 1700/1875 - Loss: 0.0063",
    "Epoch 24/30 - Batch 1800/1875 - Loss: 0.0292",
    "Epoch 24 completed - Train Acc: 99.05% - Val Acc: 98.05%",
    "Epoch 25/30 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 25/30 - Batch 100/1875 - Loss: 0.1195",
    "Epoch 25/30 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 25/30 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 25/30 - Batch 400/1875 - Loss: 0.0057",
    "Epoch 25/30 - Batch 500/1875 - Loss: 0.0185",
    "Epoch 25/30 - Batch 600/1875 - Loss: 0.0918",
    "Epoch 25/30 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 25/30 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 25/30 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 25/30 - Batch 1000/1875 - Loss: 0.0128",
    "Epoch 25/30 - Batch 1100/1875 - Loss: 0.0108",
    "Epoch 25/30 - Batch 1200/1875 - Loss: 0.1715",
    "Epoch 25/30 - Batch 1300/1875 - Loss: 0.0146",
    "Epoch 25/30 - Batch 1400/1875 - Loss: 0.0019",
    "Epoch 25/30 - Batch 1500/1875 - Loss: 0.0052",
    "Epoch 25/30 - Batch 1600/1875 - Loss: 0.0283",
    "Epoch 25/30 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 25/30 - Batch 1800/1875 - Loss: 0.0039",
    "Epoch 25 completed - Train Acc: 99.17% - Val Acc: 97.91%",
    "Epoch 26/30 - Batch 0/1875 - Loss: 0.0065",
    "Epoch 26/30 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 26/30 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 26/30 - Batch 300/1875 - Loss: 0.0056",
    "Epoch 26/30 - Batch 400/1875 - Loss: 0.0033",
    "Epoch 26/30 - Batch 500/1875 - Loss: 0.1297",
    "Epoch 26/30 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 26/30 - Batch 700/1875 - Loss: 0.0524",
    "Epoch 26/30 - Batch 800/1875 - Loss: 0.0357",
    "Epoch 26/30 - Batch 900/1875 - Loss: 0.0029",
    "Epoch 26/30 - Batch 1000/1875 - Loss: 0.1945",
    "Epoch 26/30 - Batch 1100/1875 - Loss: 0.0629",
    "Epoch 26/30 - Batch 1200/1875 - Loss: 0.0021",
    "Epoch 26/30 - Batch 1300/1875 - Loss: 0.1814",
    "Epoch 26/30 - Batch 1400/1875 - Loss: 0.0230",
    "Epoch 26/30 - Batch 1500/1875 - Loss: 0.0111",
    "Epoch 26/30 - Batch 1600/1875 - Loss: 0.0519",
    "Epoch 26/30 - Batch 1700/1875 - Loss: 0.0006",
    "Epoch 26/30 - Batch 1800/1875 - Loss: 0.0034",
    "Epoch 26 completed - Train Acc: 99.07% - Val Acc: 98.15%",
    "Epoch 27/30 - Batch 0/1875 - Loss: 0.0023"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.2665835754180948,
      "train_accuracy": 91.86,
      "val_loss": 0.18023699125388085,
      "val_accuracy": 94.82
    },
    {
      "epoch": 2,
      "train_loss": 0.13372855076398701,
      "train_accuracy": 96.20666666666666,
      "val_loss": 0.1420254402499372,
      "val_accuracy": 96.09
    },
    {
      "epoch": 3,
      "train_loss": 0.09622909826819474,
      "train_accuracy": 97.30166666666666,
      "val_loss": 0.16291268523128818,
      "val_accuracy": 95.49
    },
    {
      "epoch": 4,
      "train_loss": 0.08454191471959153,
      "train_accuracy": 97.61666666666666,
      "val_loss": 0.09286218045326387,
      "val_accuracy": 97.55
    },
    {
      "epoch": 5,
      "train_loss": 0.07252902289667788,
      "train_accuracy": 97.995,
      "val_loss": 0.11218638150524475,
      "val_accuracy": 97.05
    },
    {
      "epoch": 6,
      "train_loss": 0.06310732904980736,
      "train_accuracy": 98.18,
      "val_loss": 0.10643490644215722,
      "val_accuracy": 97.08
    },
    {
      "epoch": 7,
      "train_loss": 0.05703146017708738,
      "train_accuracy": 98.32166666666667,
      "val_loss": 0.0996561855855411,
      "val_accuracy": 97.35
    },
    {
      "epoch": 8,
      "train_loss": 0.05217273662457204,
      "train_accuracy": 98.47166666666666,
      "val_loss": 0.09014456212558165,
      "val_accuracy": 97.64
    },
    {
      "epoch": 9,
      "train_loss": 0.0462101496604213,
      "train_accuracy": 98.67666666666666,
      "val_loss": 0.11097106339931166,
      "val_accuracy": 97.33
    },
    {
      "epoch": 10,
      "train_loss": 0.0477334300947492,
      "train_accuracy": 98.655,
      "val_loss": 0.09082971186280964,
      "val_accuracy": 97.73
    },
    {
      "epoch": 11,
      "train_loss": 0.046042534641115344,
      "train_accuracy": 98.685,
      "val_loss": 0.09128100297113151,
      "val_accuracy": 97.78
    },
    {
      "epoch": 12,
      "train_loss": 0.03995091646645063,
      "train_accuracy": 98.77,
      "val_loss": 0.08025155026299005,
      "val_accuracy": 97.8
    },
    {
      "epoch": 13,
      "train_loss": 0.04062373508100767,
      "train_accuracy": 98.82166666666667,
      "val_loss": 0.08681195330304887,
      "val_accuracy": 98.02
    },
    {
      "epoch": 14,
      "train_loss": 0.03766479805761968,
      "train_accuracy": 98.86333333333333,
      "val_loss": 0.100408335099376,
      "val_accuracy": 97.67
    },
    {
      "epoch": 15,
      "train_loss": 0.03777050439258067,
      "train_accuracy": 98.855,
      "val_loss": 0.08091558275141077,
      "val_accuracy": 97.82
    },
    {
      "epoch": 16,
      "train_loss": 0.03599122974869921,
      "train_accuracy": 98.94166666666666,
      "val_loss": 0.10002088565707178,
      "val_accuracy": 97.87
    },
    {
      "epoch": 17,
      "train_loss": 0.03385498428461287,
      "train_accuracy": 99.05,
      "val_loss": 0.08175662439200863,
      "val_accuracy": 97.91
    },
    {
      "epoch": 18,
      "train_loss": 0.03251917612357647,
      "train_accuracy": 99.05333333333333,
      "val_loss": 0.08320228296630817,
      "val_accuracy": 97.98
    },
    {
      "epoch": 19,
      "train_loss": 0.032420903074500775,
      "train_accuracy": 99.02666666666667,
      "val_loss": 0.09793319770275635,
      "val_accuracy": 97.56
    },
    {
      "epoch": 20,
      "train_loss": 0.0323981483784577,
      "train_accuracy": 98.99833333333333,
      "val_loss": 0.09533932201023776,
      "val_accuracy": 97.85
    },
    {
      "epoch": 21,
      "train_loss": 0.03217301839946789,
      "train_accuracy": 99.07833333333333,
      "val_loss": 0.10966966112750823,
      "val_accuracy": 97.46
    },
    {
      "epoch": 22,
      "train_loss": 0.031802286861102036,
      "train_accuracy": 99.06666666666666,
      "val_loss": 0.08588810671934975,
      "val_accuracy": 97.86
    },
    {
      "epoch": 23,
      "train_loss": 0.03204062310052298,
      "train_accuracy": 99.025,
      "val_loss": 0.10428358523902065,
      "val_accuracy": 97.57
    },
    {
      "epoch": 24,
      "train_loss": 0.03114576810358364,
      "train_accuracy": 99.05166666666666,
      "val_loss": 0.0808205196829886,
      "val_accuracy": 98.05
    },
    {
      "epoch": 25,
      "train_loss": 0.028354900467801297,
      "train_accuracy": 99.16833333333334,
      "val_loss": 0.08615839902879129,
      "val_accuracy": 97.91
    },
    {
      "epoch": 26,
      "train_loss": 0.02962328590410325,
      "train_accuracy": 99.06833333333333,
      "val_loss": 0.08034022103819068,
      "val_accuracy": 98.15
    }
  ]
}