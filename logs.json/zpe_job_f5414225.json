{
  "job_id": "zpe_job_f5414225",
  "status": "completed",
  "parameters": {
    "modelName": "ZPE-Colab-Sim_hnn_step8",
    "totalEpochs": 60,
    "batchSize": 32,
    "learningRate": 0.0013,
    "weightDecay": 1.2567843546879078e-05,
    "momentumParams": [
      0.97,
      0.85,
      0.78,
      0.72,
      0.41,
      0.76
    ],
    "strengthParams": [
      0.19,
      0.55,
      0.41,
      0.6,
      0.37,
      0.44
    ],
    "noiseParams": [
      0.17,
      0.26,
      0.24,
      0.43,
      0.28,
      0.24
    ],
    "couplingParams": [
      0.75,
      0.85,
      0.82,
      0.79,
      0.76,
      0.6
    ],
    "quantumCircuitSize": 32,
    "labelSmoothing": 0.07,
    "quantumMode": true,
    "baseConfigId": "zpe_job_169cda4c",
    "mixupAlpha": 0.2
  },
  "start_time": "2025-06-19T22:12:37.687Z",
  "log_messages": [
    "Using device: cuda",
    "Datasets loaded successfully",
    "Model initialized",
    "Epoch 1/60 - Batch 0/1875 - Loss: 2.3293",
    "Epoch 1/60 - Batch 100/1875 - Loss: 0.3474",
    "Epoch 1/60 - Batch 200/1875 - Loss: 0.7294",
    "Epoch 1/60 - Batch 300/1875 - Loss: 0.4389",
    "Epoch 1/60 - Batch 400/1875 - Loss: 0.3246",
    "Epoch 1/60 - Batch 500/1875 - Loss: 0.2187",
    "Epoch 1/60 - Batch 600/1875 - Loss: 0.1912",
    "Epoch 1/60 - Batch 700/1875 - Loss: 0.1254",
    "Epoch 1/60 - Batch 800/1875 - Loss: 0.1392",
    "Epoch 1/60 - Batch 900/1875 - Loss: 0.2912",
    "Epoch 1/60 - Batch 1000/1875 - Loss: 0.0600",
    "Epoch 1/60 - Batch 1100/1875 - Loss: 0.2135",
    "Epoch 1/60 - Batch 1200/1875 - Loss: 0.0751",
    "Epoch 1/60 - Batch 1300/1875 - Loss: 0.0917",
    "Epoch 1/60 - Batch 1400/1875 - Loss: 0.0541",
    "Epoch 1/60 - Batch 1500/1875 - Loss: 0.1107",
    "Epoch 1/60 - Batch 1600/1875 - Loss: 0.4129",
    "Epoch 1/60 - Batch 1700/1875 - Loss: 0.1203",
    "Epoch 1/60 - Batch 1800/1875 - Loss: 0.1336",
    "Epoch 1 completed - Train Acc: 91.33% - Val Acc: 95.35%",
    "Epoch 2/60 - Batch 0/1875 - Loss: 0.1185",
    "Epoch 2/60 - Batch 100/1875 - Loss: 0.0929",
    "Epoch 2/60 - Batch 200/1875 - Loss: 0.0129",
    "Epoch 2/60 - Batch 300/1875 - Loss: 0.0827",
    "Epoch 2/60 - Batch 400/1875 - Loss: 0.0360",
    "Epoch 2/60 - Batch 500/1875 - Loss: 0.1852",
    "Epoch 2/60 - Batch 600/1875 - Loss: 0.0277",
    "Epoch 2/60 - Batch 700/1875 - Loss: 0.2251",
    "Epoch 2/60 - Batch 800/1875 - Loss: 0.0746",
    "Epoch 2/60 - Batch 900/1875 - Loss: 0.0439",
    "Epoch 2/60 - Batch 1000/1875 - Loss: 0.0848",
    "Epoch 2/60 - Batch 1100/1875 - Loss: 0.0713",
    "Epoch 2/60 - Batch 1200/1875 - Loss: 0.0362",
    "Epoch 2/60 - Batch 1300/1875 - Loss: 0.1558",
    "Epoch 2/60 - Batch 1400/1875 - Loss: 0.1967",
    "Epoch 2/60 - Batch 1500/1875 - Loss: 0.1541",
    "Epoch 2/60 - Batch 1600/1875 - Loss: 0.1312",
    "Epoch 2/60 - Batch 1700/1875 - Loss: 0.2820",
    "Epoch 2/60 - Batch 1800/1875 - Loss: 0.0840",
    "Epoch 2 completed - Train Acc: 96.27% - Val Acc: 95.55%",
    "Epoch 3/60 - Batch 0/1875 - Loss: 0.1278",
    "Epoch 3/60 - Batch 100/1875 - Loss: 0.0680",
    "Epoch 3/60 - Batch 200/1875 - Loss: 0.0030",
    "Epoch 3/60 - Batch 300/1875 - Loss: 0.0903",
    "Epoch 3/60 - Batch 400/1875 - Loss: 0.1583",
    "Epoch 3/60 - Batch 500/1875 - Loss: 0.0165",
    "Epoch 3/60 - Batch 600/1875 - Loss: 0.0169",
    "Epoch 3/60 - Batch 700/1875 - Loss: 0.2731",
    "Epoch 3/60 - Batch 800/1875 - Loss: 0.0475",
    "Epoch 3/60 - Batch 900/1875 - Loss: 0.0467",
    "Epoch 3/60 - Batch 1000/1875 - Loss: 0.0069",
    "Epoch 3/60 - Batch 1100/1875 - Loss: 0.1026",
    "Epoch 3/60 - Batch 1200/1875 - Loss: 0.1157",
    "Epoch 3/60 - Batch 1300/1875 - Loss: 0.1445",
    "Epoch 3/60 - Batch 1400/1875 - Loss: 0.0935",
    "Epoch 3/60 - Batch 1500/1875 - Loss: 0.1327",
    "Epoch 3/60 - Batch 1600/1875 - Loss: 0.1391",
    "Epoch 3/60 - Batch 1700/1875 - Loss: 0.1802",
    "Epoch 3/60 - Batch 1800/1875 - Loss: 0.0137",
    "Epoch 3 completed - Train Acc: 97.14% - Val Acc: 97.18%",
    "Epoch 4/60 - Batch 0/1875 - Loss: 0.0615",
    "Epoch 4/60 - Batch 100/1875 - Loss: 0.2244",
    "Epoch 4/60 - Batch 200/1875 - Loss: 0.0595",
    "Epoch 4/60 - Batch 300/1875 - Loss: 0.0049",
    "Epoch 4/60 - Batch 400/1875 - Loss: 0.0639",
    "Epoch 4/60 - Batch 500/1875 - Loss: 0.0490",
    "Epoch 4/60 - Batch 600/1875 - Loss: 0.0069",
    "Epoch 4/60 - Batch 700/1875 - Loss: 0.0070",
    "Epoch 4/60 - Batch 800/1875 - Loss: 0.0021",
    "Epoch 4/60 - Batch 900/1875 - Loss: 0.2378",
    "Epoch 4/60 - Batch 1000/1875 - Loss: 0.0160",
    "Epoch 4/60 - Batch 1100/1875 - Loss: 0.1498",
    "Epoch 4/60 - Batch 1200/1875 - Loss: 0.5525",
    "Epoch 4/60 - Batch 1300/1875 - Loss: 0.0118",
    "Epoch 4/60 - Batch 1400/1875 - Loss: 0.0079",
    "Epoch 4/60 - Batch 1500/1875 - Loss: 0.0126",
    "Epoch 4/60 - Batch 1600/1875 - Loss: 0.1707",
    "Epoch 4/60 - Batch 1700/1875 - Loss: 0.0696",
    "Epoch 4/60 - Batch 1800/1875 - Loss: 0.0105",
    "Epoch 4 completed - Train Acc: 97.57% - Val Acc: 97.34%",
    "Epoch 5/60 - Batch 0/1875 - Loss: 0.0050",
    "Epoch 5/60 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 5/60 - Batch 200/1875 - Loss: 0.0265",
    "Epoch 5/60 - Batch 300/1875 - Loss: 0.0379",
    "Epoch 5/60 - Batch 400/1875 - Loss: 0.0173",
    "Epoch 5/60 - Batch 500/1875 - Loss: 0.0841",
    "Epoch 5/60 - Batch 600/1875 - Loss: 0.0069",
    "Epoch 5/60 - Batch 700/1875 - Loss: 0.0018",
    "Epoch 5/60 - Batch 800/1875 - Loss: 0.0798",
    "Epoch 5/60 - Batch 900/1875 - Loss: 0.0019",
    "Epoch 5/60 - Batch 1000/1875 - Loss: 0.0349",
    "Epoch 5/60 - Batch 1100/1875 - Loss: 0.1212",
    "Epoch 5/60 - Batch 1200/1875 - Loss: 0.1043",
    "Epoch 5/60 - Batch 1300/1875 - Loss: 0.0074",
    "Epoch 5/60 - Batch 1400/1875 - Loss: 0.0612",
    "Epoch 5/60 - Batch 1500/1875 - Loss: 0.0076",
    "Epoch 5/60 - Batch 1600/1875 - Loss: 0.0588",
    "Epoch 5/60 - Batch 1700/1875 - Loss: 0.0099",
    "Epoch 5/60 - Batch 1800/1875 - Loss: 0.0830",
    "Epoch 5 completed - Train Acc: 97.98% - Val Acc: 96.65%",
    "Epoch 6/60 - Batch 0/1875 - Loss: 0.1530",
    "Epoch 6/60 - Batch 100/1875 - Loss: 0.0101",
    "Epoch 6/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 6/60 - Batch 300/1875 - Loss: 0.0381",
    "Epoch 6/60 - Batch 400/1875 - Loss: 0.0624",
    "Epoch 6/60 - Batch 500/1875 - Loss: 0.0057",
    "Epoch 6/60 - Batch 600/1875 - Loss: 0.0359",
    "Epoch 6/60 - Batch 700/1875 - Loss: 0.0005",
    "Epoch 6/60 - Batch 800/1875 - Loss: 0.0496",
    "Epoch 6/60 - Batch 900/1875 - Loss: 0.0439",
    "Epoch 6/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 6/60 - Batch 1100/1875 - Loss: 0.0168",
    "Epoch 6/60 - Batch 1200/1875 - Loss: 0.0860",
    "Epoch 6/60 - Batch 1300/1875 - Loss: 0.0608",
    "Epoch 6/60 - Batch 1400/1875 - Loss: 0.0749",
    "Epoch 6/60 - Batch 1500/1875 - Loss: 0.0068",
    "Epoch 6/60 - Batch 1600/1875 - Loss: 0.0330",
    "Epoch 6/60 - Batch 1700/1875 - Loss: 0.0008",
    "Epoch 6/60 - Batch 1800/1875 - Loss: 0.0037",
    "Epoch 6 completed - Train Acc: 98.15% - Val Acc: 97.13%",
    "Epoch 7/60 - Batch 0/1875 - Loss: 0.0684",
    "Epoch 7/60 - Batch 100/1875 - Loss: 0.0048",
    "Epoch 7/60 - Batch 200/1875 - Loss: 0.0255",
    "Epoch 7/60 - Batch 300/1875 - Loss: 0.0042",
    "Epoch 7/60 - Batch 400/1875 - Loss: 0.0015",
    "Epoch 7/60 - Batch 500/1875 - Loss: 0.0345",
    "Epoch 7/60 - Batch 600/1875 - Loss: 0.0210",
    "Epoch 7/60 - Batch 700/1875 - Loss: 0.0022",
    "Epoch 7/60 - Batch 800/1875 - Loss: 0.1155",
    "Epoch 7/60 - Batch 900/1875 - Loss: 0.0015",
    "Epoch 7/60 - Batch 1000/1875 - Loss: 0.1951",
    "Epoch 7/60 - Batch 1100/1875 - Loss: 0.2818",
    "Epoch 7/60 - Batch 1200/1875 - Loss: 0.0175",
    "Epoch 7/60 - Batch 1300/1875 - Loss: 0.0329",
    "Epoch 7/60 - Batch 1400/1875 - Loss: 0.0197",
    "Epoch 7/60 - Batch 1500/1875 - Loss: 0.1534",
    "Epoch 7/60 - Batch 1600/1875 - Loss: 0.1609",
    "Epoch 7/60 - Batch 1700/1875 - Loss: 0.2507",
    "Epoch 7/60 - Batch 1800/1875 - Loss: 0.0028",
    "Epoch 7 completed - Train Acc: 98.33% - Val Acc: 97.95%",
    "Epoch 8/60 - Batch 0/1875 - Loss: 0.0392",
    "Epoch 8/60 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 8/60 - Batch 200/1875 - Loss: 0.1137",
    "Epoch 8/60 - Batch 300/1875 - Loss: 0.3194",
    "Epoch 8/60 - Batch 400/1875 - Loss: 0.0104",
    "Epoch 8/60 - Batch 500/1875 - Loss: 0.0348",
    "Epoch 8/60 - Batch 600/1875 - Loss: 0.0021",
    "Epoch 8/60 - Batch 700/1875 - Loss: 0.0116",
    "Epoch 8/60 - Batch 800/1875 - Loss: 0.0050",
    "Epoch 8/60 - Batch 900/1875 - Loss: 0.0111",
    "Epoch 8/60 - Batch 1000/1875 - Loss: 0.0274",
    "Epoch 8/60 - Batch 1100/1875 - Loss: 0.0124",
    "Epoch 8/60 - Batch 1200/1875 - Loss: 0.1919",
    "Epoch 8/60 - Batch 1300/1875 - Loss: 0.0101",
    "Epoch 8/60 - Batch 1400/1875 - Loss: 0.1287",
    "Epoch 8/60 - Batch 1500/1875 - Loss: 0.0112",
    "Epoch 8/60 - Batch 1600/1875 - Loss: 0.0037",
    "Epoch 8/60 - Batch 1700/1875 - Loss: 0.0031",
    "Epoch 8/60 - Batch 1800/1875 - Loss: 0.0090",
    "Epoch 8 completed - Train Acc: 98.56% - Val Acc: 97.67%",
    "Epoch 9/60 - Batch 0/1875 - Loss: 0.0258",
    "Epoch 9/60 - Batch 100/1875 - Loss: 0.0195",
    "Epoch 9/60 - Batch 200/1875 - Loss: 0.0045",
    "Epoch 9/60 - Batch 300/1875 - Loss: 0.0114",
    "Epoch 9/60 - Batch 400/1875 - Loss: 0.0075",
    "Epoch 9/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 9/60 - Batch 600/1875 - Loss: 0.0094",
    "Epoch 9/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 9/60 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 9/60 - Batch 900/1875 - Loss: 0.1118",
    "Epoch 9/60 - Batch 1000/1875 - Loss: 0.0270",
    "Epoch 9/60 - Batch 1100/1875 - Loss: 0.0713",
    "Epoch 9/60 - Batch 1200/1875 - Loss: 0.0020",
    "Epoch 9/60 - Batch 1300/1875 - Loss: 0.0052",
    "Epoch 9/60 - Batch 1400/1875 - Loss: 0.0038",
    "Epoch 9/60 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 9/60 - Batch 1600/1875 - Loss: 0.0213",
    "Epoch 9/60 - Batch 1700/1875 - Loss: 0.0184",
    "Epoch 9/60 - Batch 1800/1875 - Loss: 0.1931",
    "Epoch 9 completed - Train Acc: 98.63% - Val Acc: 97.68%",
    "Epoch 10/60 - Batch 0/1875 - Loss: 0.0275",
    "Epoch 10/60 - Batch 100/1875 - Loss: 0.0021",
    "Epoch 10/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 10/60 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 10/60 - Batch 400/1875 - Loss: 0.0042",
    "Epoch 10/60 - Batch 500/1875 - Loss: 0.1336",
    "Epoch 10/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 10/60 - Batch 700/1875 - Loss: 0.0400",
    "Epoch 10/60 - Batch 800/1875 - Loss: 0.0022",
    "Epoch 10/60 - Batch 900/1875 - Loss: 0.0322",
    "Epoch 10/60 - Batch 1000/1875 - Loss: 0.0208",
    "Epoch 10/60 - Batch 1100/1875 - Loss: 0.0175",
    "Epoch 10/60 - Batch 1200/1875 - Loss: 0.1590",
    "Epoch 10/60 - Batch 1300/1875 - Loss: 0.0418",
    "Epoch 10/60 - Batch 1400/1875 - Loss: 0.0047",
    "Epoch 10/60 - Batch 1500/1875 - Loss: 0.1813",
    "Epoch 10/60 - Batch 1600/1875 - Loss: 0.0018",
    "Epoch 10/60 - Batch 1700/1875 - Loss: 0.0029",
    "Epoch 10/60 - Batch 1800/1875 - Loss: 0.0167",
    "Epoch 10 completed - Train Acc: 98.76% - Val Acc: 97.49%",
    "Epoch 11/60 - Batch 0/1875 - Loss: 0.7353",
    "Epoch 11/60 - Batch 100/1875 - Loss: 0.0085",
    "Epoch 11/60 - Batch 200/1875 - Loss: 0.0724",
    "Epoch 11/60 - Batch 300/1875 - Loss: 0.0018",
    "Epoch 11/60 - Batch 400/1875 - Loss: 0.2464",
    "Epoch 11/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 11/60 - Batch 600/1875 - Loss: 0.0430",
    "Epoch 11/60 - Batch 700/1875 - Loss: 0.0061",
    "Epoch 11/60 - Batch 800/1875 - Loss: 0.0033",
    "Epoch 11/60 - Batch 900/1875 - Loss: 0.0008",
    "Epoch 11/60 - Batch 1000/1875 - Loss: 0.1834",
    "Epoch 11/60 - Batch 1100/1875 - Loss: 0.1796",
    "Epoch 11/60 - Batch 1200/1875 - Loss: 0.0696",
    "Epoch 11/60 - Batch 1300/1875 - Loss: 0.0024",
    "Epoch 11/60 - Batch 1400/1875 - Loss: 0.0321",
    "Epoch 11/60 - Batch 1500/1875 - Loss: 0.0019",
    "Epoch 11/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 11/60 - Batch 1700/1875 - Loss: 0.0215",
    "Epoch 11/60 - Batch 1800/1875 - Loss: 0.0012",
    "Epoch 11 completed - Train Acc: 98.81% - Val Acc: 98.06%",
    "Epoch 12/60 - Batch 0/1875 - Loss: 0.0087",
    "Epoch 12/60 - Batch 100/1875 - Loss: 0.0365",
    "Epoch 12/60 - Batch 200/1875 - Loss: 0.0194",
    "Epoch 12/60 - Batch 300/1875 - Loss: 0.0035",
    "Epoch 12/60 - Batch 400/1875 - Loss: 0.0836",
    "Epoch 12/60 - Batch 500/1875 - Loss: 0.1349",
    "Epoch 12/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 12/60 - Batch 700/1875 - Loss: 0.1243",
    "Epoch 12/60 - Batch 800/1875 - Loss: 0.0354",
    "Epoch 12/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 12/60 - Batch 1000/1875 - Loss: 0.0162",
    "Epoch 12/60 - Batch 1100/1875 - Loss: 0.0007",
    "Epoch 12/60 - Batch 1200/1875 - Loss: 0.0538",
    "Epoch 12/60 - Batch 1300/1875 - Loss: 0.0091",
    "Epoch 12/60 - Batch 1400/1875 - Loss: 0.0117",
    "Epoch 12/60 - Batch 1500/1875 - Loss: 0.0320",
    "Epoch 12/60 - Batch 1600/1875 - Loss: 0.2892",
    "Epoch 12/60 - Batch 1700/1875 - Loss: 0.0424",
    "Epoch 12/60 - Batch 1800/1875 - Loss: 0.3146",
    "Epoch 12 completed - Train Acc: 98.88% - Val Acc: 97.58%",
    "Epoch 13/60 - Batch 0/1875 - Loss: 0.0520",
    "Epoch 13/60 - Batch 100/1875 - Loss: 0.0029",
    "Epoch 13/60 - Batch 200/1875 - Loss: 0.0190",
    "Epoch 13/60 - Batch 300/1875 - Loss: 0.0007",
    "Epoch 13/60 - Batch 400/1875 - Loss: 0.0050",
    "Epoch 13/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 13/60 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 13/60 - Batch 700/1875 - Loss: 0.0138",
    "Epoch 13/60 - Batch 800/1875 - Loss: 0.0065",
    "Epoch 13/60 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 13/60 - Batch 1000/1875 - Loss: 0.0073",
    "Epoch 13/60 - Batch 1100/1875 - Loss: 0.0110",
    "Epoch 13/60 - Batch 1200/1875 - Loss: 0.0997",
    "Epoch 13/60 - Batch 1300/1875 - Loss: 0.0647",
    "Epoch 13/60 - Batch 1400/1875 - Loss: 0.0911",
    "Epoch 13/60 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 13/60 - Batch 1600/1875 - Loss: 0.0432",
    "Epoch 13/60 - Batch 1700/1875 - Loss: 0.0014",
    "Epoch 13/60 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 13 completed - Train Acc: 98.94% - Val Acc: 98.23%",
    "Epoch 14/60 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 14/60 - Batch 100/1875 - Loss: 0.0027",
    "Epoch 14/60 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 14/60 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 14/60 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 14/60 - Batch 500/1875 - Loss: 0.0491",
    "Epoch 14/60 - Batch 600/1875 - Loss: 0.2784",
    "Epoch 14/60 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 14/60 - Batch 800/1875 - Loss: 0.0243",
    "Epoch 14/60 - Batch 900/1875 - Loss: 0.0064",
    "Epoch 14/60 - Batch 1000/1875 - Loss: 0.0039",
    "Epoch 14/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 14/60 - Batch 1200/1875 - Loss: 0.2894",
    "Epoch 14/60 - Batch 1300/1875 - Loss: 0.0288",
    "Epoch 14/60 - Batch 1400/1875 - Loss: 0.0754",
    "Epoch 14/60 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 14/60 - Batch 1600/1875 - Loss: 0.2606",
    "Epoch 14/60 - Batch 1700/1875 - Loss: 0.0021",
    "Epoch 14/60 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 14 completed - Train Acc: 98.98% - Val Acc: 97.41%",
    "Epoch 15/60 - Batch 0/1875 - Loss: 0.0009",
    "Epoch 15/60 - Batch 100/1875 - Loss: 0.0062",
    "Epoch 15/60 - Batch 200/1875 - Loss: 0.0044",
    "Epoch 15/60 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 15/60 - Batch 400/1875 - Loss: 0.0124",
    "Epoch 15/60 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 15/60 - Batch 600/1875 - Loss: 0.0378",
    "Epoch 15/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 15/60 - Batch 800/1875 - Loss: 0.0032",
    "Epoch 15/60 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 15/60 - Batch 1000/1875 - Loss: 0.0019",
    "Epoch 15/60 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 15/60 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 15/60 - Batch 1300/1875 - Loss: 0.0075",
    "Epoch 15/60 - Batch 1400/1875 - Loss: 0.0076",
    "Epoch 15/60 - Batch 1500/1875 - Loss: 0.0272",
    "Epoch 15/60 - Batch 1600/1875 - Loss: 0.0277",
    "Epoch 15/60 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 15/60 - Batch 1800/1875 - Loss: 0.0041",
    "Epoch 15 completed - Train Acc: 99.07% - Val Acc: 97.99%",
    "Epoch 16/60 - Batch 0/1875 - Loss: 0.0121",
    "Epoch 16/60 - Batch 100/1875 - Loss: 0.0046",
    "Epoch 16/60 - Batch 200/1875 - Loss: 0.0310",
    "Epoch 16/60 - Batch 300/1875 - Loss: 0.1190",
    "Epoch 16/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 16/60 - Batch 500/1875 - Loss: 0.0014",
    "Epoch 16/60 - Batch 600/1875 - Loss: 0.0048",
    "Epoch 16/60 - Batch 700/1875 - Loss: 0.0034",
    "Epoch 16/60 - Batch 800/1875 - Loss: 0.0052",
    "Epoch 16/60 - Batch 900/1875 - Loss: 0.1067",
    "Epoch 16/60 - Batch 1000/1875 - Loss: 0.1394",
    "Epoch 16/60 - Batch 1100/1875 - Loss: 0.1513",
    "Epoch 16/60 - Batch 1200/1875 - Loss: 0.0303",
    "Epoch 16/60 - Batch 1300/1875 - Loss: 0.0501",
    "Epoch 16/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 16/60 - Batch 1500/1875 - Loss: 0.0131",
    "Epoch 16/60 - Batch 1600/1875 - Loss: 0.0048",
    "Epoch 16/60 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 16/60 - Batch 1800/1875 - Loss: 0.0482",
    "Epoch 16 completed - Train Acc: 99.10% - Val Acc: 98.12%",
    "Epoch 17/60 - Batch 0/1875 - Loss: 0.0032",
    "Epoch 17/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 17/60 - Batch 200/1875 - Loss: 0.0124",
    "Epoch 17/60 - Batch 300/1875 - Loss: 0.0044",
    "Epoch 17/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 17/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 17/60 - Batch 600/1875 - Loss: 0.0411",
    "Epoch 17/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 17/60 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 17/60 - Batch 900/1875 - Loss: 0.2088",
    "Epoch 17/60 - Batch 1000/1875 - Loss: 0.2010",
    "Epoch 17/60 - Batch 1100/1875 - Loss: 0.0010",
    "Epoch 17/60 - Batch 1200/1875 - Loss: 0.0312",
    "Epoch 17/60 - Batch 1300/1875 - Loss: 0.0349",
    "Epoch 17/60 - Batch 1400/1875 - Loss: 0.0084",
    "Epoch 17/60 - Batch 1500/1875 - Loss: 0.0029",
    "Epoch 17/60 - Batch 1600/1875 - Loss: 0.0022",
    "Epoch 17/60 - Batch 1700/1875 - Loss: 0.0681",
    "Epoch 17/60 - Batch 1800/1875 - Loss: 0.0204",
    "Epoch 17 completed - Train Acc: 99.09% - Val Acc: 97.93%",
    "Epoch 18/60 - Batch 0/1875 - Loss: 0.0900",
    "Epoch 18/60 - Batch 100/1875 - Loss: 0.0405",
    "Epoch 18/60 - Batch 200/1875 - Loss: 0.0110",
    "Epoch 18/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 18/60 - Batch 400/1875 - Loss: 0.0297",
    "Epoch 18/60 - Batch 500/1875 - Loss: 0.0007",
    "Epoch 18/60 - Batch 600/1875 - Loss: 0.0498",
    "Epoch 18/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 18/60 - Batch 800/1875 - Loss: 0.0018",
    "Epoch 18/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 18/60 - Batch 1000/1875 - Loss: 0.0010",
    "Epoch 18/60 - Batch 1100/1875 - Loss: 0.0035",
    "Epoch 18/60 - Batch 1200/1875 - Loss: 0.0047",
    "Epoch 18/60 - Batch 1300/1875 - Loss: 0.0116",
    "Epoch 18/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 18/60 - Batch 1500/1875 - Loss: 0.0897",
    "Epoch 18/60 - Batch 1600/1875 - Loss: 0.0005",
    "Epoch 18/60 - Batch 1700/1875 - Loss: 0.0016",
    "Epoch 18/60 - Batch 1800/1875 - Loss: 0.0004",
    "Epoch 18 completed - Train Acc: 99.16% - Val Acc: 97.64%",
    "Epoch 19/60 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 19/60 - Batch 100/1875 - Loss: 0.0014",
    "Epoch 19/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 19/60 - Batch 300/1875 - Loss: 0.0117",
    "Epoch 19/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 19/60 - Batch 500/1875 - Loss: 0.0026",
    "Epoch 19/60 - Batch 600/1875 - Loss: 0.0076",
    "Epoch 19/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 19/60 - Batch 800/1875 - Loss: 0.0024",
    "Epoch 19/60 - Batch 900/1875 - Loss: 0.0562",
    "Epoch 19/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 19/60 - Batch 1100/1875 - Loss: 0.0015",
    "Epoch 19/60 - Batch 1200/1875 - Loss: 0.0077",
    "Epoch 19/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 19/60 - Batch 1400/1875 - Loss: 0.0090",
    "Epoch 19/60 - Batch 1500/1875 - Loss: 0.0613",
    "Epoch 19/60 - Batch 1600/1875 - Loss: 0.0017",
    "Epoch 19/60 - Batch 1700/1875 - Loss: 0.1981",
    "Epoch 19/60 - Batch 1800/1875 - Loss: 0.0079",
    "Epoch 19 completed - Train Acc: 99.19% - Val Acc: 97.68%",
    "Epoch 20/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 20/60 - Batch 100/1875 - Loss: 0.0692",
    "Epoch 20/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 20/60 - Batch 300/1875 - Loss: 0.0563",
    "Epoch 20/60 - Batch 400/1875 - Loss: 0.0138",
    "Epoch 20/60 - Batch 500/1875 - Loss: 0.0023",
    "Epoch 20/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 20/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 20/60 - Batch 800/1875 - Loss: 0.0012",
    "Epoch 20/60 - Batch 900/1875 - Loss: 0.0370",
    "Epoch 20/60 - Batch 1000/1875 - Loss: 0.0205",
    "Epoch 20/60 - Batch 1100/1875 - Loss: 0.0298",
    "Epoch 20/60 - Batch 1200/1875 - Loss: 0.0190",
    "Epoch 20/60 - Batch 1300/1875 - Loss: 0.0041",
    "Epoch 20/60 - Batch 1400/1875 - Loss: 0.0013",
    "Epoch 20/60 - Batch 1500/1875 - Loss: 0.0010",
    "Epoch 20/60 - Batch 1600/1875 - Loss: 0.0207",
    "Epoch 20/60 - Batch 1700/1875 - Loss: 0.0532",
    "Epoch 20/60 - Batch 1800/1875 - Loss: 0.0032",
    "Epoch 20 completed - Train Acc: 99.23% - Val Acc: 97.78%",
    "Epoch 21/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 21/60 - Batch 100/1875 - Loss: 0.0010",
    "Epoch 21/60 - Batch 200/1875 - Loss: 0.0003",
    "Epoch 21/60 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 21/60 - Batch 400/1875 - Loss: 0.0048",
    "Epoch 21/60 - Batch 500/1875 - Loss: 0.0038",
    "Epoch 21/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 21/60 - Batch 700/1875 - Loss: 0.0596",
    "Epoch 21/60 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 21/60 - Batch 900/1875 - Loss: 0.0029",
    "Epoch 21/60 - Batch 1000/1875 - Loss: 0.0068",
    "Epoch 21/60 - Batch 1100/1875 - Loss: 0.0347",
    "Epoch 21/60 - Batch 1200/1875 - Loss: 0.0109",
    "Epoch 21/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 21/60 - Batch 1400/1875 - Loss: 0.0023",
    "Epoch 21/60 - Batch 1500/1875 - Loss: 0.0671",
    "Epoch 21/60 - Batch 1600/1875 - Loss: 0.0555",
    "Epoch 21/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 21/60 - Batch 1800/1875 - Loss: 0.0033",
    "Epoch 21 completed - Train Acc: 99.20% - Val Acc: 97.68%",
    "Epoch 22/60 - Batch 0/1875 - Loss: 0.0020",
    "Epoch 22/60 - Batch 100/1875 - Loss: 0.0009",
    "Epoch 22/60 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 22/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 22/60 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 22/60 - Batch 500/1875 - Loss: 0.0173",
    "Epoch 22/60 - Batch 600/1875 - Loss: 0.0008",
    "Epoch 22/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 22/60 - Batch 800/1875 - Loss: 0.0186",
    "Epoch 22/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 22/60 - Batch 1000/1875 - Loss: 0.1474",
    "Epoch 22/60 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 22/60 - Batch 1200/1875 - Loss: 0.0020",
    "Epoch 22/60 - Batch 1300/1875 - Loss: 0.0271",
    "Epoch 22/60 - Batch 1400/1875 - Loss: 0.1156",
    "Epoch 22/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 22/60 - Batch 1600/1875 - Loss: 0.0067",
    "Epoch 22/60 - Batch 1700/1875 - Loss: 0.0008",
    "Epoch 22/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 22 completed - Train Acc: 99.26% - Val Acc: 98.09%",
    "Epoch 23/60 - Batch 0/1875 - Loss: 0.1137",
    "Epoch 23/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 23/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 23/60 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 23/60 - Batch 400/1875 - Loss: 0.0445",
    "Epoch 23/60 - Batch 500/1875 - Loss: 0.0008",
    "Epoch 23/60 - Batch 600/1875 - Loss: 0.2040",
    "Epoch 23/60 - Batch 700/1875 - Loss: 0.0325",
    "Epoch 23/60 - Batch 800/1875 - Loss: 0.0014",
    "Epoch 23/60 - Batch 900/1875 - Loss: 0.0144",
    "Epoch 23/60 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 23/60 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 23/60 - Batch 1200/1875 - Loss: 0.0472",
    "Epoch 23/60 - Batch 1300/1875 - Loss: 0.1520",
    "Epoch 23/60 - Batch 1400/1875 - Loss: 0.0004",
    "Epoch 23/60 - Batch 1500/1875 - Loss: 0.0024",
    "Epoch 23/60 - Batch 1600/1875 - Loss: 0.0422",
    "Epoch 23/60 - Batch 1700/1875 - Loss: 0.0073",
    "Epoch 23/60 - Batch 1800/1875 - Loss: 0.0200",
    "Epoch 23 completed - Train Acc: 99.27% - Val Acc: 97.47%",
    "Epoch 24/60 - Batch 0/1875 - Loss: 0.0021",
    "Epoch 24/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 24/60 - Batch 200/1875 - Loss: 0.0021",
    "Epoch 24/60 - Batch 300/1875 - Loss: 0.0901",
    "Epoch 24/60 - Batch 400/1875 - Loss: 0.0731",
    "Epoch 24/60 - Batch 500/1875 - Loss: 0.0042",
    "Epoch 24/60 - Batch 600/1875 - Loss: 0.0039",
    "Epoch 24/60 - Batch 700/1875 - Loss: 0.0270",
    "Epoch 24/60 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 24/60 - Batch 900/1875 - Loss: 0.0014",
    "Epoch 24/60 - Batch 1000/1875 - Loss: 0.0012",
    "Epoch 24/60 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 24/60 - Batch 1200/1875 - Loss: 0.0125",
    "Epoch 24/60 - Batch 1300/1875 - Loss: 0.0137",
    "Epoch 24/60 - Batch 1400/1875 - Loss: 0.0012",
    "Epoch 24/60 - Batch 1500/1875 - Loss: 0.0010",
    "Epoch 24/60 - Batch 1600/1875 - Loss: 0.3731",
    "Epoch 24/60 - Batch 1700/1875 - Loss: 0.5853",
    "Epoch 24/60 - Batch 1800/1875 - Loss: 0.0085",
    "Epoch 24 completed - Train Acc: 99.28% - Val Acc: 98.04%",
    "Epoch 25/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 25/60 - Batch 100/1875 - Loss: 0.0017",
    "Epoch 25/60 - Batch 200/1875 - Loss: 0.0006",
    "Epoch 25/60 - Batch 300/1875 - Loss: 0.1323",
    "Epoch 25/60 - Batch 400/1875 - Loss: 0.2691",
    "Epoch 25/60 - Batch 500/1875 - Loss: 0.0148",
    "Epoch 25/60 - Batch 600/1875 - Loss: 0.0024",
    "Epoch 25/60 - Batch 700/1875 - Loss: 0.0030",
    "Epoch 25/60 - Batch 800/1875 - Loss: 0.0011",
    "Epoch 25/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 25/60 - Batch 1000/1875 - Loss: 0.0093",
    "Epoch 25/60 - Batch 1100/1875 - Loss: 0.0170",
    "Epoch 25/60 - Batch 1200/1875 - Loss: 0.0083",
    "Epoch 25/60 - Batch 1300/1875 - Loss: 0.0073",
    "Epoch 25/60 - Batch 1400/1875 - Loss: 0.0119",
    "Epoch 25/60 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 25/60 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 25/60 - Batch 1700/1875 - Loss: 0.2473",
    "Epoch 25/60 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 25 completed - Train Acc: 99.32% - Val Acc: 98.21%",
    "Epoch 26/60 - Batch 0/1875 - Loss: 0.0935",
    "Epoch 26/60 - Batch 100/1875 - Loss: 0.0016",
    "Epoch 26/60 - Batch 200/1875 - Loss: 0.0025",
    "Epoch 26/60 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 26/60 - Batch 400/1875 - Loss: 0.2899",
    "Epoch 26/60 - Batch 500/1875 - Loss: 0.0230",
    "Epoch 26/60 - Batch 600/1875 - Loss: 0.0339",
    "Epoch 26/60 - Batch 700/1875 - Loss: 0.3930",
    "Epoch 26/60 - Batch 800/1875 - Loss: 0.0248",
    "Epoch 26/60 - Batch 900/1875 - Loss: 0.1285",
    "Epoch 26/60 - Batch 1000/1875 - Loss: 0.0188",
    "Epoch 26/60 - Batch 1100/1875 - Loss: 0.0588",
    "Epoch 26/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 26/60 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 26/60 - Batch 1400/1875 - Loss: 0.0006",
    "Epoch 26/60 - Batch 1500/1875 - Loss: 0.0717",
    "Epoch 26/60 - Batch 1600/1875 - Loss: 0.0999",
    "Epoch 26/60 - Batch 1700/1875 - Loss: 0.0280",
    "Epoch 26/60 - Batch 1800/1875 - Loss: 0.0140",
    "Epoch 26 completed - Train Acc: 99.26% - Val Acc: 98.15%",
    "Epoch 27/60 - Batch 0/1875 - Loss: 0.0003",
    "Epoch 27/60 - Batch 100/1875 - Loss: 0.0117",
    "Epoch 27/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 27/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 27/60 - Batch 400/1875 - Loss: 0.0023",
    "Epoch 27/60 - Batch 500/1875 - Loss: 0.0012",
    "Epoch 27/60 - Batch 600/1875 - Loss: 0.0018",
    "Epoch 27/60 - Batch 700/1875 - Loss: 0.0340",
    "Epoch 27/60 - Batch 800/1875 - Loss: 0.1589",
    "Epoch 27/60 - Batch 900/1875 - Loss: 0.0071",
    "Epoch 27/60 - Batch 1000/1875 - Loss: 0.0051",
    "Epoch 27/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 27/60 - Batch 1200/1875 - Loss: 0.0207",
    "Epoch 27/60 - Batch 1300/1875 - Loss: 0.0021",
    "Epoch 27/60 - Batch 1400/1875 - Loss: 0.0014",
    "Epoch 27/60 - Batch 1500/1875 - Loss: 0.0006",
    "Epoch 27/60 - Batch 1600/1875 - Loss: 0.0955",
    "Epoch 27/60 - Batch 1700/1875 - Loss: 0.1298",
    "Epoch 27/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 27 completed - Train Acc: 99.39% - Val Acc: 97.86%",
    "Epoch 28/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 28/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 28/60 - Batch 200/1875 - Loss: 0.1800",
    "Epoch 28/60 - Batch 300/1875 - Loss: 0.0008",
    "Epoch 28/60 - Batch 400/1875 - Loss: 0.0012",
    "Epoch 28/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 28/60 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 28/60 - Batch 700/1875 - Loss: 0.2624",
    "Epoch 28/60 - Batch 800/1875 - Loss: 0.0007",
    "Epoch 28/60 - Batch 900/1875 - Loss: 0.0033",
    "Epoch 28/60 - Batch 1000/1875 - Loss: 0.0211",
    "Epoch 28/60 - Batch 1100/1875 - Loss: 0.0068",
    "Epoch 28/60 - Batch 1200/1875 - Loss: 0.0033",
    "Epoch 28/60 - Batch 1300/1875 - Loss: 0.0010",
    "Epoch 28/60 - Batch 1400/1875 - Loss: 0.0008",
    "Epoch 28/60 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 28/60 - Batch 1600/1875 - Loss: 0.0025",
    "Epoch 28/60 - Batch 1700/1875 - Loss: 0.1233",
    "Epoch 28/60 - Batch 1800/1875 - Loss: 0.0045",
    "Epoch 28 completed - Train Acc: 99.38% - Val Acc: 97.77%",
    "Epoch 29/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 29/60 - Batch 100/1875 - Loss: 0.0072",
    "Epoch 29/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 29/60 - Batch 300/1875 - Loss: 0.0004",
    "Epoch 29/60 - Batch 400/1875 - Loss: 0.0186",
    "Epoch 29/60 - Batch 500/1875 - Loss: 0.0027",
    "Epoch 29/60 - Batch 600/1875 - Loss: 0.0285",
    "Epoch 29/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 29/60 - Batch 800/1875 - Loss: 0.0074",
    "Epoch 29/60 - Batch 900/1875 - Loss: 0.0037",
    "Epoch 29/60 - Batch 1000/1875 - Loss: 0.0024",
    "Epoch 29/60 - Batch 1100/1875 - Loss: 0.0031",
    "Epoch 29/60 - Batch 1200/1875 - Loss: 0.0003",
    "Epoch 29/60 - Batch 1300/1875 - Loss: 0.0734",
    "Epoch 29/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 29/60 - Batch 1500/1875 - Loss: 0.0431",
    "Epoch 29/60 - Batch 1600/1875 - Loss: 0.0522",
    "Epoch 29/60 - Batch 1700/1875 - Loss: 0.0484",
    "Epoch 29/60 - Batch 1800/1875 - Loss: 0.0764",
    "Epoch 29 completed - Train Acc: 99.30% - Val Acc: 97.80%",
    "Epoch 30/60 - Batch 0/1875 - Loss: 0.0011",
    "Epoch 30/60 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 30/60 - Batch 200/1875 - Loss: 0.0002",
    "Epoch 30/60 - Batch 300/1875 - Loss: 0.0642",
    "Epoch 30/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 30/60 - Batch 500/1875 - Loss: 0.0018",
    "Epoch 30/60 - Batch 600/1875 - Loss: 0.0922",
    "Epoch 30/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 30/60 - Batch 800/1875 - Loss: 0.0004",
    "Epoch 30/60 - Batch 900/1875 - Loss: 0.0005",
    "Epoch 30/60 - Batch 1000/1875 - Loss: 0.0006",
    "Epoch 30/60 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 30/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 30/60 - Batch 1300/1875 - Loss: 0.0309",
    "Epoch 30/60 - Batch 1400/1875 - Loss: 0.0563",
    "Epoch 30/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 30/60 - Batch 1600/1875 - Loss: 0.0041",
    "Epoch 30/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 30/60 - Batch 1800/1875 - Loss: 0.0242",
    "Epoch 30 completed - Train Acc: 99.40% - Val Acc: 97.76%",
    "Epoch 31/60 - Batch 0/1875 - Loss: 0.0056",
    "Epoch 31/60 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 31/60 - Batch 200/1875 - Loss: 0.0011",
    "Epoch 31/60 - Batch 300/1875 - Loss: 0.0005",
    "Epoch 31/60 - Batch 400/1875 - Loss: 0.0119",
    "Epoch 31/60 - Batch 500/1875 - Loss: 0.0787",
    "Epoch 31/60 - Batch 600/1875 - Loss: 0.1702",
    "Epoch 31/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 31/60 - Batch 800/1875 - Loss: 0.0202",
    "Epoch 31/60 - Batch 900/1875 - Loss: 0.0419",
    "Epoch 31/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 31/60 - Batch 1100/1875 - Loss: 0.0130",
    "Epoch 31/60 - Batch 1200/1875 - Loss: 0.1200",
    "Epoch 31/60 - Batch 1300/1875 - Loss: 0.0503",
    "Epoch 31/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 31/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 31/60 - Batch 1600/1875 - Loss: 0.1905",
    "Epoch 31/60 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 31/60 - Batch 1800/1875 - Loss: 0.0015",
    "Epoch 31 completed - Train Acc: 99.35% - Val Acc: 97.79%",
    "Epoch 32/60 - Batch 0/1875 - Loss: 0.0002",
    "Epoch 32/60 - Batch 100/1875 - Loss: 0.0061",
    "Epoch 32/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 32/60 - Batch 700/1875 - Loss: 0.1158",
    "Epoch 32/60 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 32/60 - Batch 900/1875 - Loss: 0.0016",
    "Epoch 32/60 - Batch 1000/1875 - Loss: 0.0646",
    "Epoch 32/60 - Batch 1100/1875 - Loss: 0.0602",
    "Epoch 32/60 - Batch 1200/1875 - Loss: 0.0068",
    "Epoch 32/60 - Batch 1300/1875 - Loss: 0.0160",
    "Epoch 32/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 32/60 - Batch 1500/1875 - Loss: 0.0043",
    "Epoch 32/60 - Batch 1600/1875 - Loss: 0.0025",
    "Epoch 32/60 - Batch 1700/1875 - Loss: 0.0077",
    "Epoch 32/60 - Batch 1800/1875 - Loss: 0.0050",
    "Epoch 32 completed - Train Acc: 99.42% - Val Acc: 97.72%",
    "Epoch 33/60 - Batch 0/1875 - Loss: 0.0531",
    "Epoch 33/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 33/60 - Batch 200/1875 - Loss: 0.0735",
    "Epoch 33/60 - Batch 300/1875 - Loss: 0.4089",
    "Epoch 33/60 - Batch 400/1875 - Loss: 0.0390",
    "Epoch 33/60 - Batch 500/1875 - Loss: 0.0016",
    "Epoch 33/60 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 33/60 - Batch 700/1875 - Loss: 0.0002",
    "Epoch 33/60 - Batch 800/1875 - Loss: 0.0184",
    "Epoch 33/60 - Batch 900/1875 - Loss: 0.0040",
    "Epoch 33/60 - Batch 1000/1875 - Loss: 0.0036",
    "Epoch 33/60 - Batch 1100/1875 - Loss: 0.0066",
    "Epoch 33/60 - Batch 1200/1875 - Loss: 0.0211",
    "Epoch 33/60 - Batch 1300/1875 - Loss: 0.0688",
    "Epoch 33/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 33/60 - Batch 1500/1875 - Loss: 0.0015",
    "Epoch 33/60 - Batch 1600/1875 - Loss: 0.0003",
    "Epoch 33/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 33/60 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 33 completed - Train Acc: 99.38% - Val Acc: 97.99%",
    "Epoch 34/60 - Batch 0/1875 - Loss: 0.0127",
    "Epoch 34/60 - Batch 100/1875 - Loss: 0.0006",
    "Epoch 34/60 - Batch 200/1875 - Loss: 0.0057",
    "Epoch 34/60 - Batch 300/1875 - Loss: 0.0117",
    "Epoch 34/60 - Batch 400/1875 - Loss: 0.0186",
    "Epoch 34/60 - Batch 500/1875 - Loss: 0.0025",
    "Epoch 34/60 - Batch 600/1875 - Loss: 0.0010",
    "Epoch 34/60 - Batch 700/1875 - Loss: 0.0007",
    "Epoch 34/60 - Batch 800/1875 - Loss: 0.0086",
    "Epoch 34/60 - Batch 900/1875 - Loss: 0.0016",
    "Epoch 34/60 - Batch 1000/1875 - Loss: 0.0416",
    "Epoch 34/60 - Batch 1100/1875 - Loss: 0.0199",
    "Epoch 34/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 34/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 34/60 - Batch 1400/1875 - Loss: 0.0020",
    "Epoch 34/60 - Batch 1500/1875 - Loss: 0.0023",
    "Epoch 34/60 - Batch 1600/1875 - Loss: 0.0025",
    "Epoch 34/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 34/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 34 completed - Train Acc: 99.37% - Val Acc: 97.99%",
    "Epoch 35/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 35/60 - Batch 100/1875 - Loss: 0.0651",
    "Epoch 35/60 - Batch 200/1875 - Loss: 0.0240",
    "Epoch 35/60 - Batch 300/1875 - Loss: 0.0035",
    "Epoch 35/60 - Batch 400/1875 - Loss: 0.0262",
    "Epoch 35/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 35/60 - Batch 600/1875 - Loss: 0.0006",
    "Epoch 35/60 - Batch 700/1875 - Loss: 0.1169",
    "Epoch 35/60 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 35/60 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 35/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 35/60 - Batch 1100/1875 - Loss: 0.0022",
    "Epoch 35/60 - Batch 1200/1875 - Loss: 0.0126",
    "Epoch 35/60 - Batch 1300/1875 - Loss: 0.2172",
    "Epoch 35/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 35/60 - Batch 1500/1875 - Loss: 0.0299",
    "Epoch 35/60 - Batch 1600/1875 - Loss: 0.0013",
    "Epoch 35/60 - Batch 1700/1875 - Loss: 0.0063",
    "Epoch 35/60 - Batch 1800/1875 - Loss: 0.1360",
    "Epoch 35 completed - Train Acc: 99.45% - Val Acc: 97.90%",
    "Epoch 36/60 - Batch 0/1875 - Loss: 0.0291",
    "Epoch 36/60 - Batch 100/1875 - Loss: 0.0016",
    "Epoch 36/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 300/1875 - Loss: 0.0346",
    "Epoch 36/60 - Batch 400/1875 - Loss: 0.1395",
    "Epoch 36/60 - Batch 500/1875 - Loss: 0.0119",
    "Epoch 36/60 - Batch 600/1875 - Loss: 0.0083",
    "Epoch 36/60 - Batch 700/1875 - Loss: 0.0764",
    "Epoch 36/60 - Batch 800/1875 - Loss: 0.0687",
    "Epoch 36/60 - Batch 900/1875 - Loss: 0.0221",
    "Epoch 36/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 1300/1875 - Loss: 0.0008",
    "Epoch 36/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 36/60 - Batch 1500/1875 - Loss: 0.0006",
    "Epoch 36/60 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 1700/1875 - Loss: 0.0001",
    "Epoch 36/60 - Batch 1800/1875 - Loss: 0.0009",
    "Epoch 36 completed - Train Acc: 99.40% - Val Acc: 97.26%",
    "Epoch 37/60 - Batch 0/1875 - Loss: 0.0107",
    "Epoch 37/60 - Batch 100/1875 - Loss: 0.0011",
    "Epoch 37/60 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 37/60 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 37/60 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 37/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 37/60 - Batch 900/1875 - Loss: 0.1787",
    "Epoch 37/60 - Batch 1000/1875 - Loss: 0.0301",
    "Epoch 37/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 37/60 - Batch 1200/1875 - Loss: 0.0012",
    "Epoch 37/60 - Batch 1300/1875 - Loss: 0.0053",
    "Epoch 37/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 37/60 - Batch 1500/1875 - Loss: 0.6848",
    "Epoch 37/60 - Batch 1600/1875 - Loss: 0.0855",
    "Epoch 37/60 - Batch 1700/1875 - Loss: 0.0003",
    "Epoch 37/60 - Batch 1800/1875 - Loss: 0.0582",
    "Epoch 37 completed - Train Acc: 99.39% - Val Acc: 97.83%",
    "Epoch 38/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 38/60 - Batch 100/1875 - Loss: 0.0222",
    "Epoch 38/60 - Batch 200/1875 - Loss: 0.0013",
    "Epoch 38/60 - Batch 300/1875 - Loss: 0.0143",
    "Epoch 38/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 500/1875 - Loss: 0.0301",
    "Epoch 38/60 - Batch 600/1875 - Loss: 0.0045",
    "Epoch 38/60 - Batch 700/1875 - Loss: 0.0627",
    "Epoch 38/60 - Batch 800/1875 - Loss: 0.0045",
    "Epoch 38/60 - Batch 900/1875 - Loss: 0.0007",
    "Epoch 38/60 - Batch 1000/1875 - Loss: 0.0144",
    "Epoch 38/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 38/60 - Batch 1300/1875 - Loss: 0.0005",
    "Epoch 38/60 - Batch 1400/1875 - Loss: 0.0012",
    "Epoch 38/60 - Batch 1500/1875 - Loss: 0.0002",
    "Epoch 38/60 - Batch 1600/1875 - Loss: 0.0042",
    "Epoch 38/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 38/60 - Batch 1800/1875 - Loss: 0.0049",
    "Epoch 38 completed - Train Acc: 99.46% - Val Acc: 97.93%",
    "Epoch 39/60 - Batch 0/1875 - Loss: 0.0317",
    "Epoch 39/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 39/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 39/60 - Batch 400/1875 - Loss: 0.0442",
    "Epoch 39/60 - Batch 500/1875 - Loss: 0.0156",
    "Epoch 39/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 39/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 800/1875 - Loss: 0.0123",
    "Epoch 39/60 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 39/60 - Batch 1000/1875 - Loss: 0.1923",
    "Epoch 39/60 - Batch 1100/1875 - Loss: 0.0006",
    "Epoch 39/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 39/60 - Batch 1300/1875 - Loss: 0.0010",
    "Epoch 39/60 - Batch 1400/1875 - Loss: 0.0049",
    "Epoch 39/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 39/60 - Batch 1600/1875 - Loss: 0.0738",
    "Epoch 39/60 - Batch 1700/1875 - Loss: 0.1839",
    "Epoch 39/60 - Batch 1800/1875 - Loss: 0.0384",
    "Epoch 39 completed - Train Acc: 99.40% - Val Acc: 97.96%",
    "Epoch 40/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 40/60 - Batch 100/1875 - Loss: 0.0154",
    "Epoch 40/60 - Batch 200/1875 - Loss: 0.0029",
    "Epoch 40/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 500/1875 - Loss: 0.0100",
    "Epoch 40/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 40/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 40/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 1000/1875 - Loss: 0.0010",
    "Epoch 40/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 40/60 - Batch 1300/1875 - Loss: 0.0145",
    "Epoch 40/60 - Batch 1400/1875 - Loss: 0.0001",
    "Epoch 40/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 40/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 40/60 - Batch 1700/1875 - Loss: 0.0031",
    "Epoch 40/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 40 completed - Train Acc: 99.53% - Val Acc: 97.86%",
    "Epoch 41/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 41/60 - Batch 100/1875 - Loss: 0.0470",
    "Epoch 41/60 - Batch 200/1875 - Loss: 0.0009",
    "Epoch 41/60 - Batch 300/1875 - Loss: 0.0023",
    "Epoch 41/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 41/60 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 41/60 - Batch 600/1875 - Loss: 0.0251",
    "Epoch 41/60 - Batch 700/1875 - Loss: 0.0036",
    "Epoch 41/60 - Batch 800/1875 - Loss: 0.0065",
    "Epoch 41/60 - Batch 900/1875 - Loss: 0.0108",
    "Epoch 41/60 - Batch 1000/1875 - Loss: 0.0142",
    "Epoch 41/60 - Batch 1100/1875 - Loss: 0.2290",
    "Epoch 41/60 - Batch 1200/1875 - Loss: 0.0209",
    "Epoch 41/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 41/60 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 41/60 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 41/60 - Batch 1600/1875 - Loss: 0.0348",
    "Epoch 41/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 41/60 - Batch 1800/1875 - Loss: 0.0001",
    "Epoch 41 completed - Train Acc: 99.39% - Val Acc: 97.82%",
    "Epoch 42/60 - Batch 0/1875 - Loss: 0.0078",
    "Epoch 42/60 - Batch 100/1875 - Loss: 0.0067",
    "Epoch 42/60 - Batch 200/1875 - Loss: 0.0603",
    "Epoch 42/60 - Batch 300/1875 - Loss: 0.0012",
    "Epoch 42/60 - Batch 400/1875 - Loss: 0.0040",
    "Epoch 42/60 - Batch 500/1875 - Loss: 0.0004",
    "Epoch 42/60 - Batch 600/1875 - Loss: 0.0071",
    "Epoch 42/60 - Batch 700/1875 - Loss: 0.0262",
    "Epoch 42/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 42/60 - Batch 900/1875 - Loss: 0.0015",
    "Epoch 42/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 42/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 42/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 42/60 - Batch 1300/1875 - Loss: 0.0048",
    "Epoch 42/60 - Batch 1400/1875 - Loss: 0.0049",
    "Epoch 42/60 - Batch 1500/1875 - Loss: 0.0004",
    "Epoch 42/60 - Batch 1600/1875 - Loss: 0.0163",
    "Epoch 42/60 - Batch 1700/1875 - Loss: 0.0007",
    "Epoch 42/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 42 completed - Train Acc: 99.47% - Val Acc: 97.72%",
    "Epoch 43/60 - Batch 0/1875 - Loss: 0.1099",
    "Epoch 43/60 - Batch 100/1875 - Loss: 0.0029",
    "Epoch 43/60 - Batch 200/1875 - Loss: 0.0020",
    "Epoch 43/60 - Batch 300/1875 - Loss: 0.0009",
    "Epoch 43/60 - Batch 400/1875 - Loss: 0.0735",
    "Epoch 43/60 - Batch 500/1875 - Loss: 0.0080",
    "Epoch 43/60 - Batch 600/1875 - Loss: 0.0004",
    "Epoch 43/60 - Batch 700/1875 - Loss: 0.0058",
    "Epoch 43/60 - Batch 800/1875 - Loss: 0.0009",
    "Epoch 43/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 43/60 - Batch 1000/1875 - Loss: 0.0004",
    "Epoch 43/60 - Batch 1100/1875 - Loss: 0.0214",
    "Epoch 43/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 43/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 43/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 43/60 - Batch 1500/1875 - Loss: 0.0140",
    "Epoch 43/60 - Batch 1600/1875 - Loss: 0.0002",
    "Epoch 43/60 - Batch 1700/1875 - Loss: 0.0163",
    "Epoch 43/60 - Batch 1800/1875 - Loss: 0.2259",
    "Epoch 43 completed - Train Acc: 99.47% - Val Acc: 97.91%",
    "Epoch 44/60 - Batch 0/1875 - Loss: 0.0095",
    "Epoch 44/60 - Batch 100/1875 - Loss: 0.0023",
    "Epoch 44/60 - Batch 200/1875 - Loss: 0.0004",
    "Epoch 44/60 - Batch 300/1875 - Loss: 0.0018",
    "Epoch 44/60 - Batch 400/1875 - Loss: 0.0006",
    "Epoch 44/60 - Batch 500/1875 - Loss: 0.0046",
    "Epoch 44/60 - Batch 600/1875 - Loss: 0.2088",
    "Epoch 44/60 - Batch 700/1875 - Loss: 0.0048",
    "Epoch 44/60 - Batch 800/1875 - Loss: 0.0050",
    "Epoch 44/60 - Batch 900/1875 - Loss: 0.2215",
    "Epoch 44/60 - Batch 1000/1875 - Loss: 0.0002",
    "Epoch 44/60 - Batch 1100/1875 - Loss: 0.0013",
    "Epoch 44/60 - Batch 1200/1875 - Loss: 0.3493",
    "Epoch 44/60 - Batch 1300/1875 - Loss: 0.0543",
    "Epoch 44/60 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 44/60 - Batch 1500/1875 - Loss: 0.0130",
    "Epoch 44/60 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 44/60 - Batch 1700/1875 - Loss: 0.0259",
    "Epoch 44/60 - Batch 1800/1875 - Loss: 0.0511",
    "Epoch 44 completed - Train Acc: 99.42% - Val Acc: 97.78%",
    "Epoch 45/60 - Batch 0/1875 - Loss: 0.0058",
    "Epoch 45/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 45/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 500/1875 - Loss: 0.0002",
    "Epoch 45/60 - Batch 600/1875 - Loss: 0.0002",
    "Epoch 45/60 - Batch 700/1875 - Loss: 0.0001",
    "Epoch 45/60 - Batch 800/1875 - Loss: 0.0190",
    "Epoch 45/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 45/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 1200/1875 - Loss: 0.0186",
    "Epoch 45/60 - Batch 1300/1875 - Loss: 0.0027",
    "Epoch 45/60 - Batch 1400/1875 - Loss: 0.0792",
    "Epoch 45/60 - Batch 1500/1875 - Loss: 0.0017",
    "Epoch 45/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 45/60 - Batch 1800/1875 - Loss: 0.0120",
    "Epoch 45 completed - Train Acc: 99.56% - Val Acc: 98.05%",
    "Epoch 46/60 - Batch 0/1875 - Loss: 0.0012",
    "Epoch 46/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 46/60 - Batch 200/1875 - Loss: 0.0073",
    "Epoch 46/60 - Batch 300/1875 - Loss: 0.0031",
    "Epoch 46/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 46/60 - Batch 500/1875 - Loss: 0.0012",
    "Epoch 46/60 - Batch 600/1875 - Loss: 0.0235",
    "Epoch 46/60 - Batch 700/1875 - Loss: 0.0477",
    "Epoch 46/60 - Batch 800/1875 - Loss: 0.0008",
    "Epoch 46/60 - Batch 900/1875 - Loss: 0.3921",
    "Epoch 46/60 - Batch 1000/1875 - Loss: 0.0062",
    "Epoch 46/60 - Batch 1100/1875 - Loss: 0.1519",
    "Epoch 46/60 - Batch 1200/1875 - Loss: 0.0026",
    "Epoch 46/60 - Batch 1300/1875 - Loss: 0.0218",
    "Epoch 46/60 - Batch 1400/1875 - Loss: 0.0009",
    "Epoch 46/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 46/60 - Batch 1600/1875 - Loss: 0.0006",
    "Epoch 46/60 - Batch 1700/1875 - Loss: 0.0008",
    "Epoch 46/60 - Batch 1800/1875 - Loss: 0.0005",
    "Epoch 46 completed - Train Acc: 99.47% - Val Acc: 98.07%",
    "Epoch 47/60 - Batch 0/1875 - Loss: 0.0025",
    "Epoch 47/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 47/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 600/1875 - Loss: 0.0015",
    "Epoch 47/60 - Batch 700/1875 - Loss: 0.3385",
    "Epoch 47/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 47/60 - Batch 900/1875 - Loss: 0.0058",
    "Epoch 47/60 - Batch 1000/1875 - Loss: 0.0003",
    "Epoch 47/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 47/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 47/60 - Batch 1500/1875 - Loss: 0.0297",
    "Epoch 47/60 - Batch 1600/1875 - Loss: 0.4149",
    "Epoch 47/60 - Batch 1700/1875 - Loss: 0.0485",
    "Epoch 47/60 - Batch 1800/1875 - Loss: 0.0142",
    "Epoch 47 completed - Train Acc: 99.51% - Val Acc: 97.89%",
    "Epoch 48/60 - Batch 0/1875 - Loss: 0.0008",
    "Epoch 48/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 200/1875 - Loss: 0.0034",
    "Epoch 48/60 - Batch 300/1875 - Loss: 0.0027",
    "Epoch 48/60 - Batch 400/1875 - Loss: 0.0027",
    "Epoch 48/60 - Batch 500/1875 - Loss: 0.0431",
    "Epoch 48/60 - Batch 600/1875 - Loss: 0.0497",
    "Epoch 48/60 - Batch 700/1875 - Loss: 0.0084",
    "Epoch 48/60 - Batch 800/1875 - Loss: 0.0002",
    "Epoch 48/60 - Batch 900/1875 - Loss: 0.0079",
    "Epoch 48/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 1100/1875 - Loss: 0.0004",
    "Epoch 48/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 1300/1875 - Loss: 0.0000",
    "Epoch 48/60 - Batch 1400/1875 - Loss: 0.0350",
    "Epoch 48/60 - Batch 1500/1875 - Loss: 0.0001",
    "Epoch 48/60 - Batch 1600/1875 - Loss: 0.1079",
    "Epoch 48/60 - Batch 1700/1875 - Loss: 0.0010",
    "Epoch 48/60 - Batch 1800/1875 - Loss: 0.0021",
    "Epoch 48 completed - Train Acc: 99.56% - Val Acc: 97.91%",
    "Epoch 49/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 49/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 49/60 - Batch 200/1875 - Loss: 0.0136",
    "Epoch 49/60 - Batch 300/1875 - Loss: 0.6524",
    "Epoch 49/60 - Batch 400/1875 - Loss: 0.0004",
    "Epoch 49/60 - Batch 500/1875 - Loss: 0.0512",
    "Epoch 49/60 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 49/60 - Batch 700/1875 - Loss: 0.0016",
    "Epoch 49/60 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 49/60 - Batch 900/1875 - Loss: 0.0364",
    "Epoch 49/60 - Batch 1000/1875 - Loss: 0.0869",
    "Epoch 49/60 - Batch 1100/1875 - Loss: 0.0002",
    "Epoch 49/60 - Batch 1200/1875 - Loss: 0.0013",
    "Epoch 49/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 49/60 - Batch 1400/1875 - Loss: 0.0025",
    "Epoch 49/60 - Batch 1500/1875 - Loss: 0.0205",
    "Epoch 49/60 - Batch 1600/1875 - Loss: 0.0001",
    "Epoch 49/60 - Batch 1700/1875 - Loss: 0.0556",
    "Epoch 49/60 - Batch 1800/1875 - Loss: 0.0075",
    "Epoch 49 completed - Train Acc: 99.52% - Val Acc: 98.19%",
    "Epoch 50/60 - Batch 0/1875 - Loss: 0.0015",
    "Epoch 50/60 - Batch 100/1875 - Loss: 0.0003",
    "Epoch 50/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 300/1875 - Loss: 0.0012",
    "Epoch 50/60 - Batch 400/1875 - Loss: 0.0043",
    "Epoch 50/60 - Batch 500/1875 - Loss: 0.0010",
    "Epoch 50/60 - Batch 600/1875 - Loss: 0.0021",
    "Epoch 50/60 - Batch 700/1875 - Loss: 0.0004",
    "Epoch 50/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 900/1875 - Loss: 0.0003",
    "Epoch 50/60 - Batch 1000/1875 - Loss: 0.0031",
    "Epoch 50/60 - Batch 1100/1875 - Loss: 0.0028",
    "Epoch 50/60 - Batch 1200/1875 - Loss: 0.0002",
    "Epoch 50/60 - Batch 1300/1875 - Loss: 0.0084",
    "Epoch 50/60 - Batch 1400/1875 - Loss: 0.0292",
    "Epoch 50/60 - Batch 1500/1875 - Loss: 0.0204",
    "Epoch 50/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 50/60 - Batch 1800/1875 - Loss: 0.0003",
    "Epoch 50 completed - Train Acc: 99.47% - Val Acc: 97.92%",
    "Epoch 51/60 - Batch 0/1875 - Loss: 0.0092",
    "Epoch 51/60 - Batch 100/1875 - Loss: 0.0813",
    "Epoch 51/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 300/1875 - Loss: 0.0001",
    "Epoch 51/60 - Batch 400/1875 - Loss: 0.0502",
    "Epoch 51/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 51/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 700/1875 - Loss: 0.0126",
    "Epoch 51/60 - Batch 800/1875 - Loss: 0.0003",
    "Epoch 51/60 - Batch 900/1875 - Loss: 0.0009",
    "Epoch 51/60 - Batch 1000/1875 - Loss: 0.0001",
    "Epoch 51/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 1200/1875 - Loss: 0.0195",
    "Epoch 51/60 - Batch 1300/1875 - Loss: 0.0002",
    "Epoch 51/60 - Batch 1400/1875 - Loss: 0.0044",
    "Epoch 51/60 - Batch 1500/1875 - Loss: 0.0005",
    "Epoch 51/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 51/60 - Batch 1700/1875 - Loss: 0.0005",
    "Epoch 51/60 - Batch 1800/1875 - Loss: 0.0000",
    "Epoch 51 completed - Train Acc: 99.53% - Val Acc: 98.05%",
    "Epoch 52/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 100/1875 - Loss: 0.0005",
    "Epoch 52/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 52/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 400/1875 - Loss: 0.0001",
    "Epoch 52/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 600/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 700/1875 - Loss: 0.0014",
    "Epoch 52/60 - Batch 800/1875 - Loss: 0.0006",
    "Epoch 52/60 - Batch 900/1875 - Loss: 0.0624",
    "Epoch 52/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 1100/1875 - Loss: 0.0000",
    "Epoch 52/60 - Batch 1200/1875 - Loss: 0.0132",
    "Epoch 52/60 - Batch 1300/1875 - Loss: 0.0003",
    "Epoch 52/60 - Batch 1400/1875 - Loss: 0.0862",
    "Epoch 52/60 - Batch 1500/1875 - Loss: 0.0011",
    "Epoch 52/60 - Batch 1600/1875 - Loss: 0.0007",
    "Epoch 52/60 - Batch 1700/1875 - Loss: 0.0064",
    "Epoch 52/60 - Batch 1800/1875 - Loss: 0.0019",
    "Epoch 52 completed - Train Acc: 99.50% - Val Acc: 97.61%",
    "Epoch 53/60 - Batch 0/1875 - Loss: 0.2143",
    "Epoch 53/60 - Batch 100/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 200/1875 - Loss: 0.0610",
    "Epoch 53/60 - Batch 300/1875 - Loss: 0.0003",
    "Epoch 53/60 - Batch 400/1875 - Loss: 0.0010",
    "Epoch 53/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 600/1875 - Loss: 0.0003",
    "Epoch 53/60 - Batch 700/1875 - Loss: 0.0009",
    "Epoch 53/60 - Batch 800/1875 - Loss: 0.0250",
    "Epoch 53/60 - Batch 900/1875 - Loss: 0.0001",
    "Epoch 53/60 - Batch 1000/1875 - Loss: 0.0041",
    "Epoch 53/60 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 53/60 - Batch 1200/1875 - Loss: 0.0017",
    "Epoch 53/60 - Batch 1300/1875 - Loss: 0.0001",
    "Epoch 53/60 - Batch 1400/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 1500/1875 - Loss: 0.0008",
    "Epoch 53/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 53/60 - Batch 1800/1875 - Loss: 0.0060",
    "Epoch 53 completed - Train Acc: 99.53% - Val Acc: 98.16%",
    "Epoch 54/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 100/1875 - Loss: 0.0008",
    "Epoch 54/60 - Batch 200/1875 - Loss: 0.0092",
    "Epoch 54/60 - Batch 300/1875 - Loss: 0.0125",
    "Epoch 54/60 - Batch 400/1875 - Loss: 0.0003",
    "Epoch 54/60 - Batch 500/1875 - Loss: 0.0001",
    "Epoch 54/60 - Batch 600/1875 - Loss: 0.0034",
    "Epoch 54/60 - Batch 700/1875 - Loss: 0.0013",
    "Epoch 54/60 - Batch 800/1875 - Loss: 0.0005",
    "Epoch 54/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 1000/1875 - Loss: 0.0129",
    "Epoch 54/60 - Batch 1100/1875 - Loss: 0.1257",
    "Epoch 54/60 - Batch 1200/1875 - Loss: 0.0574",
    "Epoch 54/60 - Batch 1300/1875 - Loss: 0.0019",
    "Epoch 54/60 - Batch 1400/1875 - Loss: 0.0039",
    "Epoch 54/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 54/60 - Batch 1600/1875 - Loss: 0.0008",
    "Epoch 54/60 - Batch 1700/1875 - Loss: 0.0002",
    "Epoch 54/60 - Batch 1800/1875 - Loss: 0.0097",
    "Epoch 54 completed - Train Acc: 99.50% - Val Acc: 97.77%",
    "Epoch 55/60 - Batch 0/1875 - Loss: 0.0001",
    "Epoch 55/60 - Batch 100/1875 - Loss: 0.0001",
    "Epoch 55/60 - Batch 200/1875 - Loss: 0.0001",
    "Epoch 55/60 - Batch 300/1875 - Loss: 0.0103",
    "Epoch 55/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 500/1875 - Loss: 0.0005",
    "Epoch 55/60 - Batch 600/1875 - Loss: 0.0035",
    "Epoch 55/60 - Batch 700/1875 - Loss: 0.0077",
    "Epoch 55/60 - Batch 800/1875 - Loss: 0.0102",
    "Epoch 55/60 - Batch 900/1875 - Loss: 0.0002",
    "Epoch 55/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 1100/1875 - Loss: 0.0005",
    "Epoch 55/60 - Batch 1200/1875 - Loss: 0.0302",
    "Epoch 55/60 - Batch 1300/1875 - Loss: 0.1023",
    "Epoch 55/60 - Batch 1400/1875 - Loss: 0.0110",
    "Epoch 55/60 - Batch 1500/1875 - Loss: 0.0006",
    "Epoch 55/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 55/60 - Batch 1800/1875 - Loss: 0.0168",
    "Epoch 55 completed - Train Acc: 99.55% - Val Acc: 97.90%",
    "Epoch 56/60 - Batch 0/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 100/1875 - Loss: 0.0012",
    "Epoch 56/60 - Batch 200/1875 - Loss: 0.0117",
    "Epoch 56/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 400/1875 - Loss: 0.0134",
    "Epoch 56/60 - Batch 500/1875 - Loss: 0.0038",
    "Epoch 56/60 - Batch 600/1875 - Loss: 0.0001",
    "Epoch 56/60 - Batch 700/1875 - Loss: 0.0443",
    "Epoch 56/60 - Batch 800/1875 - Loss: 0.0230",
    "Epoch 56/60 - Batch 900/1875 - Loss: 0.0029",
    "Epoch 56/60 - Batch 1000/1875 - Loss: 0.0024",
    "Epoch 56/60 - Batch 1100/1875 - Loss: 0.0113",
    "Epoch 56/60 - Batch 1200/1875 - Loss: 0.0083",
    "Epoch 56/60 - Batch 1300/1875 - Loss: 0.0170",
    "Epoch 56/60 - Batch 1400/1875 - Loss: 0.1410",
    "Epoch 56/60 - Batch 1500/1875 - Loss: 0.0013",
    "Epoch 56/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 56/60 - Batch 1700/1875 - Loss: 0.0406",
    "Epoch 56/60 - Batch 1800/1875 - Loss: 0.0007",
    "Epoch 56 completed - Train Acc: 99.52% - Val Acc: 97.88%",
    "Epoch 57/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 57/60 - Batch 100/1875 - Loss: 0.0027",
    "Epoch 57/60 - Batch 200/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 300/1875 - Loss: 0.0002",
    "Epoch 57/60 - Batch 400/1875 - Loss: 0.0815",
    "Epoch 57/60 - Batch 500/1875 - Loss: 0.0006",
    "Epoch 57/60 - Batch 600/1875 - Loss: 0.0005",
    "Epoch 57/60 - Batch 700/1875 - Loss: 0.0008",
    "Epoch 57/60 - Batch 800/1875 - Loss: 0.0346",
    "Epoch 57/60 - Batch 900/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1000/1875 - Loss: 0.0219",
    "Epoch 57/60 - Batch 1100/1875 - Loss: 0.0001",
    "Epoch 57/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1300/1875 - Loss: 0.0007",
    "Epoch 57/60 - Batch 1400/1875 - Loss: 0.0516",
    "Epoch 57/60 - Batch 1500/1875 - Loss: 0.0031",
    "Epoch 57/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 57/60 - Batch 1700/1875 - Loss: 0.0048",
    "Epoch 57/60 - Batch 1800/1875 - Loss: 0.0534",
    "Epoch 57 completed - Train Acc: 99.44% - Val Acc: 97.48%",
    "Epoch 58/60 - Batch 0/1875 - Loss: 0.0004",
    "Epoch 58/60 - Batch 100/1875 - Loss: 0.0168",
    "Epoch 58/60 - Batch 200/1875 - Loss: 0.0211",
    "Epoch 58/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 58/60 - Batch 400/1875 - Loss: 0.0006",
    "Epoch 58/60 - Batch 500/1875 - Loss: 0.0000",
    "Epoch 58/60 - Batch 600/1875 - Loss: 0.0013",
    "Epoch 58/60 - Batch 700/1875 - Loss: 0.0011",
    "Epoch 58/60 - Batch 800/1875 - Loss: 0.0001",
    "Epoch 58/60 - Batch 900/1875 - Loss: 0.4528",
    "Epoch 58/60 - Batch 1000/1875 - Loss: 0.2237",
    "Epoch 58/60 - Batch 1100/1875 - Loss: 0.0134",
    "Epoch 58/60 - Batch 1200/1875 - Loss: 0.0124",
    "Epoch 58/60 - Batch 1300/1875 - Loss: 0.0018",
    "Epoch 58/60 - Batch 1400/1875 - Loss: 0.0002",
    "Epoch 58/60 - Batch 1500/1875 - Loss: 0.0184",
    "Epoch 58/60 - Batch 1600/1875 - Loss: 0.0004",
    "Epoch 58/60 - Batch 1700/1875 - Loss: 0.0004",
    "Epoch 58/60 - Batch 1800/1875 - Loss: 0.0014",
    "Epoch 58 completed - Train Acc: 99.55% - Val Acc: 98.05%",
    "Epoch 59/60 - Batch 0/1875 - Loss: 0.0184",
    "Epoch 59/60 - Batch 100/1875 - Loss: 0.0156",
    "Epoch 59/60 - Batch 200/1875 - Loss: 0.0578",
    "Epoch 59/60 - Batch 300/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 400/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 500/1875 - Loss: 0.0029",
    "Epoch 59/60 - Batch 600/1875 - Loss: 0.0519",
    "Epoch 59/60 - Batch 700/1875 - Loss: 0.0006",
    "Epoch 59/60 - Batch 800/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 900/1875 - Loss: 0.0004",
    "Epoch 59/60 - Batch 1000/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 1100/1875 - Loss: 0.0131",
    "Epoch 59/60 - Batch 1200/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 1300/1875 - Loss: 0.0570",
    "Epoch 59/60 - Batch 1400/1875 - Loss: 0.0120",
    "Epoch 59/60 - Batch 1500/1875 - Loss: 0.0000",
    "Epoch 59/60 - Batch 1600/1875 - Loss: 0.1583",
    "Epoch 59/60 - Batch 1700/1875 - Loss: 0.0017",
    "Epoch 59/60 - Batch 1800/1875 - Loss: 0.0019",
    "Epoch 59 completed - Train Acc: 99.54% - Val Acc: 97.93%",
    "Epoch 60/60 - Batch 0/1875 - Loss: 0.0010",
    "Epoch 60/60 - Batch 100/1875 - Loss: 0.0019",
    "Epoch 60/60 - Batch 200/1875 - Loss: 0.0071",
    "Epoch 60/60 - Batch 300/1875 - Loss: 0.0203",
    "Epoch 60/60 - Batch 400/1875 - Loss: 0.0002",
    "Epoch 60/60 - Batch 500/1875 - Loss: 0.0003",
    "Epoch 60/60 - Batch 600/1875 - Loss: 0.0007",
    "Epoch 60/60 - Batch 700/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 800/1875 - Loss: 0.0025",
    "Epoch 60/60 - Batch 900/1875 - Loss: 0.0806",
    "Epoch 60/60 - Batch 1000/1875 - Loss: 0.0036",
    "Epoch 60/60 - Batch 1100/1875 - Loss: 0.0024",
    "Epoch 60/60 - Batch 1200/1875 - Loss: 0.0001",
    "Epoch 60/60 - Batch 1300/1875 - Loss: 0.0099",
    "Epoch 60/60 - Batch 1400/1875 - Loss: 0.0003",
    "Epoch 60/60 - Batch 1500/1875 - Loss: 0.0007",
    "Epoch 60/60 - Batch 1600/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 1700/1875 - Loss: 0.0000",
    "Epoch 60/60 - Batch 1800/1875 - Loss: 0.0002",
    "Epoch 60 completed - Train Acc: 99.49% - Val Acc: 98.07%",
    "Training completed. Model saved to models/ZPE-Colab-Sim_hnn_step8.pt"
  ],
  "metrics": [
    {
      "epoch": 1,
      "train_loss": 0.288929066093266,
      "train_accuracy": 91.32666666666667,
      "val_loss": 0.16943997358077595,
      "val_accuracy": 95.35
    },
    {
      "epoch": 2,
      "train_loss": 0.1386741122727127,
      "train_accuracy": 96.27166666666666,
      "val_loss": 0.19021571862044373,
      "val_accuracy": 95.55
    },
    {
      "epoch": 3,
      "train_loss": 0.10825038759844999,
      "train_accuracy": 97.14333333333333,
      "val_loss": 0.10598084067328345,
      "val_accuracy": 97.18
    },
    {
      "epoch": 4,
      "train_loss": 0.08862846600030704,
      "train_accuracy": 97.57166666666667,
      "val_loss": 0.10986289391527976,
      "val_accuracy": 97.34
    },
    {
      "epoch": 5,
      "train_loss": 0.07433261332090091,
      "train_accuracy": 97.98166666666667,
      "val_loss": 0.11856214169975587,
      "val_accuracy": 96.65
    },
    {
      "epoch": 6,
      "train_loss": 0.0689165967001735,
      "train_accuracy": 98.15,
      "val_loss": 0.11227237880761816,
      "val_accuracy": 97.13
    },
    {
      "epoch": 7,
      "train_loss": 0.06296765599535138,
      "train_accuracy": 98.33166666666666,
      "val_loss": 0.08901529088069846,
      "val_accuracy": 97.95
    },
    {
      "epoch": 8,
      "train_loss": 0.05455612239802091,
      "train_accuracy": 98.555,
      "val_loss": 0.08808589044434112,
      "val_accuracy": 97.67
    },
    {
      "epoch": 9,
      "train_loss": 0.05197818799252976,
      "train_accuracy": 98.63333333333334,
      "val_loss": 0.0877222028411663,
      "val_accuracy": 97.68
    },
    {
      "epoch": 10,
      "train_loss": 0.04527815052240949,
      "train_accuracy": 98.76,
      "val_loss": 0.10931366299309347,
      "val_accuracy": 97.49
    },
    {
      "epoch": 11,
      "train_loss": 0.0431005266664825,
      "train_accuracy": 98.81333333333333,
      "val_loss": 0.09676316211328095,
      "val_accuracy": 98.06
    },
    {
      "epoch": 12,
      "train_loss": 0.0436327341193155,
      "train_accuracy": 98.87833333333333,
      "val_loss": 0.10261081791897556,
      "val_accuracy": 97.58
    },
    {
      "epoch": 13,
      "train_loss": 0.04100970381548711,
      "train_accuracy": 98.94,
      "val_loss": 0.09175040171545368,
      "val_accuracy": 98.23
    },
    {
      "epoch": 14,
      "train_loss": 0.039829851477163905,
      "train_accuracy": 98.98166666666667,
      "val_loss": 0.10355557072138742,
      "val_accuracy": 97.41
    },
    {
      "epoch": 15,
      "train_loss": 0.03603964527248672,
      "train_accuracy": 99.06833333333333,
      "val_loss": 0.09301813907853948,
      "val_accuracy": 97.99
    },
    {
      "epoch": 16,
      "train_loss": 0.03360600247800752,
      "train_accuracy": 99.10333333333334,
      "val_loss": 0.09905297966428916,
      "val_accuracy": 98.12
    },
    {
      "epoch": 17,
      "train_loss": 0.036377410172291944,
      "train_accuracy": 99.08666666666667,
      "val_loss": 0.09517080316969098,
      "val_accuracy": 97.93
    },
    {
      "epoch": 18,
      "train_loss": 0.03365439830365315,
      "train_accuracy": 99.16166666666666,
      "val_loss": 0.11178034753977652,
      "val_accuracy": 97.64
    },
    {
      "epoch": 19,
      "train_loss": 0.031534660637528765,
      "train_accuracy": 99.19333333333333,
      "val_loss": 0.11758698361680377,
      "val_accuracy": 97.68
    },
    {
      "epoch": 20,
      "train_loss": 0.02877303574306546,
      "train_accuracy": 99.22666666666667,
      "val_loss": 0.1349224017515508,
      "val_accuracy": 97.78
    },
    {
      "epoch": 21,
      "train_loss": 0.03223050090749297,
      "train_accuracy": 99.2,
      "val_loss": 0.13672319794777416,
      "val_accuracy": 97.68
    },
    {
      "epoch": 22,
      "train_loss": 0.029986445102779695,
      "train_accuracy": 99.26,
      "val_loss": 0.12189303568904429,
      "val_accuracy": 98.09
    },
    {
      "epoch": 23,
      "train_loss": 0.028718061999042872,
      "train_accuracy": 99.26666666666667,
      "val_loss": 0.13060645930737647,
      "val_accuracy": 97.47
    },
    {
      "epoch": 24,
      "train_loss": 0.028166724936459133,
      "train_accuracy": 99.275,
      "val_loss": 0.1372559129426364,
      "val_accuracy": 98.04
    },
    {
      "epoch": 25,
      "train_loss": 0.025259770902032762,
      "train_accuracy": 99.32166666666667,
      "val_loss": 0.13011798023981314,
      "val_accuracy": 98.21
    },
    {
      "epoch": 26,
      "train_loss": 0.028635979393176466,
      "train_accuracy": 99.26,
      "val_loss": 0.10067233451706897,
      "val_accuracy": 98.15
    },
    {
      "epoch": 27,
      "train_loss": 0.02478076560146479,
      "train_accuracy": 99.395,
      "val_loss": 0.14351124008429839,
      "val_accuracy": 97.86
    },
    {
      "epoch": 28,
      "train_loss": 0.0240981191650461,
      "train_accuracy": 99.37833333333333,
      "val_loss": 0.21906211127970526,
      "val_accuracy": 97.77
    },
    {
      "epoch": 29,
      "train_loss": 0.027684350445811804,
      "train_accuracy": 99.3,
      "val_loss": 0.12416750414607118,
      "val_accuracy": 97.8
    },
    {
      "epoch": 30,
      "train_loss": 0.02395656495477793,
      "train_accuracy": 99.39666666666666,
      "val_loss": 0.14915051319050487,
      "val_accuracy": 97.76
    },
    {
      "epoch": 31,
      "train_loss": 0.02588988308367962,
      "train_accuracy": 99.35333333333334,
      "val_loss": 0.12468072630620664,
      "val_accuracy": 97.79
    },
    {
      "epoch": 32,
      "train_loss": 0.02448049891356719,
      "train_accuracy": 99.415,
      "val_loss": 0.14634392228802745,
      "val_accuracy": 97.72
    },
    {
      "epoch": 33,
      "train_loss": 0.023838723340070252,
      "train_accuracy": 99.38333333333334,
      "val_loss": 0.1598787779886924,
      "val_accuracy": 97.99
    },
    {
      "epoch": 34,
      "train_loss": 0.025050635778443513,
      "train_accuracy": 99.36833333333334,
      "val_loss": 0.10441806033218133,
      "val_accuracy": 97.99
    },
    {
      "epoch": 35,
      "train_loss": 0.023539273881195033,
      "train_accuracy": 99.44666666666667,
      "val_loss": 0.11861480778791328,
      "val_accuracy": 97.9
    },
    {
      "epoch": 36,
      "train_loss": 0.02331454457988562,
      "train_accuracy": 99.40166666666667,
      "val_loss": 0.20010920597507303,
      "val_accuracy": 97.26
    },
    {
      "epoch": 37,
      "train_loss": 0.026682492549504822,
      "train_accuracy": 99.38666666666667,
      "val_loss": 0.1246617445437059,
      "val_accuracy": 97.83
    },
    {
      "epoch": 38,
      "train_loss": 0.022498137795883468,
      "train_accuracy": 99.45833333333333,
      "val_loss": 0.10208443630285045,
      "val_accuracy": 97.93
    },
    {
      "epoch": 39,
      "train_loss": 0.02187153454461207,
      "train_accuracy": 99.40333333333334,
      "val_loss": 0.12181953757589686,
      "val_accuracy": 97.96
    },
    {
      "epoch": 40,
      "train_loss": 0.018417575604625692,
      "train_accuracy": 99.53,
      "val_loss": 0.12316091735729279,
      "val_accuracy": 97.86
    },
    {
      "epoch": 41,
      "train_loss": 0.024313751954677592,
      "train_accuracy": 99.39166666666667,
      "val_loss": 0.15176580726298028,
      "val_accuracy": 97.82
    },
    {
      "epoch": 42,
      "train_loss": 0.02095269303859975,
      "train_accuracy": 99.47166666666666,
      "val_loss": 0.14780566488611174,
      "val_accuracy": 97.72
    },
    {
      "epoch": 43,
      "train_loss": 0.025580115111880044,
      "train_accuracy": 99.465,
      "val_loss": 0.13282858715664442,
      "val_accuracy": 97.91
    },
    {
      "epoch": 44,
      "train_loss": 0.023222335453406965,
      "train_accuracy": 99.41666666666667,
      "val_loss": 0.11209026919361016,
      "val_accuracy": 97.78
    },
    {
      "epoch": 45,
      "train_loss": 0.020204427182498704,
      "train_accuracy": 99.56,
      "val_loss": 0.14911301456769557,
      "val_accuracy": 98.05
    },
    {
      "epoch": 46,
      "train_loss": 0.022843035451618122,
      "train_accuracy": 99.465,
      "val_loss": 0.11917743064923463,
      "val_accuracy": 98.07
    },
    {
      "epoch": 47,
      "train_loss": 0.02008471742281575,
      "train_accuracy": 99.51,
      "val_loss": 0.1262502595092295,
      "val_accuracy": 97.89
    },
    {
      "epoch": 48,
      "train_loss": 0.01872736528734762,
      "train_accuracy": 99.55666666666667,
      "val_loss": 0.17668358279279558,
      "val_accuracy": 97.91
    },
    {
      "epoch": 49,
      "train_loss": 0.019929259936464768,
      "train_accuracy": 99.51666666666667,
      "val_loss": 0.12450072495221928,
      "val_accuracy": 98.19
    },
    {
      "epoch": 50,
      "train_loss": 0.02225609730413223,
      "train_accuracy": 99.47166666666666,
      "val_loss": 0.11256105459771071,
      "val_accuracy": 97.92
    },
    {
      "epoch": 51,
      "train_loss": 0.018734636449246035,
      "train_accuracy": 99.53333333333333,
      "val_loss": 0.13184024610681294,
      "val_accuracy": 98.05
    },
    {
      "epoch": 52,
      "train_loss": 0.02135443409671484,
      "train_accuracy": 99.505,
      "val_loss": 0.17449961579349568,
      "val_accuracy": 97.61
    },
    {
      "epoch": 53,
      "train_loss": 0.019631195696966884,
      "train_accuracy": 99.53333333333333,
      "val_loss": 0.1329040529119917,
      "val_accuracy": 98.16
    },
    {
      "epoch": 54,
      "train_loss": 0.021348671925880986,
      "train_accuracy": 99.505,
      "val_loss": 0.17085733992022992,
      "val_accuracy": 97.77
    },
    {
      "epoch": 55,
      "train_loss": 0.018042875449820743,
      "train_accuracy": 99.55333333333333,
      "val_loss": 0.13858008264196256,
      "val_accuracy": 97.9
    },
    {
      "epoch": 56,
      "train_loss": 0.019692443712508705,
      "train_accuracy": 99.51833333333333,
      "val_loss": 0.1282307977492178,
      "val_accuracy": 97.88
    },
    {
      "epoch": 57,
      "train_loss": 0.02324150065100527,
      "train_accuracy": 99.445,
      "val_loss": 0.16412399359517899,
      "val_accuracy": 97.48
    },
    {
      "epoch": 58,
      "train_loss": 0.019971430724295525,
      "train_accuracy": 99.55166666666666,
      "val_loss": 0.14187674746317885,
      "val_accuracy": 98.05
    },
    {
      "epoch": 59,
      "train_loss": 0.02034919870707068,
      "train_accuracy": 99.53833333333333,
      "val_loss": 0.18476645398246055,
      "val_accuracy": 97.93
    },
    {
      "epoch": 60,
      "train_loss": 0.020845774110362085,
      "train_accuracy": 99.48833333333333,
      "val_loss": 0.14990759648278598,
      "val_accuracy": 98.07
    }
  ]
}